; Copyright 2018-2021 - Mersenne Research, Inc.  All rights reserved
; Author:  George Woltman
; Email: woltman@alum.mit.edu
;

;;
;; Macros for an AVX-512 radix-64 final step in pass 2 of an FFT.  This is simply an eight-complex
;; forward FFT, swizzle, eight-complex-with-square, swizzle, eight-complex inverse FFT.  No
;; floating point operations are saved, but we do eliminate 32 loads and 32 stores as well as
;; perform the maximum amount of work while data is in registers and we reduce L1 cache thrashing.
;;

;;; BUG is scinc always zero???  Can we preload some of the sin/cos values??  Can we in any way reduce operations
;;; because we know what the sin/cos values are??  pre-swizzle them and use some FMA instructions??


zr64_sixtyfour_complex_fft_final_preload MACRO
	zr64_64c_fft_cmn_preload
	ENDM
zr64_sixtyfour_complex_fft_final MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	zr64_64c_fft_cmn srcreg,0,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	ENDM
zr64f_sixtyfour_complex_fft_final_preload MACRO
	zr64_64c_fft_cmn_preload
	ENDM
zr64f_sixtyfour_complex_fft_final MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	zr64_64c_fft_cmn srcreg,rbx,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

zr64_64c_fft_cmn_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRTHALF
	ENDM
zr64_64c_fft_cmn MACRO srcreg,srcoff,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	zmm0, [srcreg+srcoff]			;; R1
	vmovapd	zmm20, [srcreg+srcoff+d4]		;; R5
	vaddpd	zmm15, zmm0, zmm20			;; R1+R5					; 1-4		n 9
	vsubpd	zmm0, zmm0, zmm20			;; R1-R5					; 1-4		n 18

	vmovapd	zmm16, [srcreg+srcoff+d2]		;; R3
	vmovapd	zmm20, [srcreg+srcoff+d4+d2]		;; R7
	vaddpd	zmm1, zmm16, zmm20			;; R3+R7					; 2-5		n 9
	vsubpd	zmm16, zmm16, zmm20			;; R3-R7					; 2-5		n 23

	vmovapd	zmm14, [srcreg+srcoff+d1]		;; R2
	vmovapd	zmm20, [srcreg+srcoff+d4+d1]		;; R6
	vaddpd	zmm13, zmm14, zmm20			;; R2+R6					; 3-6		n 10
	vsubpd	zmm14, zmm14, zmm20			;; R2-R6					; 3-6		n 13

	vmovapd	zmm6, [srcreg+srcoff+d2+d1]		;; R4
	vmovapd	zmm20, [srcreg+srcoff+d4+d2+d1]		;; R8
	vaddpd	zmm2, zmm6, zmm20			;; R4+R8					; 4-7		n 10
	vsubpd	zmm6, zmm6, zmm20			;; R4-R8					; 4-7		n 13

	vmovapd	zmm7, [srcreg+srcoff+d2+d1+64]		;; I4
	vmovapd	zmm20, [srcreg+srcoff+d4+d2+d1+64]	;; I8
	vaddpd	zmm10, zmm7, zmm20			;; I4+I8					; 5-8		n 11
	vsubpd	zmm7, zmm7, zmm20			;; I4-I8					; 5-8		n 14

	vmovapd	zmm8, [srcreg+srcoff+d1+64]		;; I2
	vmovapd	zmm20, [srcreg+srcoff+d4+d1+64]		;; I6
	vaddpd	zmm4, zmm8, zmm20			;; I2+I6					; 6-9		n 11
	vsubpd	zmm8, zmm8, zmm20			;; I2-I6					; 6-9		n 14

	vmovapd	zmm5, [srcreg+srcoff+64]		;; I1
	vmovapd	zmm20, [srcreg+srcoff+d4+64]		;; I5
	vaddpd	zmm11, zmm5, zmm20			;; I1+I5					; 7-10		n 12
	vsubpd	zmm5, zmm5, zmm20			;; I1-I5					; 7-10		n 19

	vmovapd	zmm9, [srcreg+srcoff+d2+64]		;; I3
	vmovapd	zmm20, [srcreg+srcoff+d4+d2+64]		;; I7
	vaddpd	zmm12, zmm9, zmm20			;; I3+I7					; 8-11		n 12
	vsubpd	zmm9, zmm9, zmm20			;; I3-I7					; 8-11		n 24

	vaddpd	zmm3, zmm15, zmm1		;; r1++ = (r1+r5) + (r3+r7)				; 9-12		n 15
	vsubpd	zmm15, zmm15, zmm1		;; r1+- = (r1+r5) - (r3+r7)				; 9-12		n 21
	vmovapd zmm22, [screg+1*128]		;; sine for R3/I3 and R7/I7

	vsubpd	zmm1, zmm13, zmm2		;; r2+- = (r2+r6) - (r4+r8)				; 10-13		n 17
	vaddpd	zmm13, zmm13, zmm2		;; r2++ = (r2+r6) + (r4+r8)				; 10-13		n 15
	vmovapd zmm29, [screg+3*128+64]		;; cosine/sine for R5/I5

	vsubpd	zmm2, zmm4, zmm10		;; i2+- = (i2+i6) - (i4+i8)				; 11-14		n 17
	vaddpd	zmm4, zmm4, zmm10		;; i2++ = (i2+i6) + (i4+i8)				; 11-14		n 16
	vmovapd zmm23, [screg+0*128]		;; sine for R2/I2 and R8/I8

	vaddpd	zmm10, zmm11, zmm12		;; i1++ = (i1+i5) + (i3+i7)				; 12-15		n 16
	vsubpd	zmm11, zmm11, zmm12		;; i1+- = (i1+i5) - (i3+i7)				; 12-15		n 21
	vmovapd zmm24, [screg+2*128]		;; sine for R4/I4 and R6/I6

	vaddpd	zmm12, zmm14, zmm6		;; r2-+ = (r2-r6) + (r4-r8)				; 13-16		n 23
	vsubpd	zmm14, zmm14, zmm6		;; r2-- = (r2-r6) - (r4-r8)				; 13-16		n 18
	vmovapd zmm25, [screg+3*128]		;; sine for R5/I5

	vsubpd	zmm6, zmm8, zmm7		;; i2-- = (i2-i6) - (i4-i8)				; 14-17		n 19
	vaddpd	zmm8, zmm8, zmm7		;; i2-+ = (i2-i6) + (i4-i8)				; 14-17		n 24
	vmovapd zmm26, [screg+1*128+64]		;; cosine/sine for R3/I3 and R7/I7

	vsubpd	zmm7, zmm3, zmm13		;; R5 = (r1++) - (r2++)					; 15-18		n 20
	vaddpd	zmm3, zmm3, zmm13		;; R1 = (r1++) + (r2++)					; 15-18		n 33
	vmovapd zmm27, [screg+0*128+64]		;; cosine/sine for R2/I2 and R8/I8

	vsubpd	zmm17, zmm10, zmm4		;; I5 = (i1++) - (i2++)					; 16-19		n 20
	vaddpd	zmm10, zmm10, zmm4		;; I1 = (i1++) + (i2++)					; 16-19		n 35
	vmovapd zmm28, [screg+2*128+64]		;; cosine/sine for R4/I4 and R6/I6

	vmulpd	zmm1, zmm1, zmm22		;; r2+-s = r2+- * sine37				; 17-20		n 21
	vmulpd	zmm2, zmm2, zmm22		;; i2+-s = i2+- * sine37				; 17-20		n 21
	L1prefetchw srcreg+L1pd, L1pt

	zfmaddpd zmm4, zmm14, zmm31, zmm0	;; r1-+ = (r1-r5) + .707*(r2--)				; 18-21		n 25
	zfnmaddpd zmm14, zmm14, zmm31, zmm0	;; r1-- = (r1-r5) - .707*(r2--)				; 18-21		n 26
	L1prefetchw srcreg+64+L1pd, L1pt

	zfmaddpd zmm0, zmm6, zmm31, zmm5	;; i1-+ = (i1-i5) + .707*(i2--)				; 19-22		n 25
	zfnmaddpd zmm6, zmm6, zmm31, zmm5	;; i1-- = (i1-i5) - .707*(i2--)				; 19-22		n 26
	L1prefetchw srcreg+d1+L1pd, L1pt

	zfmsubpd zmm13, zmm7, zmm29, zmm17	;; A5 = R5 * cosine/sine - I5				; 20-23		n 27
	zfmaddpd zmm17, zmm17, zmm29, zmm7	;; B5 = I5 * cosine/sine + R5				; 20-23		n 27
	L1prefetchw srcreg+d1+64+L1pd, L1pt

	zfmsubpd zmm7, zmm15, zmm22, zmm2	;; R3s = (r1+-)*sine37 - (i2+-s)			; 21-24		n 28
	zfmaddpd zmm5, zmm11, zmm22, zmm1	;; I3s = (i1+-)*sine37 + (r2+-s)			; 21-24		n 28
	L1prefetchw srcreg+d2+L1pd, L1pt

	zfmaddpd zmm15, zmm15, zmm22, zmm2	;; R7s = (r1+-)*sine37 + (i2+-s)			; 22-25		n 29
	zfmsubpd zmm11, zmm11, zmm22, zmm1	;; I7s = (i1+-)*sine37 - (r2+-s)			; 22-25		n 29
	L1prefetchw srcreg+d2+64+L1pd, L1pt

	zfmaddpd zmm2, zmm12, zmm31, zmm16	;; r3-+ = (r3-r7) + .707*(r2-+)				; 23-26		n 30
	zfnmaddpd zmm12, zmm12, zmm31, zmm16	;; r3-- = (r3-r7) - .707*(r2-+)				; 23-26		n 31
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	zfmaddpd zmm19, zmm8, zmm31, zmm9	;; i3-+ = (i3-i7) + .707*(i2-+)				; 24-27		n 30
	zfnmaddpd zmm8, zmm8, zmm31, zmm9	;; i3-- = (i3-i7) - .707*(i2-+)				; 24-27		n 31
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt

	vmulpd	zmm4, zmm4, zmm23		;; r1-+s = r1-+ * sine28				; 25-28		n 30
	vmulpd	zmm0, zmm0, zmm23		;; i1-+s = i1-+ * sine28				; 25-28		n 30
	L1prefetchw srcreg+d4+L1pd, L1pt

	vmulpd	zmm14, zmm14, zmm24		;; r1--s = r1-- * sine46				; 26-29		n 31
	vmulpd	zmm6, zmm6, zmm24		;; i1--s = i1-- * sine46				; 26-29		n 31
	L1prefetchw srcreg+d4+64+L1pd, L1pt

	vmulpd	zmm13, zmm13, zmm25		;; A5 = A5 * sine (final R5)				; 27-30		n 33
	vmulpd	zmm17, zmm17, zmm25		;; B5 = B5 * sine (final I5)				; 27-30		n 35
	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	zfmsubpd zmm16, zmm7, zmm26, zmm5	;; R3s * cosine/sine - I3s (final R3)			; 28-31		n 37
	zfmaddpd zmm5, zmm5, zmm26, zmm7	;; I3s * cosine/sine + R3s (final I3)			; 28-31		n 39
	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt

	zfmaddpd zmm7, zmm15, zmm26, zmm11	;; R7s * cosine/sine + I7s (final R7)			; 29-32		n 37
	zfmsubpd zmm11, zmm11, zmm26, zmm15	;; I7s * cosine/sine - R7s (final I7)			; 29-32		n 39
	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	zfnmaddpd zmm9, zmm19, zmm23, zmm4	;; R2s = (r1-+s) - sine28*(i3-+)			; 30-33		n 35
	zfmaddpd zmm1, zmm2, zmm23, zmm0	;; I2s = (i1-+s) + sine28*(r3-+)			; 30-33		n 35
	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt

	zfnmaddpd zmm18, zmm8, zmm24, zmm14	;; R6s = (r1--s) - sine46*(i3--)			; 31-34		n 36
	zfmaddpd zmm15, zmm12, zmm24, zmm6	;; I6s = (i1--s) + sine46*(r3--)			; 31-34		n 36
	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	zfmaddpd zmm8, zmm8, zmm24, zmm14	;; R4s = (r1--s) + sine46*(i3--)			; 32-35		n 39
	zfnmaddpd zmm12, zmm12, zmm24, zmm6	;; I4s = (i1--s) - sine46*(r3--)			; 32-35		n 39
	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt

	;; Swap the four aparts
	vshuff64x2 zmm14, zmm3, zmm13, 01000100b;; R5_3 R5_2 R5_1 R5_0 R1_3 R1_2 R1_1 R1_0		; 33-35		n 43
	vshuff64x2 zmm3, zmm3, zmm13, 11101110b	;; R5_7	R5_6 R5_5 R5_4 R1_7 R1_6 R1_5 R1_4		; 34-36		n 43
	bump	screg, scinc

	zfmaddpd zmm19, zmm19, zmm23, zmm4	;; R8s = (r1-+s) + sine28*(i3-+)			; 33-36		n 40
	zfnmaddpd zmm2, zmm2, zmm23, zmm0	;; I8s = (i1-+s) - sine28*(r3-+)			; 34-37		n 40

	vshuff64x2 zmm0, zmm10, zmm17, 01000100b;; I5_3 I5_2 I5_1 I5_0 I1_3 I1_2 I1_1 I1_0		; 35-37		n 47
	vshuff64x2 zmm10, zmm10, zmm17, 11101110b;; I5_7 I5_6 I5_5 I5_4 I1_7 I1_6 I1_5 I1_4		; 36-38		n 47

	zfmsubpd zmm4, zmm9, zmm27, zmm1	;; R2s * cosine/sine - I2s (final R2)			; 35-38		n 41
	zfmaddpd zmm6, zmm18, zmm28, zmm15	;; R6s * cosine/sine + I6s (final R6)			; 36-39		n 41

	vshuff64x2 zmm17, zmm16, zmm7, 01000100b;; R7_3 R7_2 R7_1 R7_0 R3_3 R3_2 R3_1 R3_0		; 37-39		n 44
	vshuff64x2 zmm16, zmm16, zmm7, 11101110b;; R7_7 R7_6 R7_5 R7_4 R3_7 R3_6 R3_5 R3_4		; 38-40		n 44

	zfmaddpd zmm1, zmm1, zmm27, zmm9	;; I2s * cosine/sine + R2s (final I2)			; 37-40		n 43
	zfmsubpd zmm15, zmm15, zmm28, zmm18	;; I6s * cosine/sine - R6s (final I6)			; 38-41		n 43

	vshuff64x2 zmm7, zmm5, zmm11, 01000100b	;; I7_3 I7_2 I7_1 I7_0 I3_3 I3_2 I3_1 I3_0		; 39-41		n 48
	vshuff64x2 zmm5, zmm5, zmm11, 11101110b	;; I7_7	I7_6 I7_5 I7_4 I3_7 I3_6 I3_5 I3_4		; 40-42		n 48

	zfmsubpd zmm11, zmm8, zmm28, zmm12	;; R4s * cosine/sine - I4s (final R4)			; 39-42		n 45
	zfmaddpd zmm9, zmm19, zmm27, zmm2	;; R8s * cosine/sine + I8s (final R8)			; 40-43		n 45

	vshuff64x2 zmm18, zmm4, zmm6, 01000100b	;; R6_3 R6_2 R6_1 R6_0 R2_3 R2_2 R2_1 R2_0		; 41-43		n 51
	vshuff64x2 zmm4, zmm4, zmm6, 11101110b	;; R6_7	R6_6 R6_5 R6_4 R2_7 R2_6 R2_5 R2_4		; 42-44		n 51

	zfmaddpd zmm12, zmm12, zmm28, zmm8	;; I4s * cosine/sine + R4s (final I4)			; 41-44		n 47
	zfmsubpd zmm2, zmm2, zmm27, zmm19	;; I8s * cosine/sine - R8s (final I8)			; 42-45		n 47

	vshuff64x2 zmm6, zmm1, zmm15, 01000100b	;; I6_3 I6_2 I6_1 I6_0 I2_3 I2_2 I2_1 I2_0		; 43-45		n 53
	vshuff64x2 zmm1, zmm1, zmm15, 11101110b	;; I6_7	I6_6 I6_5 I6_4 I2_7 I2_6 I2_5 I2_4		; 44-46		n 53

	vaddpd	zmm8, zmm14, zmm3		;; add r1/r5 4-aparts					; 43-46		n 49
	vaddpd	zmm15, zmm17, zmm16		;; add r3/r7 4-aparts					; 44-47		n 49

	vshuff64x2 zmm13, zmm11, zmm9, 01000100b;; R8_3 R8_2 R8_1 R8_0 R4_3 R4_2 R4_1 R4_0		; 45-47		n 52
	vshuff64x2 zmm11, zmm11, zmm9, 11101110b;; R8_7 R8_6 R8_5 R8_4 R4_7 R4_6 R4_5 R4_4		; 46-48		n 52

	vsubpd	zmm14, zmm14, zmm3		;; sub r1/r5 4-aparts					; 45-48		n 51
	vsubpd	zmm17, zmm17, zmm16		;; sub r3/r7 4-aparts					; 46-49		n 51

	vshuff64x2 zmm9, zmm12, zmm2, 01000100b	;; I8_3 I8_2 I8_1 I8_0 I4_3 I4_2 I4_1 I4_0		; 47-49		n 54
	vshuff64x2 zmm12, zmm12, zmm2, 11101110b;; I8_7	I8_6 I8_5 I8_4 I4_7 I4_6 I4_5 I4_4		; 48-50		n 54

	vaddpd	zmm3, zmm0, zmm10		;; add i1/i5 4-aparts					; 47-50		n 53
	vaddpd	zmm16, zmm7, zmm5		;; add i3/i7 4-aparts					; 48-51		n 53

	;; Swap the two aparts
	vshuff64x2 zmm2, zmm8, zmm15, 10001000b	;; R7_1 R7_0 R3_1 R3_0 R5_1 R5_0 R1_1 R1_0 (r15/37+)	; 49-51		n 61
	vshuff64x2 zmm8, zmm8, zmm15, 11011101b	;; R7_3 R7_2 R3_3 R3_2 R5_3 R5_2 R1_3 R1_2		; 50-52		n 62

	vsubpd	zmm0, zmm0, zmm10		;; sub i1/i5 4-aparts					; 49-52		n 55
	vsubpd	zmm7, zmm7, zmm5		;; sub i3/i7 4-aparts					; 50-53		n 55

	vshuff64x2 zmm15, zmm14, zmm17, 10001000b;; R7_5 R7_4 R3_5 R3_4 R5_5 R5_4 R1_5 R1_4 (r15/37-)	; 51-53		n 73
	vshuff64x2 zmm14, zmm14, zmm17, 11011101b;; R7_7 R7_6 R3_7 R3_6 R5_7 R5_6 R1_7 R1_6		; 52-54		n 76

	vaddpd	zmm10, zmm18, zmm4		;; add r2/r6 4-aparts					; 51-54		n 57
	vaddpd	zmm5, zmm13, zmm11		;; add r4/r8 4-aparts					; 52-55		n 57

	vshuff64x2 zmm17, zmm3, zmm16, 10001000b;; I7_1 I7_0 I3_1 I3_0 I5_1 I5_0 I1_1 I1_0 (i15/37+)	; 53-55		n 65
	vshuff64x2 zmm3, zmm3, zmm16, 11011101b	;; I7_3 I7_2 I3_3 I3_2 I5_3 I5_2 I1_3 I1_2		; 54-56		n 66

	vaddpd	zmm16, zmm6, zmm1		;; add i2/i6 4-aparts					; 53-56		n 59  
	vaddpd	zmm19, zmm9, zmm12		;; add i4/i8 4-aparts					; 54-57		n 59

	vshuff64x2 zmm20, zmm0, zmm7, 10001000b	;; I7_5 I7_4 I3_5 I3_4 I5_5 I5_4 I1_5 I1_4 (i15/37-)	; 55-57		n 75
	vshuff64x2 zmm0, zmm0, zmm7, 11011101b	;; I7_7 I7_6 I3_7 I3_6 I5_7 I5_6 I1_7 I1_6		; 56-58		n 74

	vsubpd	zmm18, zmm18, zmm4		;; sub r2/r6 4-aparts					; 55-58		n 69
	vsubpd	zmm13, zmm13, zmm11		;; sub r4/r8 4-aparts					; 56-59		n 69

	vshuff64x2 zmm7, zmm10, zmm5, 10001000b	;; R8_1 R8_0 R4_1 R4_0 R6_1 R6_0 R2_1 R2_0 (r26/48+)	; 57-59		n 61
	vshuff64x2 zmm10, zmm10, zmm5, 11011101b;; R8_3 R8_2 R4_3 R4_2 R6_3 R6_2 R2_3 R2_2		; 58-60		n 62

	vsubpd	zmm6, zmm6, zmm1		;; sub i2/i6 4-aparts					; 57-60		n 71
	vsubpd	zmm9, zmm9, zmm12		;; sub i4/i8 4-aparts					; 58-61		n 71

	vshuff64x2 zmm4, zmm16, zmm19, 10001000b;; I8_1 I8_0 I4_1 I4_0 I6_1 I6_0 I2_1 I2_0 (i26/48+)	; 59-61		n 65
	vshuff64x2 zmm16, zmm16, zmm19, 11011101b;; I8_3 I8_2 I4_3 I4_2 I6_3 I6_2 I2_3 I2_2		; 60-62		n 66

	;; Swap the one aparts
	vshufpd	zmm11, zmm2, zmm7, 00000000b	;; R8_0 R7_0 R4_0 R3_0 R6_0 R5_0 R2_0 R1_0 (R1+R5 = new R1) ; 61	n 63
	vshufpd	zmm5, zmm8, zmm10, 00000000b	;; R8_2 R7_2 R4_2 R3_2 R6_2 R5_2 R2_2 R1_2 (R3+R7 = new R3) ; 62	n 63
	vshufpd	zmm2, zmm2, zmm7, 11111111b	;; R8_1 R7_1 R4_1 R3_1 R6_1 R5_1 R2_1 R1_1 (R2+R6=  new R2) ; 63	n 65
	vshufpd	zmm8, zmm8, zmm10, 11111111b	;; R8_3 R7_3 R4_3 R3_3 R6_3 R5_3 R2_3 R1_3 (R4+R8 = new R4) ; 64	n 65

	vaddpd	zmm19, zmm11, zmm5		;; R1 + R3 (newer R1)					; 63-66		n 73
	vsubpd	zmm11, zmm11, zmm5		;; R1 - R3 (newer R3)					; 64-67		n 84

	vshufpd	zmm7, zmm17, zmm4, 00000000b	;; I8_0 I7_0 I4_0 I3_0 I6_0 I5_0 I2_0 I1_0 (I1+I5 = new I1) ; 65	n 67
	vshufpd	zmm10, zmm3, zmm16, 00000000b	;; I8_2 I7_2 I4_2 I3_2 I6_2 I5_2 I2_2 I1_2 (I3+I7 = new I3) ; 66	n 67

	vaddpd	zmm5, zmm2, zmm8		;; R2 + R4 (newer R2)					; 65-68		n 73
	vsubpd	zmm2, zmm2, zmm8		;; R2 - R4 (newer R4)					; 66-69		n 86

	vshufpd	zmm17, zmm17, zmm4, 11111111b	;; I8_1 I7_1 I4_1 I3_1 I6_1 I5_1 I2_1 I1_1 (I2+I6 = new I2) ; 67	n 69
	vshufpd	zmm3, zmm3, zmm16, 11111111b	;; I8_3 I7_3 I4_3 I3_3 I6_3 I5_3 I2_3 I1_3 (I4+I8 = new I4) ; 68	n 69

	vaddpd	zmm8, zmm7, zmm10		;; I1 + I3 (newer I1)					; 67-70		n 73
	vsubpd	zmm7, zmm7, zmm10		;; I1 - I3 (newer I3)					; 68-71		n 86

	vshuff64x2 zmm4, zmm18, zmm13, 10001000b;; R8_5 R8_4 R4_5 R4_4 R6_5 R6_4 R2_5 R2_4 (r26/48-)	; 69-71		n 73
	vshuff64x2 zmm18, zmm18, zmm13, 11011101b;; R8_7 R8_6 R4_7 R4_6 R6_7 R6_6 R2_7 R2_6		; 70-72		n 76

	vaddpd	zmm16, zmm17, zmm3		;; I2 + I4 (newer I2)					; 69-72		n 73
	vsubpd	zmm17, zmm17, zmm3		;; I2 - I4 (newer I4)					; 70-73		n 84

	vshuff64x2 zmm10, zmm6, zmm9, 11011101b	;; I8_7	I8_6 I4_7 I4_6 I6_7 I6_6 I2_7 I2_6 (i26/48-)	; 71-73		n 74
	vshuff64x2 zmm6, zmm6, zmm9, 10001000b	;; I8_5 I8_4 I4_5 I4_4 I6_5 I6_4 I2_5 I2_4		; 72-74		n 75

	vaddpd	zmm13, zmm19, zmm5		;; R1 + R2 (final R1)					; 71-74
	vsubpd	zmm19, zmm19, zmm5		;; R1 - R2 (final R2)					; 72-75

	vshufpd	zmm3, zmm15, zmm4, 11111111b	;; R8_5 R7_5 R4_5 R3_5 R6_5 R5_5 R2_5 R1_5 (R2-R6 = new R6) ; 73	n 75
	vshufpd	zmm9, zmm0, zmm10, 11111111b	;; I8_7	I7_7 I4_7 I3_7 I6_7 I5_7 I2_7 I1_7 (I4-I8 = new I8) ; 74	n 75

	vaddpd	zmm5, zmm8, zmm16		;; I1 + I2 (final I1)					; 73-76
	vsubpd	zmm8, zmm8, zmm16		;; I1 - I2 (final I2)					; 74-77

	vshufpd	zmm16, zmm20, zmm6, 11111111b	;; I8_5 I7_5 I4_5 I3_5 I6_5 I5_5 I2_5 I1_5 (I2-I6 = new I6) ; 75	n 77
	vshufpd	zmm1, zmm14, zmm18, 11111111b	;; R8_7	R7_7 R4_7 R3_7 R6_7 R5_7 R2_7 R1_7 (R4-R8 = new R8) ; 76	n 77

	vsubpd	zmm12, zmm3, zmm9		;; R6 - I8 (new2 R6)					; 75-78		n 82
	vaddpd	zmm3, zmm3, zmm9		;; R6 + I8 (new2 R8)					; 76-79		n 83
	zstore	[srcreg], zmm13			;; Save R1						; 75
	zstore	[srcreg+d1], zmm19		;; Save R2						; 76

	vshufpd	zmm20, zmm20, zmm6, 00000000b	;; I8_4 I7_4 I4_4 I3_4 I6_4 I5_4 I2_4 I1_4 (I1-I5 = new I5) ; 77	n 79
	vshufpd	zmm14, zmm14, zmm18, 00000000b	;; R8_6	R7_6 R4_6 R3_6 R6_6 R5_6 R2_6 R1_6 (R3-R7 = new R7) ; 78	n 79

	vaddpd	zmm9, zmm16, zmm1		;; I6 + R8 (new2 I6)					; 77-80		n 82
	vsubpd	zmm16, zmm16, zmm1		;; I6 - R8 (new2 I8)					; 78-81		n 83
	zstore	[srcreg+64], zmm5		;; Save I1						; 77
	zstore	[srcreg+d1+64], zmm8		;; Save I2						; 78

	vshufpd	zmm15, zmm15, zmm4, 00000000b	;; R8_4 R7_4 R4_4 R3_4 R6_4 R5_4 R2_4 R1_4 (R1-R5 = new R5) ; 79	n 81
	vshufpd	zmm0, zmm0, zmm10, 00000000b	;; I8_6	I7_6 I4_6 I3_6 I6_6 I5_6 I2_6 I1_6 (I3-I7 = new I7) ; 80	n 81

	vaddpd	zmm6, zmm20, zmm14		;; I5 + R7 (newer I5)					; 79-82		n 86
	vsubpd	zmm20, zmm20, zmm14		;; I5 - R7 (newer I7)					; 80-83		n 87

	vsubpd	zmm18, zmm15, zmm0		;; R5 - I7 (newer R5)					; 81-83		n 88
	vaddpd	zmm15, zmm15, zmm0		;; R5 + I7 (newer R7)					; 81-84		n 89

	vaddpd	zmm4, zmm12, zmm9		;; I6 = R6 + I6 (newer I6/SQRTHALF)			; 82-85		n 86
	vsubpd	zmm12, zmm12, zmm9		;; R6 = R6 - I6 (newer R6/SQRTHALF)			; 82-85		n 88

	vsubpd	zmm10, zmm3, zmm16		;; R8 = R8 - I8 (newer R8/SQRTHALF)			; 83-86		n 87
	vaddpd	zmm3, zmm3, zmm16		;; I8 = R8 + I8 (newer I8/SQRTHALF)			; 83-86		n 89

	vsubpd	zmm14, zmm11, zmm17		;; R3 - I4 (final R3)					; 84-87
	vaddpd	zmm11, zmm11, zmm17		;; R3 + I4 (final R4)					; 84-87

	vaddpd	zmm0, zmm7, zmm2		;; I3 + R4 (final I3)					; 85-88
	vsubpd	zmm7, zmm7, zmm2		;; I3 - R4 (final I4)					; 85-88

	zfmaddpd zmm9, zmm4, zmm31, zmm6	;; I5 + I6 * SQRTHALF (final I5)			; 86-89
	zfnmaddpd zmm4, zmm4, zmm31, zmm6	;; I5 - I6 * SQRTHALF (final I6)			; 86-89

	zfmaddpd zmm2, zmm10, zmm31, zmm20	;; I7 + R8 * SQRTHALF (final I7)			; 87-90
	zfnmaddpd zmm10, zmm10, zmm31, zmm20	;; I7 - R8 * SQRTHALF (final I8)			; 87-90

	zfmaddpd zmm6, zmm12, zmm31, zmm18	;; R5 + R6 * SQRTHALF (final R5)			; 88-91
	zfnmaddpd zmm12, zmm12, zmm31, zmm18	;; R5 - R6 * SQRTHALF (final R6)			; 88-91
	zstore	[srcreg+d2], zmm14		;; Save R3						; 88

	zfnmaddpd zmm16, zmm3, zmm31, zmm15	;; R7 - I8 * SQRTHALF (final R7)			; 89-92
	zfmaddpd zmm3, zmm3, zmm31, zmm15	;; R7 + I8 * SQRTHALF (final R8)			; 89-92
	zstore	[srcreg+d2+d1], zmm11		;; Save R4						; 88+1

	zstore	[srcreg+d2+64], zmm0		;; Save I3						; 89+1
	zstore	[srcreg+d2+d1+64], zmm7		;; Save I4						; 89+2
	zstore	[srcreg+d4+64], zmm9		;; Save I5						; 90+2
	zstore	[srcreg+d4+d1+64], zmm4		;; Save I6						; 90+3
	zstore	[srcreg+d4+d2+64], zmm2		;; Save I7						; 91+3
	zstore	[srcreg+d4+d2+d1+64], zmm10	;; Save I8						; 91+4
	zstore	[srcreg+d4], zmm6		;; Save R5						; 92+4
	zstore	[srcreg+d4+d1], zmm12		;; Save R6						; 92+5
	zstore	[srcreg+d4+d2], zmm16		;; Save R7						; 93+5
	zstore	[srcreg+d4+d2+d1], zmm3		;; Save R8						; 93+6
	bump	srcreg, srcinc
	ENDM


zr64_sixtyfour_complex_with_square_preload MACRO
	zr64_64c_square_cmn_preload
	ENDM
zr64_sixtyfour_complex_with_square MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	zr64_64c_square_cmn srcreg,0,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	ENDM
zr64f_sixtyfour_complex_with_square_preload MACRO
	zr64_64c_square_cmn_preload
	ENDM
zr64f_sixtyfour_complex_with_square MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	zr64_64c_square_cmn srcreg,rbx,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

zr64_64c_square_cmn_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRTHALF
	vbroadcastsd zmm30, ZMM_TWO
	ENDM
zr64_64c_square_cmn MACRO srcreg,srcoff,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	LOCAL	orig, back_to_orig

	vmovapd	zmm0, [srcreg+srcoff]			;; R1
	vmovapd	zmm20, [srcreg+srcoff+d4]		;; R5
	vaddpd	zmm15, zmm0, zmm20			;; R1+R5					; 1-4		n 9
	vsubpd	zmm0, zmm0, zmm20			;; R1-R5					; 1-4		n 18

	vmovapd	zmm16, [srcreg+srcoff+d2]		;; R3
	vmovapd	zmm20, [srcreg+srcoff+d4+d2]		;; R7
	vaddpd	zmm1, zmm16, zmm20			;; R3+R7					; 2-5		n 9
	vsubpd	zmm16, zmm16, zmm20			;; R3-R7					; 2-5		n 23

	vmovapd	zmm14, [srcreg+srcoff+d1]		;; R2
	vmovapd	zmm20, [srcreg+srcoff+d4+d1]		;; R6
	vaddpd	zmm13, zmm14, zmm20			;; R2+R6					; 3-6		n 10
	vsubpd	zmm14, zmm14, zmm20			;; R2-R6					; 3-6		n 13

	vmovapd	zmm6, [srcreg+srcoff+d2+d1]		;; R4
	vmovapd	zmm20, [srcreg+srcoff+d4+d2+d1]		;; R8
	vaddpd	zmm2, zmm6, zmm20			;; R4+R8					; 4-7		n 10
	vsubpd	zmm6, zmm6, zmm20			;; R4-R8					; 4-7		n 13

	vmovapd	zmm7, [srcreg+srcoff+d2+d1+64]		;; I4
	vmovapd	zmm20, [srcreg+srcoff+d4+d2+d1+64]	;; I8
	vaddpd	zmm10, zmm7, zmm20			;; I4+I8					; 5-8		n 11
	vsubpd	zmm7, zmm7, zmm20			;; I4-I8					; 5-8		n 14

	vmovapd	zmm8, [srcreg+srcoff+d1+64]		;; I2
	vmovapd	zmm20, [srcreg+srcoff+d4+d1+64]		;; I6
	vaddpd	zmm4, zmm8, zmm20			;; I2+I6					; 6-9		n 11
	vsubpd	zmm8, zmm8, zmm20			;; I2-I6					; 6-9		n 14

	vmovapd	zmm5, [srcreg+srcoff+64]		;; I1
	vmovapd	zmm20, [srcreg+srcoff+d4+64]		;; I5
	vaddpd	zmm11, zmm5, zmm20			;; I1+I5					; 7-10		n 12
	vsubpd	zmm5, zmm5, zmm20			;; I1-I5					; 7-10		n 19

	vmovapd	zmm9, [srcreg+srcoff+d2+64]		;; I3
	vmovapd	zmm20, [srcreg+srcoff+d4+d2+64]		;; I7
	vaddpd	zmm12, zmm9, zmm20			;; I3+I7					; 8-11		n 12
	vsubpd	zmm9, zmm9, zmm20			;; I3-I7					; 8-11		n 24

	vaddpd	zmm3, zmm15, zmm1		;; r1++ = (r1+r5) + (r3+r7)				; 9-12		n 15
	vsubpd	zmm15, zmm15, zmm1		;; r1+- = (r1+r5) - (r3+r7)				; 9-12		n 21
	vmovapd zmm22, [screg+1*128]		;; sine for R3/I3 and R7/I7

	vsubpd	zmm1, zmm13, zmm2		;; r2+- = (r2+r6) - (r4+r8)				; 10-13		n 17
	vaddpd	zmm13, zmm13, zmm2		;; r2++ = (r2+r6) + (r4+r8)				; 10-13		n 15
	vmovapd zmm29, [screg+3*128+64]		;; cosine/sine for R5/I5

	vsubpd	zmm2, zmm4, zmm10		;; i2+- = (i2+i6) - (i4+i8)				; 11-14		n 17
	vaddpd	zmm4, zmm4, zmm10		;; i2++ = (i2+i6) + (i4+i8)				; 11-14		n 16
	vmovapd zmm23, [screg+0*128]		;; sine for R2/I2 and R8/I8

	vaddpd	zmm10, zmm11, zmm12		;; i1++ = (i1+i5) + (i3+i7)				; 12-15		n 16
	vsubpd	zmm11, zmm11, zmm12		;; i1+- = (i1+i5) - (i3+i7)				; 12-15		n 21
	vmovapd zmm24, [screg+2*128]		;; sine for R4/I4 and R6/I6

	vaddpd	zmm12, zmm14, zmm6		;; r2-+ = (r2-r6) + (r4-r8)				; 13-16		n 23
	vsubpd	zmm14, zmm14, zmm6		;; r2-- = (r2-r6) - (r4-r8)				; 13-16		n 18
	vmovapd zmm25, [screg+3*128]		;; sine for R5/I5

	vsubpd	zmm6, zmm8, zmm7		;; i2-- = (i2-i6) - (i4-i8)				; 14-17		n 19
	vaddpd	zmm8, zmm8, zmm7		;; i2-+ = (i2-i6) + (i4-i8)				; 14-17		n 24
	vmovapd zmm26, [screg+1*128+64]		;; cosine/sine for R3/I3 and R7/I7

	vsubpd	zmm7, zmm3, zmm13		;; R5 = (r1++) - (r2++)					; 15-18		n 20
	vaddpd	zmm3, zmm3, zmm13		;; R1 = (r1++) + (r2++)					; 15-18		n 33
	vmovapd zmm27, [screg+0*128+64]		;; cosine/sine for R2/I2 and R8/I8

	vsubpd	zmm17, zmm10, zmm4		;; I5 = (i1++) - (i2++)					; 16-19		n 20
	vaddpd	zmm10, zmm10, zmm4		;; I1 = (i1++) + (i2++)					; 16-19		n 35
	vmovapd zmm28, [screg+2*128+64]		;; cosine/sine for R4/I4 and R6/I6

	vmulpd	zmm1, zmm1, zmm22		;; r2+-s = r2+- * sine37				; 17-20		n 21
	vmulpd	zmm2, zmm2, zmm22		;; i2+-s = i2+- * sine37				; 17-20		n 21
	L1prefetchw srcreg+L1pd, L1pt

	zfmaddpd zmm4, zmm14, zmm31, zmm0	;; r1-+ = (r1-r5) + .707*(r2--)				; 18-21		n 25
	zfnmaddpd zmm14, zmm14, zmm31, zmm0	;; r1-- = (r1-r5) - .707*(r2--)				; 18-21		n 26
	L1prefetchw srcreg+64+L1pd, L1pt

	zfmaddpd zmm0, zmm6, zmm31, zmm5	;; i1-+ = (i1-i5) + .707*(i2--)				; 19-22		n 25
	zfnmaddpd zmm6, zmm6, zmm31, zmm5	;; i1-- = (i1-i5) - .707*(i2--)				; 19-22		n 26
	L1prefetchw srcreg+d1+L1pd, L1pt

	zfmsubpd zmm13, zmm7, zmm29, zmm17	;; A5 = R5 * cosine/sine - I5				; 20-23		n 27
	zfmaddpd zmm17, zmm17, zmm29, zmm7	;; B5 = I5 * cosine/sine + R5				; 20-23		n 27
	L1prefetchw srcreg+d1+64+L1pd, L1pt

	zfmsubpd zmm7, zmm15, zmm22, zmm2	;; R3s = (r1+-)*sine37 - (i2+-s)			; 21-24		n 28
	zfmaddpd zmm5, zmm11, zmm22, zmm1	;; I3s = (i1+-)*sine37 + (r2+-s)			; 21-24		n 28
	L1prefetchw srcreg+d2+L1pd, L1pt

	zfmaddpd zmm15, zmm15, zmm22, zmm2	;; R7s = (r1+-)*sine37 + (i2+-s)			; 22-25		n 29
	zfmsubpd zmm11, zmm11, zmm22, zmm1	;; I7s = (i1+-)*sine37 - (r2+-s)			; 22-25		n 29
	L1prefetchw srcreg+d2+64+L1pd, L1pt

	zfmaddpd zmm2, zmm12, zmm31, zmm16	;; r3-+ = (r3-r7) + .707*(r2-+)				; 23-26		n 30
	zfnmaddpd zmm12, zmm12, zmm31, zmm16	;; r3-- = (r3-r7) - .707*(r2-+)				; 23-26		n 31
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	zfmaddpd zmm19, zmm8, zmm31, zmm9	;; i3-+ = (i3-i7) + .707*(i2-+)				; 24-27		n 30
	zfnmaddpd zmm8, zmm8, zmm31, zmm9	;; i3-- = (i3-i7) - .707*(i2-+)				; 24-27		n 31
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt

	vmulpd	zmm4, zmm4, zmm23		;; r1-+s = r1-+ * sine28				; 25-28		n 30
	vmulpd	zmm0, zmm0, zmm23		;; i1-+s = i1-+ * sine28				; 25-28		n 30
	L1prefetchw srcreg+d4+L1pd, L1pt

	vmulpd	zmm14, zmm14, zmm24		;; r1--s = r1-- * sine46				; 26-29		n 31
	vmulpd	zmm6, zmm6, zmm24		;; i1--s = i1-- * sine46				; 26-29		n 31
	L1prefetchw srcreg+d4+64+L1pd, L1pt

	vmulpd	zmm13, zmm13, zmm25		;; A5 = A5 * sine (final R5)				; 27-30		n 33
	vmulpd	zmm17, zmm17, zmm25		;; B5 = B5 * sine (final I5)				; 27-30		n 35
	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	zfmsubpd zmm16, zmm7, zmm26, zmm5	;; R3s * cosine/sine - I3s (final R3)			; 28-31		n 37
	zfmaddpd zmm5, zmm5, zmm26, zmm7	;; I3s * cosine/sine + R3s (final I3)			; 28-31		n 39
	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt

	zfmaddpd zmm7, zmm15, zmm26, zmm11	;; R7s * cosine/sine + I7s (final R7)			; 29-32		n 37
	zfmsubpd zmm11, zmm11, zmm26, zmm15	;; I7s * cosine/sine - R7s (final I7)			; 29-32		n 39
	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	zfnmaddpd zmm9, zmm19, zmm23, zmm4	;; R2s = (r1-+s) - sine28*(i3-+)			; 30-33		n 35
	zfmaddpd zmm1, zmm2, zmm23, zmm0	;; I2s = (i1-+s) + sine28*(r3-+)			; 30-33		n 35
	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt

	zfnmaddpd zmm18, zmm8, zmm24, zmm14	;; R6s = (r1--s) - sine46*(i3--)			; 31-34		n 36
	zfmaddpd zmm15, zmm12, zmm24, zmm6	;; I6s = (i1--s) + sine46*(r3--)			; 31-34		n 36
	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	zfmaddpd zmm8, zmm8, zmm24, zmm14	;; R4s = (r1--s) + sine46*(i3--)			; 32-35		n 39
	zfnmaddpd zmm12, zmm12, zmm24, zmm6	;; I4s = (i1--s) - sine46*(r3--)			; 32-35		n 39
	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt

	;; Swap the four aparts
	vshuff64x2 zmm14, zmm3, zmm13, 01000100b;; R5_3 R5_2 R5_1 R5_0 R1_3 R1_2 R1_1 R1_0		; 33-35		n 43
	vshuff64x2 zmm3, zmm3, zmm13, 11101110b	;; R5_7	R5_6 R5_5 R5_4 R1_7 R1_6 R1_5 R1_4		; 34-36		n 43
	bump	screg, scinc

	zfmaddpd zmm19, zmm19, zmm23, zmm4	;; R8s = (r1-+s) + sine28*(i3-+)			; 33-36		n 40
	zfnmaddpd zmm2, zmm2, zmm23, zmm0	;; I8s = (i1-+s) - sine28*(r3-+)			; 34-37		n 40

	vshuff64x2 zmm0, zmm10, zmm17, 01000100b;; I5_3 I5_2 I5_1 I5_0 I1_3 I1_2 I1_1 I1_0		; 35-37		n 47
	vshuff64x2 zmm10, zmm10, zmm17, 11101110b;; I5_7 I5_6 I5_5 I5_4 I1_7 I1_6 I1_5 I1_4		; 36-38		n 47

	zfmsubpd zmm4, zmm9, zmm27, zmm1	;; R2s * cosine/sine - I2s (final R2)			; 35-38		n 41
	zfmaddpd zmm6, zmm18, zmm28, zmm15	;; R6s * cosine/sine + I6s (final R6)			; 36-39		n 41

	vshuff64x2 zmm17, zmm16, zmm7, 01000100b;; R7_3 R7_2 R7_1 R7_0 R3_3 R3_2 R3_1 R3_0		; 37-39		n 44
	vshuff64x2 zmm16, zmm16, zmm7, 11101110b;; R7_7 R7_6 R7_5 R7_4 R3_7 R3_6 R3_5 R3_4		; 38-40		n 44

	zfmaddpd zmm1, zmm1, zmm27, zmm9	;; I2s * cosine/sine + R2s (final I2)			; 37-40		n 43
	zfmsubpd zmm15, zmm15, zmm28, zmm18	;; I6s * cosine/sine - R6s (final I6)			; 38-41		n 43

	vshuff64x2 zmm7, zmm5, zmm11, 01000100b	;; I7_3 I7_2 I7_1 I7_0 I3_3 I3_2 I3_1 I3_0		; 39-41		n 48
	vshuff64x2 zmm5, zmm5, zmm11, 11101110b	;; I7_7	I7_6 I7_5 I7_4 I3_7 I3_6 I3_5 I3_4		; 40-42		n 48

	zfmsubpd zmm11, zmm8, zmm28, zmm12	;; R4s * cosine/sine - I4s (final R4)			; 39-42		n 45
	zfmaddpd zmm9, zmm19, zmm27, zmm2	;; R8s * cosine/sine + I8s (final R8)			; 40-43		n 45

	vshuff64x2 zmm18, zmm4, zmm6, 01000100b	;; R6_3 R6_2 R6_1 R6_0 R2_3 R2_2 R2_1 R2_0		; 41-43		n 51
	vshuff64x2 zmm4, zmm4, zmm6, 11101110b	;; R6_7	R6_6 R6_5 R6_4 R2_7 R2_6 R2_5 R2_4		; 42-44		n 51

	zfmaddpd zmm12, zmm12, zmm28, zmm8	;; I4s * cosine/sine + R4s (final I4)			; 41-44		n 47
	zfmsubpd zmm2, zmm2, zmm27, zmm19	;; I8s * cosine/sine - R8s (final I8)			; 42-45		n 47

	vshuff64x2 zmm6, zmm1, zmm15, 01000100b	;; I6_3 I6_2 I6_1 I6_0 I2_3 I2_2 I2_1 I2_0		; 43-45		n 53
	vshuff64x2 zmm1, zmm1, zmm15, 11101110b	;; I6_7	I6_6 I6_5 I6_4 I2_7 I2_6 I2_5 I2_4		; 44-46		n 53

	vaddpd	zmm8, zmm14, zmm3		;; add r1/r5 4-aparts					; 43-46		n 49
	vaddpd	zmm15, zmm17, zmm16		;; add r3/r7 4-aparts					; 44-47		n 49

	vshuff64x2 zmm13, zmm11, zmm9, 01000100b;; R8_3 R8_2 R8_1 R8_0 R4_3 R4_2 R4_1 R4_0		; 45-47		n 52
	vshuff64x2 zmm11, zmm11, zmm9, 11101110b;; R8_7 R8_6 R8_5 R8_4 R4_7 R4_6 R4_5 R4_4		; 46-48		n 52

	vsubpd	zmm14, zmm14, zmm3		;; sub r1/r5 4-aparts					; 45-48		n 51
	vsubpd	zmm17, zmm17, zmm16		;; sub r3/r7 4-aparts					; 46-49		n 51

	vshuff64x2 zmm9, zmm12, zmm2, 01000100b	;; I8_3 I8_2 I8_1 I8_0 I4_3 I4_2 I4_1 I4_0		; 47-49		n 54
	vshuff64x2 zmm12, zmm12, zmm2, 11101110b;; I8_7	I8_6 I8_5 I8_4 I4_7 I4_6 I4_5 I4_4		; 48-50		n 54

	vaddpd	zmm3, zmm0, zmm10		;; add i1/i5 4-aparts					; 47-50		n 53
	vaddpd	zmm16, zmm7, zmm5		;; add i3/i7 4-aparts					; 48-51		n 53

	;; Swap the two aparts
	vshuff64x2 zmm2, zmm8, zmm15, 10001000b	;; R7_1 R7_0 R3_1 R3_0 R5_1 R5_0 R1_1 R1_0 (r15/37+)	; 49-51		n 61
	vshuff64x2 zmm8, zmm8, zmm15, 11011101b	;; R7_3 R7_2 R3_3 R3_2 R5_3 R5_2 R1_3 R1_2		; 50-52		n 62

	vsubpd	zmm0, zmm0, zmm10		;; sub i1/i5 4-aparts					; 49-52		n 55
	vsubpd	zmm7, zmm7, zmm5		;; sub i3/i7 4-aparts					; 50-53		n 55

	vshuff64x2 zmm15, zmm14, zmm17, 10001000b;; R7_5 R7_4 R3_5 R3_4 R5_5 R5_4 R1_5 R1_4 (r15/37-)	; 51-53		n 73
	vshuff64x2 zmm14, zmm14, zmm17, 11011101b;; R7_7 R7_6 R3_7 R3_6 R5_7 R5_6 R1_7 R1_6		; 52-54		n 76

	vaddpd	zmm10, zmm18, zmm4		;; add r2/r6 4-aparts					; 51-54		n 57
	vaddpd	zmm5, zmm13, zmm11		;; add r4/r8 4-aparts					; 52-55		n 57

	vshuff64x2 zmm17, zmm3, zmm16, 10001000b;; I7_1 I7_0 I3_1 I3_0 I5_1 I5_0 I1_1 I1_0 (i15/37+)	; 53-55		n 65
	vshuff64x2 zmm3, zmm3, zmm16, 11011101b	;; I7_3 I7_2 I3_3 I3_2 I5_3 I5_2 I1_3 I1_2		; 54-56		n 66

	vaddpd	zmm16, zmm6, zmm1		;; add i2/i6 4-aparts					; 53-56		n 59  
	vaddpd	zmm19, zmm9, zmm12		;; add i4/i8 4-aparts					; 54-57		n 59

	vshuff64x2 zmm20, zmm0, zmm7, 10001000b	;; I7_5 I7_4 I3_5 I3_4 I5_5 I5_4 I1_5 I1_4 (i15/37-)	; 55-57		n 75
	vshuff64x2 zmm0, zmm0, zmm7, 11011101b	;; I7_7 I7_6 I3_7 I3_6 I5_7 I5_6 I1_7 I1_6		; 56-58		n 74

	vsubpd	zmm18, zmm18, zmm4		;; sub r2/r6 4-aparts					; 55-58		n 69
	vsubpd	zmm13, zmm13, zmm11		;; sub r4/r8 4-aparts					; 56-59		n 69

	vshuff64x2 zmm7, zmm10, zmm5, 10001000b	;; R8_1 R8_0 R4_1 R4_0 R6_1 R6_0 R2_1 R2_0 (r26/48+)	; 57-59		n 61
	vshuff64x2 zmm10, zmm10, zmm5, 11011101b;; R8_3 R8_2 R4_3 R4_2 R6_3 R6_2 R2_3 R2_2		; 58-60		n 62

	vsubpd	zmm6, zmm6, zmm1		;; sub i2/i6 4-aparts					; 57-60		n 71
	vsubpd	zmm9, zmm9, zmm12		;; sub i4/i8 4-aparts					; 58-61		n 71

	vshuff64x2 zmm4, zmm16, zmm19, 10001000b;; I8_1 I8_0 I4_1 I4_0 I6_1 I6_0 I2_1 I2_0 (i26/48+)	; 59-61		n 65
	vshuff64x2 zmm16, zmm16, zmm19, 11011101b;; I8_3 I8_2 I4_3 I4_2 I6_3 I6_2 I2_3 I2_2		; 60-62		n 66

	;; Swap the one aparts
	vshufpd	zmm11, zmm2, zmm7, 00000000b	;; R8_0 R7_0 R4_0 R3_0 R6_0 R5_0 R2_0 R1_0 (R1+R5 = new R1) ; 61	n 63
	vshufpd	zmm5, zmm8, zmm10, 00000000b	;; R8_2 R7_2 R4_2 R3_2 R6_2 R5_2 R2_2 R1_2 (R3+R7 = new R3) ; 62	n 63
	vshufpd	zmm2, zmm2, zmm7, 11111111b	;; R8_1 R7_1 R4_1 R3_1 R6_1 R5_1 R2_1 R1_1 (R2+R6=  new R2) ; 63	n 65
	vshufpd	zmm8, zmm8, zmm10, 11111111b	;; R8_3 R7_3 R4_3 R3_3 R6_3 R5_3 R2_3 R1_3 (R4+R8 = new R4) ; 64	n 65

	vaddpd	zmm19, zmm11, zmm5		;; R1 + R3 (newer R1)					; 63-66		n 73
	vsubpd	zmm11, zmm11, zmm5		;; R1 - R3 (newer R3)					; 64-67		n 84

	vshufpd	zmm7, zmm17, zmm4, 00000000b	;; I8_0 I7_0 I4_0 I3_0 I6_0 I5_0 I2_0 I1_0 (I1+I5 = new I1) ; 65	n 67
	vshufpd	zmm10, zmm3, zmm16, 00000000b	;; I8_2 I7_2 I4_2 I3_2 I6_2 I5_2 I2_2 I1_2 (I3+I7 = new I3) ; 66	n 67

	vaddpd	zmm5, zmm2, zmm8		;; R2 + R4 (newer R2)					; 65-68		n 71
	vsubpd	zmm2, zmm2, zmm8		;; R2 - R4 (newer R4)					; 66-69		n 86

	vshufpd	zmm17, zmm17, zmm4, 11111111b	;; I8_1 I7_1 I4_1 I3_1 I6_1 I5_1 I2_1 I1_1 (I2+I6 = new I2) ; 67	n 69
	vshufpd	zmm3, zmm3, zmm16, 11111111b	;; I8_3 I7_3 I4_3 I3_3 I6_3 I5_3 I2_3 I1_3 (I4+I8 = new I4) ; 68	n 69

	vaddpd	zmm8, zmm7, zmm10		;; I1 + I3 (newer I1)					; 67-70		n 73
	vsubpd	zmm7, zmm7, zmm10		;; I1 - I3 (newer I3)					; 68-71		n 85

	vshuff64x2 zmm4, zmm18, zmm13, 10001000b;; R8_5 R8_4 R4_5 R4_4 R6_5 R6_4 R2_5 R2_4 (r26/48-)	; 69-71		n 73
	vshuff64x2 zmm18, zmm18, zmm13, 11011101b;; R8_7 R8_6 R4_7 R4_6 R6_7 R6_6 R2_7 R2_6		; 70-72		n 76

	vaddpd	zmm16, zmm17, zmm3		;; I2 + I4 (newer I2)					; 69-72		n 73
	vsubpd	zmm17, zmm17, zmm3		;; I2 - I4 (newer I4)					; 70-73		n 84

	vshuff64x2 zmm10, zmm6, zmm9, 11011101b	;; I8_7	I8_6 I4_7 I4_6 I6_7 I6_6 I2_7 I2_6 (i26/48-)	; 71-73		n 74
	vshuff64x2 zmm6, zmm6, zmm9, 10001000b	;; I8_5 I8_4 I4_5 I4_4 I6_5 I6_4 I2_5 I2_4		; 72-74		n 75

	vaddpd	zmm13, zmm19, zmm5		;; R1 + R2 (final R1)					; 71-74		n 90
	vsubpd	zmm19, zmm19, zmm5		;; R1 - R2 (final R2)					; 72-75		n 96

	vshufpd	zmm3, zmm15, zmm4, 11111111b	;; R8_5 R7_5 R4_5 R3_5 R6_5 R5_5 R2_5 R1_5 (R2-R6 = new R6) ; 73	n 75
	vshufpd	zmm9, zmm0, zmm10, 11111111b	;; I8_7	I7_7 I4_7 I3_7 I6_7 I5_7 I2_7 I1_7 (I4-I8 = new I8) ; 74	n 75

	vaddpd	zmm5, zmm8, zmm16		;; I1 + I2 (final I1)					; 73-76		n 90
	vsubpd	zmm8, zmm8, zmm16		;; I1 - I2 (final I2)					; 74-77		n 100

	vshufpd	zmm16, zmm20, zmm6, 11111111b	;; I8_5 I7_5 I4_5 I3_5 I6_5 I5_5 I2_5 I1_5 (I2-I6 = new I6) ; 75	n 77
	vshufpd	zmm1, zmm14, zmm18, 11111111b	;; R8_7	R7_7 R4_7 R3_7 R6_7 R5_7 R2_7 R1_7 (R4-R8 = new R8) ; 76	n 77

	vsubpd	zmm12, zmm3, zmm9		;; R6 - I8 (new2 R6)					; 75-78		n 82
	vaddpd	zmm3, zmm3, zmm9		;; R6 + I8 (new2 R8)					; 76-79		n 83

	vshufpd	zmm20, zmm20, zmm6, 00000000b	;; I8_4 I7_4 I4_4 I3_4 I6_4 I5_4 I2_4 I1_4 (I1-I5 = new I5) ; 77	n 79
	vshufpd	zmm14, zmm14, zmm18, 00000000b	;; R8_6	R7_6 R4_6 R3_6 R6_6 R5_6 R2_6 R1_6 (R3-R7 = new R7) ; 78	n 79

	vaddpd	zmm9, zmm16, zmm1		;; I6 + R8 (new2 I6)					; 77-80		n 82
	vsubpd	zmm16, zmm16, zmm1		;; I6 - R8 (new2 I8)					; 78-81		n 83

	vshufpd	zmm15, zmm15, zmm4, 00000000b	;; R8_4 R7_4 R4_4 R3_4 R6_4 R5_4 R2_4 R1_4 (R1-R5 = new R5) ; 79	n 81
	vshufpd	zmm0, zmm0, zmm10, 00000000b	;; I8_6	I7_6 I4_6 I3_6 I6_6 I5_6 I2_6 I1_6 (I3-I7 = new I7) ; 80	n 81

	vaddpd	zmm6, zmm20, zmm14		;; I5 + R7 (newer I5)					; 79-82		n 86
	vsubpd	zmm20, zmm20, zmm14		;; I5 - R7 (newer I7)					; 80-83		n 87

	vsubpd	zmm18, zmm15, zmm0		;; R5 - I7 (newer R5)					; 81-83		n 88
	vaddpd	zmm15, zmm15, zmm0		;; R5 + I7 (newer R7)					; 81-84		n 89

	vaddpd	zmm4, zmm12, zmm9		;; I6 = R6 + I6 (newer I6/SQRTHALF)			; 82-85		n 86
	vsubpd	zmm12, zmm12, zmm9		;; R6 = R6 - I6 (newer R6/SQRTHALF)			; 82-85		n 88

	vsubpd	zmm10, zmm3, zmm16		;; R8 = R8 - I8 (newer R8/SQRTHALF)			; 83-86		n 87
	vaddpd	zmm3, zmm3, zmm16		;; I8 = R8 + I8 (newer I8/SQRTHALF)			; 83-86		n 89

	vsubpd	zmm14, zmm11, zmm17		;; R3 - I4 (final R3)					; 84-87		n 91
	vaddpd	zmm11, zmm11, zmm17		;; R3 + I4 (final R4)					; 84-87		n 97

	vaddpd	zmm0, zmm7, zmm2		;; I3 + R4 (final I3)					; 85-88		n 91
	vsubpd	zmm7, zmm7, zmm2		;; I3 - R4 (final I4)					; 85-88		n 101

	zfmaddpd zmm9, zmm4, zmm31, zmm6	;; I5 + I6 * SQRTHALF (final I5)			; 86-89		n 92
	zfnmaddpd zmm4, zmm4, zmm31, zmm6	;; I5 - I6 * SQRTHALF (final I6)			; 86-89		n 94

	zfmaddpd zmm2, zmm10, zmm31, zmm20	;; I7 + R8 * SQRTHALF (final I7)			; 87-90		n 93
	zfnmaddpd zmm10, zmm10, zmm31, zmm20	;; I7 - R8 * SQRTHALF (final I8)			; 87-90		n 95

	zfmaddpd zmm6, zmm12, zmm31, zmm18	;; R5 + R6 * SQRTHALF (final R5)			; 88-91		n 92
	zfnmaddpd zmm12, zmm12, zmm31, zmm18	;; R5 - R6 * SQRTHALF (final R6)			; 88-91		n 98

	zfnmaddpd zmm16, zmm3, zmm31, zmm15	;; R7 - I8 * SQRTHALF (final R7)			; 89-92		n 93
	zfmaddpd zmm3, zmm3, zmm31, zmm15	;; R7 + I8 * SQRTHALF (final R8)			; 89-92		n 99

	cmp	mul4_opcode, 0			;; See if we need to do more than the original type-2 FFT squaring
	je	short orig
	call	zcomplex_square_opcode		;; Handle more difficult cases
	jmp	back_to_orig
orig:
	;; Square the complex numbers
	vmulpd	zmm1, zmm13, zmm13		;; R1 * R1						; 90-93		n 94
	vmulpd	zmm13, zmm13, zmm5		;; R1 * I1 (I1/2)					; 90-93		n 100

	vmulpd	zmm15, zmm14, zmm14		;; R3 * R3						; 91-94		n 95
	vmulpd	zmm14, zmm14, zmm0		;; R3 * I3 (I3/2)					; 91-94		n 102

	vmulpd	zmm17, zmm9, zmm9		;; I5 * I5						; 92-95		n 96
	vmulpd	zmm9, zmm9, zmm6		;; I5 * R5 (I5/2)					; 92-95		n 98

 	vmulpd	zmm18, zmm2, zmm2		;; I7 * I7						; 93-96		n 97
	vmulpd	zmm2, zmm2, zmm16		;; I7 * R7 (I7/2)					; 93-96		n 103

	vmulpd	zmm20, zmm4, zmm4		;; I6 * I6						; 94-97		n 99
	zfnmaddpd zmm5, zmm5, zmm5, zmm1	;; R1^2 - I1 * I1 (R1)					; 94-97		n 107

	vmulpd	zmm1, zmm10, zmm10		;; I8 * I8						; 95-98		n 99
	zfnmaddpd zmm0, zmm0, zmm0, zmm15	;; R3^2 - I3 * I3 (R3)					; 95-98		n 108

	vmulpd	zmm15, zmm19, zmm19		;; R2 * R2						; 96-99		n 101
	zfmsubpd zmm6, zmm6, zmm6, zmm17	;; R5 * R5 - I5^2 (R5)					; 96-99		n 104

	vmulpd	zmm17, zmm11, zmm11		;; R4 * R4						; 97-100	n 101
	zfmsubpd zmm16, zmm16, zmm16, zmm18	;; R7 * R7 - I7^2 (R7)					; 97-100	n 105

	zfmaddpd zmm18, zmm4, zmm12, zmm9	;; I5/2 + I6 * R6 (new I5/2)				; 98-101	n 106
	zfnmaddpd zmm4, zmm4, zmm12, zmm9	;; I5/2 - I6 * R6 (new I6/2)				; 98-101	n 112

	zfmsubpd zmm12, zmm12, zmm12, zmm20	;; R6 * R6 - I6^2 (R6)					; 99-102	n 104
	zfmsubpd zmm1, zmm3, zmm3, zmm1		;; R8 * R8 - I8^2 (R8)					; 99-102	n 105

	zfmaddpd zmm9, zmm8, zmm19, zmm13	;; I1/2 + I2 * R2 (new I1/2)				; 100-103	n 109
	zfnmaddpd zmm13, zmm8, zmm19, zmm13	;; I1/2 - I2 * R2 (new I2/2)				; 100-103	n 116

	zfnmaddpd zmm15, zmm8, zmm8, zmm15	;; R2^2 - I2 * I2 (R2)					; 101-104	n 107
	zfnmaddpd zmm17, zmm7, zmm7, zmm17	;; R4^2 - I4 * I4 (R4)					; 101-104	n 108

	zfmaddpd zmm8, zmm7, zmm11, zmm14	;; I3/2 + I4 * R4 (new I3/2)				; 102-105	n 109
	zfnmaddpd zmm14, zmm7, zmm11, zmm14	;; I3/2 - I4 * R4 (new R4/2)				; 102-105	n 122

	zfmaddpd zmm7, zmm10, zmm3, zmm2	;; I7/2 + I8 * R8 (new I7/2)				; 103-106	n 111
	zfnmaddpd zmm2, zmm10, zmm3, zmm2	;; I7/2 - I8 * R8 (new R8/2)				; 103-106	n 113

back_to_orig:
	vaddpd	zmm3, zmm6, zmm12		;; R5 + R6 (new R5)					; 104-107	n 110
	vsubpd	zmm6, zmm6, zmm12		;; R5 - R6 (new R6)					; 104-107	n 112

	vaddpd	zmm10, zmm1, zmm16		;; R8 + R7 (new R7)					; 105-108	n 110
	vsubpd	zmm1, zmm1, zmm16		;; R8 - R7 (new I8)					; 105-108	n 113

	vmulpd	zmm18, zmm18, zmm30		;; I5/2 * 2 (new I5)					; 106-109	n 111
	;;; unused slot, well maybe it will get used							; 106-109

	vaddpd	zmm11, zmm5, zmm15		;; R1 + R2 (new R1)					; 107-110	n 114
	vsubpd	zmm5, zmm5, zmm15		;; R1 - R2 (new R2)					; 107-110	n 122

 	vaddpd	zmm12, zmm17, zmm0		;; R4 + R3 (new R3)					; 108-111	n 114
	vsubpd	zmm17, zmm17, zmm0		;; R4 - R3 (new I4)					; 108-111	n 116

	vaddpd	zmm16, zmm9, zmm8		;; I1/2 + I3/2 (newer I1/2)				; 109-112	n 114
	vsubpd	zmm9, zmm9, zmm8		;; I1/2 - I3/2 (newer I3/2)				; 109-112	n 114

	vsubpd	zmm15, zmm10, zmm3		;; R7 - R5 (newer I7)					; 110-113	n 116
	vaddpd	zmm10, zmm10, zmm3		;; R7 + R5 (newer R5)					; 110-113	n 118

	zfmaddpd zmm0, zmm7, zmm30, zmm18	;; I5 + I7/2 * 2 (newer I5)				; 111-114	n 116
	zfnmaddpd zmm7, zmm7, zmm30, zmm18	;; I5 - I7/2 * 2 (newer R7)				; 111-114	n 118

	;; multiply R6/I6 by SQRTHALF - i*SQRTHALF
	zfmsubpd zmm8, zmm4, zmm30, zmm6	;; I6 = I6/2 * 2 - R6					; 112-115	n 118
	zfmaddpd zmm4, zmm4, zmm30, zmm6	;; R6 = R6 + I6/2 * 2					; 112-115	n 120

	;; multiply R8/I8 by SQRTHALF - i*SQRTHALF
	zfnmaddpd zmm3, zmm2, zmm30, zmm1	;; I8 = I8 - R8/2 * 2					; 113-116	n 118
	zfmaddpd zmm2, zmm2, zmm30, zmm1	;; R8 = R8/2 * 2 + I8					; 113-116	n 120

	;; shuffle inputs are:
	;; R1 = R8_0 R7_0 R4_0 R3_0 R6_0 R5_0 R2_0 R1_0	goal new R1 is  R1_7 R1_6 R1_5 R1_4 R1_3 R1_2 R1_1 R1_0
	;; R2 = R8_1 R7_1 R4_1 R3_1 R6_1 R5_1 R2_1 R1_1	goal new R2 is  R2_7 R2_6 R2_5 R2_4 R2_3 R2_2 R2_1 R2_0, etc.
	;; I1 = I8_0 I7_0 I4_0 I3_0 I6_0 I5_0 I2_0 I1_0	goal new I1 is  I1_7 I1_6 I1_5 I1_4 I1_3 I1_2 I1_1 I1_0, etc.
	;; shuffle the two aparts		(can be done before last FFT level!)
	vshuff64x2 zmm1, zmm16, zmm9, 11101110b	;; I8_2 I7_2 I4_2 I3_2 I8_0 I7_0 I4_0 I3_0 (I13H/2)	; 114-116	n 124
	vshuff64x2 zmm16, zmm16, zmm9, 01000100b;; I6_2 I5_2 I2_2 I1_2 I6_0 I5_0 I2_0 I1_0 (I13L/2)	; 115-117	n 132

	vaddpd	zmm9, zmm11, zmm12		;; R1 + R3 (newer R1)					; 114-117	n 120
	vsubpd	zmm11, zmm11, zmm12		;; R1 - R3 (newer R3)					; 115-118	n 120

	vshuff64x2 zmm12, zmm0, zmm15, 11101110b;; I8_6 I7_6 I4_6 I3_6 I8_4 I7_4 I4_4 I3_4 (I57H)	; 116-118	n 124
	vshuff64x2 zmm0, zmm0, zmm15, 01000100b;; I6_6 I5_6 I2_6 I1_6 I6_4 I5_4 I2_4 I1_4 (I57L)	; 117-119	n 132

 	zfmaddpd zmm15, zmm13, zmm30, zmm17	;; I2/2 * 2 + I4 (newer I2)				; 116-119	n 122
	zfmsubpd zmm13, zmm13, zmm30, zmm17	;; I2/2 * 2 - I4 (newer I4)				; 117-120	n 122

	vshuff64x2 zmm6, zmm10, zmm7, 11101110b	;; R8_6 R7_6 R4_6 R3_6 R8_4 R7_4 R4_4 R3_4 (R57H)	; 118-120	n 126
	vshuff64x2 zmm10, zmm10, zmm7, 01000100b;; R6_6 R5_6 R2_6 R1_6 R6_4 R5_4 R2_4 R1_4 (R57L)	; 119-121	n 134

	vaddpd	zmm7, zmm8, zmm3		;; I6 + I8 (newer I6/SQRTHALF)				; 118-121	n 124
	vsubpd	zmm8, zmm8, zmm3		;; I6 - I8 (newer R8/SQRTHALF)				; 119-122	n 126

	vshuff64x2 zmm3, zmm9, zmm11, 11101110b	;; R8_2 R7_2 R4_2 R3_2 R8_0 R7_0 R4_0 R3_0 (R13H)	; 120-122	n 126
	vshuff64x2 zmm9, zmm9, zmm11, 01000100b	;; R6_2 R5_2 R2_2 R1_2 R6_0 R5_0 R2_0 R1_0 (R13L)	; 121-123	n 134

	vsubpd	zmm11, zmm2, zmm4		;; R8 - R6 (newer I8/SQRTHALF)				; 120-123	n 124
	vaddpd	zmm2, zmm2, zmm4		;; R8 + R6 (newer R6/SQRTHALF)				; 121-124	n 126

	vshuff64x2 zmm4, zmm15, zmm13, 11101110b;; I8_3 I7_3 I4_3 I3_3 I8_1 I7_1 I4_1 I3_1 (I24H)	; 122-124	n 128
	vshuff64x2 zmm15, zmm15, zmm13, 01000100b;; I6_3 I5_3 I2_3 I1_3 I6_1 I5_1 I2_1 I1_1 (I24L)	; 123-125	n 136

	zfmaddpd zmm13, zmm14, zmm30, zmm5	;; R2 + R4/2 * 2 (newer R2)				; 122-125	n 127
	zfnmaddpd zmm14, zmm14, zmm30, zmm5	;; R2 - R4/2 * 2 (newer R4)				; 123-126	n 127

	vshuff64x2 zmm5, zmm7, zmm11, 11101110b	;; I8_7 I7_7 I4_7 I3_7 I8_5 I7_5 I4_5 I3_5 (I68H)	; 124-126	n 128
	vshuff64x2 zmm7, zmm7, zmm11, 01000100b	;; I6_7 I5_7 I2_7 I1_7 I6_5 I5_5 I2_5 I1_5 (I68L)	; 125-127	n 136

	zfmaddpd zmm11, zmm1, zmm30, zmm12	;; I13H/2 * 2 + I57H (final I13H)			; 124-127	n 130
	zfmsubpd zmm1, zmm1, zmm30, zmm12	;; I13H/2 * 2 - I57H (final I57H)			; 125-128	n 130

	vshuff64x2 zmm17, zmm2, zmm8, 11101110b	;; R8_7 R7_7 R4_7 R3_7 R8_5 R7_5 R4_5 R3_5 (R68H)	; 126-128	n 130
	vshuff64x2 zmm18, zmm13, zmm14, 11101110b;; R8_3 R7_3 R4_3 R3_3 R8_1 R7_1 R4_1 R3_1 (R24H)	; 127-129	n 130	r127

	vaddpd	zmm12, zmm3, zmm6		;; R13H + R57H (final R13H)				; 126-129	n 132
	vsubpd	zmm3, zmm3, zmm6		;; R13H - R57H (final R57H)				; 127-130	n 132

	vshuff64x2 zmm2, zmm2, zmm8, 01000100b	;; R6_7 R5_7 R2_7 R1_7 R6_5 R5_5 R2_5 R1_5 (R68L)	; 128-130	n 138
	vshuff64x2 zmm13, zmm13, zmm14, 01000100b;; R6_3 R5_3 R2_3 R1_3 R6_1 R5_1 R2_1 R1_1 (R24L)	; 129-131	n 138

	zfmaddpd zmm6, zmm5, zmm31, zmm4	;; I24H + I68H * SQRTHALF (final I24H)			; 128-131	n 133	r128
	zfnmaddpd zmm5, zmm5, zmm31, zmm4	;; I24H - I68H * SQRTHALF (final I68H)			; 129-132	n 133

	vshuff64x2 zmm8, zmm11, zmm1, 11011101b	;; I8_6 I7_6 I8_4 I7_4 I8_2 I7_2 I8_0 I7_0		; 130-132	n 138	r129
	vshuff64x2 zmm11, zmm11, zmm1, 10001000b;; I4_6 I3_6 I4_4 I3_4 I4_2 I3_2 I4_0 I3_0		; 131-133	n 142	r129

	zfmaddpd zmm14, zmm17, zmm31, zmm18	;; R24H + R68H * SQRTHALF (final R24H)			; 130-133	n 135	r130
	zfnmaddpd zmm17, zmm17, zmm31, zmm18	;; R24H - R68H * SQRTHALF (final R68H)			; 131-134	n 135

	vshuff64x2 zmm1, zmm12, zmm3, 11011101b	;; R8_6 R7_6 R8_4 R7_4 R8_2 R7_2 R8_0 R7_0		; 132-134	n 139	r131
	vshuff64x2 zmm4, zmm6, zmm5, 11011101b	;; I8_7 I7_7 I8_5 I7_5 I8_3 I7_3 I8_1 I7_1		; 133-135	n 138	r133

	zfmaddpd zmm18, zmm16, zmm30, zmm0	;; I13L/2 * 2 + I57L (final I13L)			; 132-135	n 146
	zfmsubpd zmm16, zmm16, zmm30, zmm0	;; I13L/2 * 2 - I57L (final I57L)			; 133-136	n 146

	vshuff64x2 zmm6, zmm6, zmm5, 10001000b	;; I4_7 I3_7 I4_5 I3_5 I4_3 I3_3 I4_1 I3_1		; 134-136	n 142	r133
	vshuff64x2 zmm0, zmm14, zmm17, 11011101b;; R8_7 R7_7 R8_5 R7_5 R8_3 R7_3 R8_1 R7_1		; 135-137	n 139	r135

	vaddpd	zmm5, zmm9, zmm10		;; R13L + R57L (final R13L)				; 134-137	n 149
	vsubpd	zmm9, zmm9, zmm10		;; R13L - R57L (final R57L)				; 135-138	n 149

	vshuff64x2 zmm12, zmm12, zmm3, 10001000b;; R4_6 R3_6 R4_4 R3_4 R4_2 R3_2 R4_0 R3_0		; 136-138	n 143	r131
	vshuff64x2 zmm14, zmm14, zmm17, 10001000b;; R4_7 R3_7 R4_5 R3_5 R4_3 R3_3 R4_1 R3_1		; 137-139	n 143	r135

	zfmaddpd zmm10, zmm7, zmm31, zmm15	;; I24L + I68L * SQRTHALF (final I24L)			; 136-139	n 147
	zfnmaddpd zmm7, zmm7, zmm31, zmm15	;; I24L - I68L * SQRTHALF (final I68L)			; 137-140	n 147

	vshufpd	zmm3, zmm8, zmm4, 11111111b	;; I8_7 I8_6 I8_5 I8_4 I8_3 I8_2 I8_1 I8_0 (next I8)	; 138		n 140
	vshufpd	zmm15, zmm1, zmm0, 11111111b	;; R8_7 R8_6 R8_5 R8_4 R8_3 R8_2 R8_1 R8_0 (next R8)	; 139		n 140

	zfmaddpd zmm17, zmm2, zmm31, zmm13	;; R24L + R68L * SQRTHALF (final R24L)			; 138-141	n 148
	zfnmaddpd zmm2, zmm2, zmm31, zmm13	;; R24L - R68L * SQRTHALF (final R68L)			; 139-142	n 148

	vshufpd	zmm8, zmm8, zmm4, 00000000b	;; I7_7 I7_6 I7_5 I7_4 I7_3 I7_2 I7_1 I7_0 (next I7)	; 140		n 142
	vshufpd	zmm1, zmm1, zmm0, 00000000b	;; R7_7 R7_6 R7_5 R7_4 R7_3 R7_2 R7_1 R7_0 (next R7)	; 141		n 142

	zfmsubpd zmm13, zmm15, zmm27, zmm3	;; A8 = R8 * cosine/sine - I8				; 140-143	n 152
	zfmaddpd zmm3, zmm3, zmm27, zmm15	;; B8 = I8 * cosine/sine + R8				; 141-144	n 153

	vshufpd	zmm4, zmm11, zmm6, 00000000b	;; I3_7 I3_6 I3_5 I3_4 I3_3 I3_2 I3_1 I3_0 (next I3)	; 142		n 144
	vshufpd	zmm0, zmm12, zmm14, 00000000b	;; R3_7 R3_6 R3_5 R3_4 R3_3 R3_2 R3_1 R3_0 (next R3)	; 143		n 144

	zfmsubpd zmm15, zmm1, zmm26, zmm8	;; R7 * cosine/sine - I7 (first R7/sine)		; 142-145	n 148
	zfmaddpd zmm8, zmm8, zmm26, zmm1	;; I7 * cosine/sine + R7 (first I7/sine)		; 143-146	n 150

	vshufpd	zmm11, zmm11, zmm6, 11111111b	;; I4_7 I4_6 I4_5 I4_4 I4_3 I4_2 I4_1 I4_0 (next I4)	; 144		n 146
 	vshufpd	zmm12, zmm12, zmm14, 11111111b	;; R4_7 R4_6 R4_5 R4_4 R4_3 R4_2 R4_1 R4_0 (next R4)	; 145		n 146

	zfmaddpd zmm1, zmm0, zmm26, zmm4	;; R3 * cosine/sine + I3 (first R3/sine)		; 144-147	n 148
	zfmsubpd zmm4, zmm4, zmm26, zmm0	;; I3 * cosine/sine - R3 (first I3/sine)		; 145-148	n 150

	;; shuffle the four aparts
	vshuff64x2 zmm6, zmm18, zmm16, 11011101b;; I6_6 I5_6 I6_4 I5_4 I6_2 I5_2 I6_0 I5_0		; 146-148	n 152
	vshuff64x2 zmm14, zmm10, zmm7, 11011101b;; I6_7 I5_7 I6_5 I5_5 I6_3 I5_3 I6_1 I5_1		; 147-149	n 152

	zfmaddpd zmm0, zmm12, zmm28, zmm11	;; R4 * cosine/sine + I4 (first R4/sine)		; 146-149	n 160
	zfmsubpd zmm11, zmm11, zmm28, zmm12	;; I4 * cosine/sine - R4 (first I4/sine)		; 147-150	n 161

	vshuff64x2 zmm12, zmm17, zmm2, 11011101b;; R6_7 R5_7 R6_5 R5_5 R6_3 R5_3 R6_1 R5_1		; 148-150	n 153
	vshuff64x2 zmm19, zmm5, zmm9, 11011101b	;; R6_6 R5_6 R6_4 R5_4 R6_2 R5_2 R6_0 R5_0		; 149-151	n 153

	vaddpd	zmm20, zmm1, zmm15		;; R3/sine + R7/sine					; 148-151	n 169
	vsubpd	zmm1, zmm1, zmm15		;; R3/sine - R7/sine					; 149-152	n 166

	vshuff64x2 zmm18, zmm18, zmm16, 10001000b;; I2_6 I1_6 I2_4 I1_4 I2_2 I1_2 I2_0 I1_0		; 150-152	n 156
	vshuff64x2 zmm10, zmm10, zmm7, 10001000b;; I2_7 I1_7 I2_5 I1_5 I2_3 I1_3 I2_1 I1_1		; 151-153	n 156

	vaddpd	zmm15, zmm4, zmm8		;; I3/sine + I7/sine					; 150-153	n 165
	vsubpd	zmm4, zmm4, zmm8		;; I3/sine - I7/sine					; 151-154	n 170

	;; shufpd the one aparts
	vshufpd	zmm16, zmm6, zmm14, 00000000b	;; I5_7 I5_6 I5_5 I5_4 I5_3 I5_2 I5_1 I5_0 (next I5)	; 152		n 154
	vshufpd	zmm7, zmm19, zmm12, 00000000b	;; R5_7 R5_6 R5_5 R5_4 R5_3 R5_2 R5_1 R5_0 (next R5)	; 153		n 154

	vmulpd	zmm13, zmm13, zmm23		;; R8 = R8 * sine28					; 152-155	n 167
	vmulpd	zmm3, zmm3, zmm23		;; I8 = I8 * sine28					; 153-156	n 168

	vshufpd	zmm6, zmm6, zmm14, 11111111b	;; I6_7 I6_6 I6_5 I6_4 I6_3 I6_2 I6_1 I6_0 (next I6)	; 154		n 156
	vshufpd	zmm19, zmm19, zmm12, 11111111b	;; R6_7 R6_6 R6_5 R6_4 R6_3 R6_2 R6_1 R6_0 (next R6)	; 155		n 156

	zfmsubpd zmm8, zmm16, zmm29, zmm7	;; I5 * cosine/sine - R5 (first I5/sine)		; 154-157	n 158
	zfmaddpd zmm7, zmm7, zmm29, zmm16	;; R5 * cosine/sine + I5 (first R5/sine)		; 155-158	n 164

	vshufpd	zmm14, zmm18, zmm10, 00000000b	;; I1_7 I1_6 I1_5 I1_4 I1_3 I1_2 I1_1 I1_0 (next I1)	; 156		n 158
	vshuff64x2 zmm5, zmm5, zmm9, 10001000b	;; R2_6 R1_6 R2_4 R1_4 R2_2 R1_2 R2_0 R1_0		; 157-159	n 161

	zfmsubpd zmm12, zmm19, zmm28, zmm6	;; R6 * cosine/sine - I6 (first R6/sine)		; 156-159	n 160
	zfmaddpd zmm6, zmm6, zmm28, zmm19	;; I6 * cosine/sine + R6 (first I6/sine)		; 157-160	n 161

	vshuff64x2 zmm17, zmm17, zmm2, 10001000b;; R2_7 R1_7 R2_5 R1_5 R2_3 R1_3 R2_1 R1_1		; 158-160	n 161
	vshufpd	zmm18, zmm18, zmm10, 11111111b	;; I2_7 I2_6 I2_5 I2_4 I2_3 I2_2 I2_1 I2_0 (next I2)	; 159		n 163

	zfmaddpd zmm16, zmm8, zmm25, zmm14	;; I1 + I5 * sine					; 158-161	n 165
	zfnmaddpd zmm8, zmm8, zmm25, zmm14	;; I1 - I5 * sine					; 159-162	n 166

	vaddpd	zmm9, zmm0, zmm12		;; R4/sine + R6/sine					; 160-163	n 171
	vsubpd	zmm0, zmm0, zmm12		;; R4/sine - R6/sine					; 160-163	n 172

	vshufpd	zmm2, zmm5, zmm17, 11111111b	;; R2_7 R2_6 R2_5 R2_4 R2_3 R2_2 R2_1 R2_0 (next R2)	; 161		n 163
	vshufpd	zmm5, zmm5, zmm17, 00000000b	;; R1_7 R1_6 R1_5 R1_4 R1_3 R1_2 R1_1 R1_0 (next R1)	; 162		n 164

	vaddpd	zmm10, zmm11, zmm6		;; I4/sine + I6/sine					; 161-164	n 174
	vsubpd	zmm11, zmm11, zmm6		;; I4/sine - I6/sine					; 162-165	n 173

	zfmaddpd zmm14, zmm2, zmm27, zmm18	;; R2 * cosine/sine + I2 (first R2/sine)		; 163-166	n 167	r162
	zfmsubpd zmm18, zmm18, zmm27, zmm2	;; I2 * cosine/sine - R2 (first I2/sine)		; 163-166	n 168

	zfmaddpd zmm12, zmm7, zmm25, zmm5	;; R1 + R5 * sine					; 164-167	n 169	r163
	zfnmaddpd zmm7, zmm7, zmm25, zmm5	;; R1 - R5 * sine					; 164-167	n 170

	zfmaddpd zmm6, zmm15, zmm22, zmm16	;; i1++ = (i1+i5) + (i3+i7) * sine			; 165-168	n 179
	zfnmaddpd zmm15, zmm15, zmm22, zmm16	;; i1+- = (i1+i5) - (i3+i7) * sine			; 165-168	n 176

	zfmaddpd zmm2, zmm1, zmm22, zmm8	;; i1-+ = (i1-i5) + (r3-r7) * sine			; 166-169	n 184
	zfnmaddpd zmm1, zmm1, zmm22, zmm8	;; i1-- = (i1-i5) - (r3-r7) * sine			; 166-169	n 183

	zfmaddpd zmm5, zmm14, zmm23, zmm13	;; R2 * sine + R8					; 167-170	n 171	r166
	zfmsubpd zmm14, zmm14, zmm23, zmm13	;; R2 * sine - R8					; 167-170	n 172

	zfmaddpd zmm8, zmm18, zmm23, zmm3	;; I2 * sine + I8					; 168-171	n 174	r166
	zfmsubpd zmm18, zmm18, zmm23, zmm3	;; I2 * sine - I8					; 168-171	n 173

	zfmaddpd zmm13, zmm20, zmm22, zmm12	;; r1++ = (r1+r5) + (r3+r7) * sine			; 169-172	n 175	r168
	zfnmaddpd zmm20, zmm20, zmm22, zmm12	;; r1+- = (r1+r5) - (r3+r7) * sine			; 169-172	n 180

	zfmaddpd zmm3, zmm4, zmm22, zmm7	;; r1-+ = (r1-r5) + (i3-i7) * sine			; 170-173	n 181
	zfnmaddpd zmm4, zmm4, zmm22, zmm7	;; r1-- = (r1-r5) - (i3-i7) * sine			; 170-173	n 182

	zfmaddpd zmm12, zmm9, zmm24, zmm5	;; r2++ = (r2+r8) + (r4+r6) * sine			; 171-174	n 175
	zfnmaddpd zmm9, zmm9, zmm24, zmm5	;; r2+- = (r2+r8) - (r4+r6) * sine			; 171-174	n 177

	zfmaddpd zmm7, zmm0, zmm24, zmm14	;; r2-+ = (r2-r8) + (r4-r6) * sine			; 172-175	n 178
	zfnmaddpd zmm0, zmm0, zmm24, zmm14	;; r2-- = (r2-r8) - (r4-r6) * sine			; 172-175	n 176

	zfmaddpd zmm5, zmm11, zmm24, zmm18	;; i2-+ = (i2-i8) + (i4-i6) * sine			; 173-176	n 177
	zfnmaddpd zmm11, zmm11, zmm24, zmm18	;; i2-- = (i2-i8) - (i4-i6) * sine			; 173-176	n 180

	zfmaddpd zmm14, zmm10, zmm24, zmm8	;; i2++ = (i2+i8) + (i4+i6) * sine			; 174-177	n 179
	zfnmaddpd zmm10, zmm10, zmm24, zmm8	;; i2+- = (i2+i8) - (i4+i6) * sine			; 174-177	n 178

	vaddpd	zmm8, zmm13, zmm12		;; R1 = (r1++) + (r2++)					; 175-178		r175
	vsubpd	zmm13, zmm13, zmm12		;; R5 = (r1++) - (r2++)					; 175-178

	vsubpd	zmm12, zmm15, zmm0		;; I3 = (i1+-) - (r2--)					; 176-179		r176
	vaddpd	zmm15, zmm15, zmm0		;; I7 = (i1+-) + (r2--)					; 176-179

	vaddpd	zmm0, zmm9, zmm5		;; r2+-+ = (r2+-) + (i2-+)				; 177-180	n 181	r177
	vsubpd	zmm9, zmm9, zmm5		;; r2+-- = (r2+-) - (i2-+)				; 177-180	n 182

	vaddpd	zmm5, zmm7, zmm10		;; r2-++ = (r2-+) + (i2+-)				; 178-181	n 184	r178
	vsubpd	zmm7, zmm7, zmm10		;; r2-+- = (r2-+) - (i2+-)				; 178-181	n 183

	vaddpd	zmm10, zmm6, zmm14		;; I1 = (i1++) + (i2++)					; 179-182		r178
	vsubpd	zmm6, zmm6, zmm14		;; I5 = (i1++) - (i2++)					; 179-182
	zstore	[srcreg+r8], zmm8		;; Save R1						; 179

	vaddpd	zmm14, zmm20, zmm11		;; R3 = (r1+-) + (i2--)					; 180-183		r177
	vsubpd	zmm20, zmm20, zmm11		;; R7 = (r1+-) - (i2--)					; 180-183
	zstore	[srcreg+r8+d4], zmm13		;; Save R5						; 179+1

	zfmaddpd zmm11, zmm0, zmm31, zmm3	;; R2 = (r1-+) + .707*(r2+-+)				; 181-184		r181
	zfnmaddpd zmm0, zmm0, zmm31, zmm3	;; R6 = (r1-+) - .707*(r2+-+)				; 181-184
	zstore	[srcreg+r8+d2+64], zmm12	;; Save I3						; 180+1

	zfnmaddpd zmm3, zmm9, zmm31, zmm4	;; R4 = (r1--) - .707*(r2+--)				; 182-185		r181
	zfmaddpd zmm9, zmm9, zmm31, zmm4	;; R8 = (r1--) + .707*(r2+--)				; 182-185
	zstore	[srcreg+r8+d4+d2+64], zmm15	;; Save I7						; 180+2

	zfnmaddpd zmm4, zmm7, zmm31, zmm1	;; I2 = (i1--) - .707*(r2-+-)				; 183-186		r182
	zfmaddpd zmm7, zmm7, zmm31, zmm1	;; I6 = (i1--) + .707*(r2-+-)				; 183-186
	zstore	[srcreg+r8+64], zmm10		;; Save I1						; 183

	zfnmaddpd zmm1, zmm5, zmm31, zmm2	;; I4 = (i1-+) - .707*(r2-++)				; 184-187		r182
	zfmaddpd zmm5, zmm5, zmm31, zmm2	;; I8 = (i1-+) + .707*(r2-++)				; 184-187
	zstore	[srcreg+r8+d4+64], zmm6		;; Save I5						; 183+1

	zstore	[srcreg+r8+d2], zmm14		;; Save R3						; 184+1
	zstore	[srcreg+r8+d4+d2], zmm20	;; Save R7						; 184+2
	zstore	[srcreg+r8+d1], zmm11		;; Save R2						; 185+2
	zstore	[srcreg+r8+d4+d1], zmm0		;; Save R6						; 185+3
	zstore	[srcreg+r8+d2+d1], zmm3		;; Save R4						; 186+3
	zstore	[srcreg+r8+d4+d2+d1], zmm9	;; Save R8						; 186+4
	zstore	[srcreg+r8+d1+64], zmm4		;; Save I2						; 187+5
	zstore	[srcreg+r8+d4+d1+64], zmm7	;; Save I6						; 187+5
	zstore	[srcreg+r8+d2+d1+64], zmm1	;; Save I4						; 188+5
	zstore	[srcreg+r8+d4+d2+d1+64], zmm5	;; Save I8						; 188+6
	bump	srcreg, srcinc
	ENDM

;; Same as above code between orig and back_to_orig, except we implement mul4_opcode options

zr64_64c_square_opcode MACRO srcreg,d1,d2,d4
	LOCAL	fma, fmasave, muladd, muladdhard, mulsub, mulsubhard, fma_done

	;; The 0x80 bit in the opcode is "save FFT results"
	movzx	r10, mul4_opcode		;; Load opcode
	cmp	r10, 80h			;; Test opcode
	jb	fma				;; muladd, mulsub, not saving FFT data
	ja	fmasave				;; muladd, mulsub, saving FFT data
	;je	save_and_square			;; Original code (no muladd,mulsub) plus saving FFT data

	;; Square the complex numbers
	zstore	[srcreg], zmm13			;; R1
	zstore	[srcreg+64], zmm5		;; I1
	vmulpd	zmm1, zmm13, zmm13		;; R1 * R1						; 90-93		n 94
	vmulpd	zmm13, zmm13, zmm5		;; R1 * I1 (I1/2)					; 90-93		n 100

	zstore	[srcreg+d2], zmm14		;; R3
	zstore	[srcreg+d2+64], zmm0		;; I3
	vmulpd	zmm15, zmm14, zmm14		;; R3 * R3						; 91-94		n 95
	vmulpd	zmm14, zmm14, zmm0		;; R3 * I3 (I3/2)					; 91-94		n 102

	zstore	[srcreg+d4], zmm6		;; R5
	zstore	[srcreg+d4+64], zmm9		;; I5
	vmulpd	zmm17, zmm9, zmm9		;; I5 * I5						; 92-95		n 96
	vmulpd	zmm9, zmm9, zmm6		;; I5 * R5 (I5/2)					; 92-95		n 98

	zstore	[srcreg+d4+d2], zmm16		;; R7
	zstore	[srcreg+d4+d2+64], zmm2		;; I7
 	vmulpd	zmm18, zmm2, zmm2		;; I7 * I7						; 93-96		n 97
	vmulpd	zmm2, zmm2, zmm16		;; I7 * R7 (I7/2)					; 93-96		n 103

	zstore	[srcreg+d4+d1+64], zmm4		;; I6
	vmulpd	zmm20, zmm4, zmm4		;; I6 * I6						; 94-97		n 99
	zfnmaddpd zmm5, zmm5, zmm5, zmm1	;; R1^2 - I1 * I1 (R1)					; 94-97		n 107

	zstore	[srcreg+d4+d2+d1+64], zmm10	;; I8
	vmulpd	zmm1, zmm10, zmm10		;; I8 * I8						; 95-98		n 99
	zfnmaddpd zmm0, zmm0, zmm0, zmm15	;; R3^2 - I3 * I3 (R3)					; 95-98		n 108

	zstore	[srcreg+d1], zmm19		;; R2
	vmulpd	zmm15, zmm19, zmm19		;; R2 * R2						; 96-99		n 101
	zfmsubpd zmm6, zmm6, zmm6, zmm17	;; R5 * R5 - I5^2 (R5)					; 96-99		n 104

	zstore	[srcreg+d2+d1], zmm11		;; R4
	vmulpd	zmm17, zmm11, zmm11		;; R4 * R4						; 97-100	n 101
	zfmsubpd zmm16, zmm16, zmm16, zmm18	;; R7 * R7 - I7^2 (R7)					; 97-100	n 105

	zstore	[srcreg+d4+d1], zmm12		;; R6
	zfmaddpd zmm18, zmm4, zmm12, zmm9	;; I5/2 + I6 * R6 (new I5/2)				; 98-101	n 106
	zfnmaddpd zmm4, zmm4, zmm12, zmm9	;; I5/2 - I6 * R6 (new I6/2)				; 98-101	n 112

	zstore	[srcreg+d4+d2+d1], zmm3		;; R8
	zfmsubpd zmm12, zmm12, zmm12, zmm20	;; R6 * R6 - I6^2 (R6)					; 99-102	n 104
	zfmsubpd zmm1, zmm3, zmm3, zmm1		;; R8 * R8 - I8^2 (R8)					; 99-102	n 105

	zstore	[srcreg+d1+64], zmm8		;; I2
	zfmaddpd zmm9, zmm8, zmm19, zmm13	;; I1/2 + I2 * R2 (new I1/2)				; 100-103	n 109
	zfnmaddpd zmm13, zmm8, zmm19, zmm13	;; I1/2 - I2 * R2 (new I2/2)				; 100-103	n 116

	zstore	[srcreg+d2+d1+64], zmm7		;; I4
	zfnmaddpd zmm15, zmm8, zmm8, zmm15	;; R2^2 - I2 * I2 (R2)					; 101-104	n 107
	zfnmaddpd zmm17, zmm7, zmm7, zmm17	;; R4^2 - I4 * I4 (R4)					; 101-104	n 108

	zfmaddpd zmm8, zmm7, zmm11, zmm14	;; I3/2 + I4 * R4 (new I3/2)				; 102-105	n 109
	zfnmaddpd zmm14, zmm7, zmm11, zmm14	;; I3/2 - I4 * R4 (new R4/2)				; 102-105	n 122

	zfmaddpd zmm7, zmm10, zmm3, zmm2	;; I7/2 + I8 * R8 (new I7/2)				; 103-106	n 111
	zfnmaddpd zmm2, zmm10, zmm3, zmm2	;; I7/2 - I8 * R8 (new R8/2)				; 103-106	n 113

	jmp	done

fmasave:
	zstore	[srcreg], zmm13			;; R1
	zstore	[srcreg+64], zmm5		;; I1
	zstore	[srcreg+d2], zmm14		;; R3
	zstore	[srcreg+d2+64], zmm0		;; I3
	zstore	[srcreg+d4], zmm6		;; R5
	zstore	[srcreg+d4+64], zmm9		;; I5
	zstore	[srcreg+d4+d2], zmm16		;; R7
	zstore	[srcreg+d4+d2+64], zmm2		;; I7
	zstore	[srcreg+d4+d1+64], zmm4		;; I6
	zstore	[srcreg+d4+d2+d1+64], zmm10	;; I8
	zstore	[srcreg+d1], zmm19		;; R2
	zstore	[srcreg+d2+d1], zmm11		;; R4
	zstore	[srcreg+d4+d1], zmm12		;; R6
	zstore	[srcreg+d4+d2+d1], zmm3		;; R8
	zstore	[srcreg+d1+64], zmm8		;; I2
	zstore	[srcreg+d2+d1+64], zmm7		;; I4
	and	r10, 7Fh			;; Strip off save-FFT-data bit

fma:
	;; Square the complex numbers
	vmulpd	zmm1, zmm13, zmm13		;; R1 * R1						; 90-93		n 94
	vmulpd	zmm13, zmm13, zmm5		;; R1 * I1 (I1/2)					; 90-93		n 100

	vmulpd	zmm15, zmm14, zmm14		;; R3 * R3						; 91-94		n 95
	vmulpd	zmm14, zmm14, zmm0		;; R3 * I3 (I3/2)					; 91-94		n 102

	vmulpd	zmm17, zmm9, zmm9		;; I5 * I5						; 92-95		n 96
	vmulpd	zmm9, zmm9, zmm6		;; I5 * R5 (I5/2)					; 92-95		n 98

 	vmulpd	zmm18, zmm2, zmm2		;; I7 * I7						; 93-96		n 97
	vmulpd	zmm2, zmm2, zmm16		;; I7 * R7 (I7/2)					; 93-96		n 103

	vmulpd	zmm20, zmm4, zmm4		;; I6 * I6						; 94-97		n 99
	zfnmaddpd zmm5, zmm5, zmm5, zmm1	;; R1^2 - I1 * I1 (R1)					; 94-97		n 107

	vmulpd	zmm1, zmm10, zmm10		;; I8 * I8						; 95-98		n 99
	zfnmaddpd zmm0, zmm0, zmm0, zmm15	;; R3^2 - I3 * I3 (R3)					; 95-98		n 108

	vmulpd	zmm15, zmm19, zmm19		;; R2 * R2						; 96-99		n 101
	zfmsubpd zmm6, zmm6, zmm6, zmm17	;; R5 * R5 - I5^2 (R5)					; 96-99		n 104

	vmulpd	zmm17, zmm11, zmm11		;; R4 * R4						; 97-100	n 101
	zfmsubpd zmm16, zmm16, zmm16, zmm18	;; R7 * R7 - I7^2 (R7)					; 97-100	n 105

	vmulpd	zmm4, zmm4, zmm12				;; I6 * R6 (I6/2)
	vmulpd	zmm10, zmm10, zmm3				;; I8 * R8 (I8/2)

	zfmsubpd zmm12, zmm12, zmm12, zmm20	;; R6 * R6 - I6^2 (R6)					; 99-102	n 104
	zfmsubpd zmm1, zmm3, zmm3, zmm1		;; R8 * R8 - I8^2 (R8)					; 99-102	n 105

	vmulpd	zmm19, zmm8, zmm19				;; I2 * R2 (I2/2)
	vmulpd	zmm11, zmm7, zmm11				;; I4 * R4 (I4/2)

	zfnmaddpd zmm15, zmm8, zmm8, zmm15	;; R2^2 - I2 * I2 (R2)					; 101-104	n 107
	zfnmaddpd zmm17, zmm7, zmm7, zmm17	;; R4^2 - I4 * I4 (R4)					; 101-104	n 108

	vbroadcastsd zmm18, ZMM_HALF

	mov	r9, SRC2ARG			;; Distance to s2 arg from gwaddmul4 or gwsubmul4 or s3 arg in gwmuladd4 or gwmulsub4
	cmp	r10, 4				;; Case off opcode (opcodes 1,2 make no sense)
	mov	r10, SRC3ARG			;; Load distance to FFT(1)
	;jb	muladd				;; 3=muladd, fall through
	je	mulsub				;; 4=mulsub

muladd:	cmp	r10, 1				;; Check if we need to multiply add-in by FFT(1)
	jne	muladdhard			;; Yes, do it the hard way

	vaddpd	zmm5, zmm5, [srcreg+r9]				;; R1 = R1 + MemR1
	zfmaddpd zmm13, zmm18, [srcreg+r9+64], zmm13		;; I1/2 = I1/2 + MemI1/2
	vaddpd	zmm15, zmm15, [srcreg+r9+d1]			;; R2 = R2 + MemR2
	zfmaddpd zmm19, zmm18, [srcreg+r9+d1+64], zmm19		;; I2/2 = I2/2 + MemI2/2
	vaddpd	zmm0, zmm0, [srcreg+r9+d2]			;; R3 = R3 + MemR3
	zfmaddpd zmm14, zmm18, [srcreg+r9+d2+64], zmm14		;; I3/2 = I3/2 + MemI3/2
	vaddpd	zmm17, zmm17, [srcreg+r9+d2+d1]			;; R4 = R4 + MemR4
	zfmaddpd zmm11, zmm18, [srcreg+r9+d2+d1+64], zmm11	;; I4/2 = I4/2 + MemI4/2
	vaddpd	zmm6, zmm6, [srcreg+r9+d4]			;; R5 = R5 + MemR5
	zfmaddpd zmm9, zmm18, [srcreg+r9+d4+64], zmm9		;; I5/2 = I5/2 + MemI5/2
	vaddpd	zmm12, zmm12, [srcreg+r9+d4+d1]			;; R6 = R6 + MemR6
	zfmaddpd zmm4, zmm18, [srcreg+r9+d4+d1+64], zmm4	;; I6/2 = I6/2 + MemI6/2
	vaddpd	zmm16, zmm16, [srcreg+r9+d4+d2]			;; R7 = R7 + MemR7
	zfmaddpd zmm2, zmm18, [srcreg+r9+d4+d2+64], zmm2	;; I7/2 = I7/2 + MemI7/2
	vaddpd	zmm1, zmm1, [srcreg+r9+d4+d2+d1]		;; R8 = R8 + MemR8
	zfmaddpd zmm10, zmm18, [srcreg+r9+d4+d2+d1+64], zmm10	;; I8/2 = I8/2 + MemI8/2

	jmp	fma_done

muladdhard:
	vmovapd	zmm3, [srcreg+r9]				;; MemR1
	vmovapd	zmm7, [srcreg+r10]				;; MemR1#2
	zfmaddpd zmm5, zmm3, zmm7, zmm5				;; R1 = R1 + MemR1*MemR1#2
	vmovapd	zmm8, [srcreg+r10+64]				;; MemI1#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR1*MemI1#2
	vmovapd	zmm20, [srcreg+r9+64]				;; MemI1
	zfnmaddpd zmm5, zmm20, zmm8, zmm5			;; R1 = R1 - MemI1*MemI1#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI1*MemR1#2
	zfmaddpd zmm13, zmm18, zmm3, zmm13			;; I1/2 = I1/2 + TMP/2

	vmovapd	zmm3, [srcreg+r9+d1]				;; MemR2
	vmovapd	zmm7, [srcreg+r10+d1]				;; MemR2#2
	zfmaddpd zmm15, zmm3, zmm7, zmm15			;; R2 = R2 + MemR2*MemR2#2
	vmovapd	zmm8, [srcreg+r10+d1+64]			;; MemI2#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR2*MemI2#2
	vmovapd	zmm20, [srcreg+r9+d1+64]			;; MemI2
	zfnmaddpd zmm15, zmm20, zmm8, zmm15			;; R2 = R2 - MemI2*MemI2#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI2*MemR2#2
	zfmaddpd zmm19, zmm18, zmm3, zmm19			;; I2/2 = I2/2 + TMP/2

	vmovapd	zmm3, [srcreg+r9+d2]				;; MemR3
	vmovapd	zmm7, [srcreg+r10+d2]				;; MemR3#2
	zfmaddpd zmm0, zmm3, zmm7, zmm0				;; R3 = R3 + MemR3*MemR3#2
	vmovapd	zmm8, [srcreg+r10+d2+64]			;; MemI3#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR3*MemI3#2
	vmovapd	zmm20, [srcreg+r9+d2+64]			;; MemI3
	zfnmaddpd zmm0, zmm20, zmm8, zmm0			;; R3 = R3 - MemI3*MemI3#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI3*MemR3#2
	zfmaddpd zmm14, zmm18, zmm3, zmm14			;; I3/2 = I3/2 + TMP/2

	vmovapd	zmm3, [srcreg+r9+d2+d1]				;; MemR4
	vmovapd	zmm7, [srcreg+r10+d2+d1]			;; MemR4#2
	zfmaddpd zmm17, zmm3, zmm7, zmm17			;; R4 = R4 + MemR4*MemR4#2
	vmovapd	zmm8, [srcreg+r10+d2+d1+64]			;; MemI4#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR4*MemI4#2
	vmovapd	zmm20, [srcreg+r9+d2+d1+64]			;; MemI4
	zfnmaddpd zmm17, zmm20, zmm8, zmm17			;; R4 = R4 - MemI4*MemI4#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI4*MemR4#2
	zfmaddpd zmm11, zmm18, zmm3, zmm11			;; I4/2 = I4/2 + TMP/2

	vmovapd	zmm3, [srcreg+r9+d4]				;; MemR5
	vmovapd	zmm7, [srcreg+r10+d4]				;; MemR5#2
	zfmaddpd zmm6, zmm3, zmm7, zmm6				;; R5 = R5 + MemR5*MemR5#2
	vmovapd	zmm8, [srcreg+r10+d4+64]			;; MemI5#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR5*MemI5#2
	vmovapd	zmm20, [srcreg+r9+d4+64]			;; MemI5
	zfnmaddpd zmm6, zmm20, zmm8, zmm6			;; R5 = R5 - MemI5*MemI5#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI5*MemR5#2
	zfmaddpd zmm9, zmm18, zmm3, zmm9			;; I5/2 = I5/2 + TMP/2

	vmovapd	zmm3, [srcreg+r9+d4+d1]				;; MemR6
	vmovapd	zmm7, [srcreg+r10+d4+d1]			;; MemR6#2
	zfmaddpd zmm12, zmm3, zmm7, zmm12			;; R6 = R6 + MemR6*MemR6#2
	vmovapd	zmm8, [srcreg+r10+d4+d1+64]			;; MemI6#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR6*MemI6#2
	vmovapd	zmm20, [srcreg+r9+d4+d1+64]			;; MemI6
	zfnmaddpd zmm12, zmm20, zmm8, zmm12			;; R6 = R6 - MemI6*MemI6#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI6*MemR6#2
	zfmaddpd zmm4, zmm18, zmm3, zmm4			;; I6/2 = I6/2 + TMP/2

	vmovapd	zmm3, [srcreg+r9+d4+d2]				;; MemR7
	vmovapd	zmm7, [srcreg+r10+d4+d2]			;; MemR7#2
	zfmaddpd zmm16, zmm3, zmm7, zmm16			;; R7 = R7 + MemR7*MemR7#2
	vmovapd	zmm8, [srcreg+r10+d4+d2+64]			;; MemI7#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR7*MemI7#2
	vmovapd	zmm20, [srcreg+r9+d4+d2+64]			;; MemI7
	zfnmaddpd zmm16, zmm20, zmm8, zmm16			;; R7 = R7 - MemI7*MemI7#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI7*MemR7#2
	zfmaddpd zmm2, zmm18, zmm3, zmm2			;; I7/2 = I7/2 + TMP/2

	vmovapd	zmm3, [srcreg+r9+d4+d2+d1]			;; MemR8
	vmovapd	zmm7, [srcreg+r10+d4+d2+d1]			;; MemR8#2
	zfmaddpd zmm1, zmm3, zmm7, zmm1				;; R8 = R8 + MemR8*MemR8#2
	vmovapd	zmm8, [srcreg+r10+d4+d2+d1+64]			;; MemI8#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR8*MemI8#2
	vmovapd	zmm20, [srcreg+r9+d4+d2+d1+64]			;; MemI8
	zfnmaddpd zmm1, zmm20, zmm8, zmm1			;; R8 = R8 - MemI8*MemI8#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI8*MemR8#2
	zfmaddpd zmm10, zmm18, zmm3, zmm10			;; I8/2 = I8/2 + TMP/2

	jmp	fma_done

mulsub:	cmp	r10, 1				;; Check if we need to multiply add-in by FFT(1)
	jne	mulsubhard			;; Yes, do it the hard way

	vsubpd	zmm5, zmm5, [srcreg+r9]				;; R1 = R1 - MemR1
	zfnmaddpd zmm13, zmm18, [srcreg+r9+64], zmm13		;; I1/2 = I1/2 - MemI1/2
	vsubpd	zmm15, zmm15, [srcreg+r9+d1]			;; R2 = R2 - MemR2
	zfnmaddpd zmm19, zmm18, [srcreg+r9+d1+64], zmm19	;; I2/2 = I2/2 - MemI2/2
	vsubpd	zmm0, zmm0, [srcreg+r9+d2]			;; R3 = R3 - MemR3
	zfnmaddpd zmm14, zmm18, [srcreg+r9+d2+64], zmm14	;; I3/2 = I3/2 - MemI3/2
	vsubpd	zmm17, zmm17, [srcreg+r9+d2+d1]			;; R4 = R4 - MemR4
	zfnmaddpd zmm11, zmm18, [srcreg+r9+d2+d1+64], zmm11	;; I4/2 = I4/2 - MemI4/2
	vsubpd	zmm6, zmm6, [srcreg+r9+d4]			;; R5 = R5 - MemR5
	zfnmaddpd zmm9, zmm18, [srcreg+r9+d4+64], zmm9		;; I5/2 = I5/2 - MemI5/2
	vsubpd	zmm12, zmm12, [srcreg+r9+d4+d1]			;; R6 = R6 - MemR6
	zfnmaddpd zmm4, zmm18, [srcreg+r9+d4+d1+64], zmm4	;; I6/2 = I6/2 - MemI6/2
	vsubpd	zmm16, zmm16, [srcreg+r9+d4+d2]			;; R7 = R7 - MemR7
	zfnmaddpd zmm2, zmm18, [srcreg+r9+d4+d2+64], zmm2	;; I7/2 = I7/2 - MemI7/2
	vsubpd	zmm1, zmm1, [srcreg+r9+d4+d2+d1]		;; R8 = R8 - MemR8
	zfnmaddpd zmm10, zmm18, [srcreg+r9+d4+d2+d1+64], zmm10	;; I8/2 = I8/2 - MemI8/2

	jmp	fma_done

mulsubhard:
	vmovapd	zmm3, [srcreg+r9]				;; MemR1
	vmovapd	zmm7, [srcreg+r10]				;; MemR1#2
	zfnmaddpd zmm5, zmm3, zmm7, zmm5			;; R1 = R1 - MemR1*MemR1#2
	vmovapd	zmm8, [srcreg+r10+64]				;; MemI1#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR1*MemI1#2
	vmovapd	zmm20, [srcreg+r9+64]				;; MemI1
	zfmaddpd zmm5, zmm20, zmm8, zmm5			;; R1 = R1 + MemI1*MemI1#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI1*MemR1#2
	zfnmaddpd zmm13, zmm18, zmm3, zmm13			;; I1/2 = I1/2 - TMP/2

	vmovapd	zmm3, [srcreg+r9+d1]				;; MemR2
	vmovapd	zmm7, [srcreg+r10+d1]				;; MemR2#2
	zfnmaddpd zmm15, zmm3, zmm7, zmm15			;; R2 = R2 - MemR2*MemR2#2
	vmovapd	zmm8, [srcreg+r10+d1+64]			;; MemI2#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR2*MemI2#2
	vmovapd	zmm20, [srcreg+r9+d1+64]			;; MemI2
	zfmaddpd zmm15, zmm20, zmm8, zmm15			;; R2 = R2 + MemI2*MemI2#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI2*MemR2#2
	zfnmaddpd zmm19, zmm18, zmm3, zmm19			;; I2/2 = I2/2 - TMP/2

	vmovapd	zmm3, [srcreg+r9+d2]				;; MemR3
	vmovapd	zmm7, [srcreg+r10+d2]				;; MemR3#2
	zfnmaddpd zmm0, zmm3, zmm7, zmm0			;; R3 = R3 - MemR3*MemR3#2
	vmovapd	zmm8, [srcreg+r10+d2+64]			;; MemI3#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR3*MemI3#2
	vmovapd	zmm20, [srcreg+r9+d2+64]			;; MemI3
	zfmaddpd zmm0, zmm20, zmm8, zmm0			;; R3 = R3 + MemI3*MemI3#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI3*MemR3#2
	zfnmaddpd zmm14, zmm18, zmm3, zmm14			;; I3/2 = I3/2 - TMP/2

	vmovapd	zmm3, [srcreg+r9+d2+d1]				;; MemR4
	vmovapd	zmm7, [srcreg+r10+d2+d1]			;; MemR4#2
	zfnmaddpd zmm17, zmm3, zmm7, zmm17			;; R4 = R4 - MemR4*MemR4#2
	vmovapd	zmm8, [srcreg+r10+d2+d1+64]			;; MemI4#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR4*MemI4#2
	vmovapd	zmm20, [srcreg+r9+d2+d1+64]			;; MemI4
	zfmaddpd zmm17, zmm20, zmm8, zmm17			;; R4 = R4 + MemI4*MemI4#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI4*MemR4#2
	zfnmaddpd zmm11, zmm18, zmm3, zmm11			;; I4/2 = I4/2 - TMP/2

	vmovapd	zmm3, [srcreg+r9+d4]				;; MemR5
	vmovapd	zmm7, [srcreg+r10+d4]				;; MemR5#2
	zfnmaddpd zmm6, zmm3, zmm7, zmm6			;; R5 = R5 - MemR5*MemR5#2
	vmovapd	zmm8, [srcreg+r10+d4+64]			;; MemI5#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR5*MemI5#2
	vmovapd	zmm20, [srcreg+r9+d4+64]			;; MemI5
	zfmaddpd zmm6, zmm20, zmm8, zmm6			;; R5 = R5 + MemI5*MemI5#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI5*MemR5#2
	zfnmaddpd zmm9, zmm18, zmm3, zmm9			;; I5/2 = I5/2 - TMP/2

	vmovapd	zmm3, [srcreg+r9+d4+d1]				;; MemR6
	vmovapd	zmm7, [srcreg+r10+d4+d1]			;; MemR6#2
	zfnmaddpd zmm12, zmm3, zmm7, zmm12			;; R6 = R6 - MemR6*MemR6#2
	vmovapd	zmm8, [srcreg+r10+d4+d1+64]			;; MemI6#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR6*MemI6#2
	vmovapd	zmm20, [srcreg+r9+d4+d1+64]			;; MemI6
	zfmaddpd zmm12, zmm20, zmm8, zmm12			;; R6 = R6 + MemI6*MemI6#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI6*MemR6#2
	zfnmaddpd zmm4, zmm18, zmm3, zmm4			;; I6/2 = I6/2 - TMP/2

	vmovapd	zmm3, [srcreg+r9+d4+d2]				;; MemR7
	vmovapd	zmm7, [srcreg+r10+d4+d2]			;; MemR7#2
	zfnmaddpd zmm16, zmm3, zmm7, zmm16			;; R7 = R7 - MemR7*MemR7#2
	vmovapd	zmm8, [srcreg+r10+d4+d2+64]			;; MemI7#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR7*MemI7#2
	vmovapd	zmm20, [srcreg+r9+d4+d2+64]			;; MemI7
	zfmaddpd zmm16, zmm20, zmm8, zmm16			;; R7 = R7 + MemI7*MemI7#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI7*MemR7#2
	zfnmaddpd zmm2, zmm18, zmm3, zmm2			;; I7/2 = I7/2 - TMP/2

	vmovapd	zmm3, [srcreg+r9+d4+d2+d1]			;; MemR8
	vmovapd	zmm7, [srcreg+r10+d4+d2+d1]			;; MemR8#2
	zfnmaddpd zmm1, zmm3, zmm7, zmm1			;; R8 = R8 - MemR8*MemR8#2
	vmovapd	zmm8, [srcreg+r10+d4+d2+d1+64]			;; MemI8#2
	vmulpd	zmm3, zmm3, zmm8				;; TMP = MemR8*MemI8#2
	vmovapd	zmm20, [srcreg+r9+d4+d2+d1+64]			;; MemI8
	zfmaddpd zmm1, zmm20, zmm8, zmm1			;; R8 = R8 + MemI8*MemI8#2
	zfmaddpd zmm3, zmm20, zmm7, zmm3			;; TMP += MemI8*MemR8#2
	zfnmaddpd zmm10, zmm18, zmm3, zmm10			;; I8/2 = I8/2 - TMP/2

fma_done:
	vaddpd	zmm18, zmm9, zmm4		;; I5/2 + I6/2 (new I5/2)				; 98-101	n 106
	vsubpd	zmm4, zmm9, zmm4		;; I5/2 - I6/2 (new I6/2)				; 98-101	n 112

	vaddpd	zmm9, zmm13, zmm19		;; I1/2 + I2/2 (new I1/2)				; 100-103	n 109
	vsubpd	zmm13, zmm13, zmm19		;; I1/2 - I2/2 (new I2/2)				; 100-103	n 116

	vaddpd	zmm8, zmm14, zmm11		;; I3/2 + I4/2 (new I3/2)				; 102-105	n 109
	vsubpd	zmm14, zmm14, zmm11		;; I3/2 - I4/2 (new R4/2)				; 102-105	n 122

	vaddpd	zmm7, zmm2, zmm10		;; I7/2 + I8/2 (new I7/2)				; 103-106	n 111
	vsubpd	zmm2, zmm2, zmm10		;; I7/2 - I8/2 (new R8/2)				; 103-106	n 113

done:
	ENDM

zr64_sixtyfour_complex_with_mult_preload MACRO
	zr64_64c_mult_cmn_preload
	ENDM
zr64_sixtyfour_complex_with_mult MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	zr64_64c_mult_cmn srcreg,0,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	ENDM
zr64f_sixtyfour_complex_with_mult_preload MACRO
	zr64_64c_mult_cmn_preload
	ENDM
zr64f_sixtyfour_complex_with_mult MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	zr64_64c_mult_cmn srcreg,rbx,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

zr64_64c_mult_cmn_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRTHALF
	ENDM
zr64_64c_mult_cmn MACRO srcreg,srcoff,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	LOCAL	orig, back_to_orig

	vmovapd	zmm0, [srcreg+srcoff]			;; R1
	vmovapd	zmm20, [srcreg+srcoff+d4]		;; R5
	vaddpd	zmm15, zmm0, zmm20			;; R1+R5					; 1-4		n 9
	vsubpd	zmm0, zmm0, zmm20			;; R1-R5					; 1-4		n 18

	vmovapd	zmm16, [srcreg+srcoff+d2]		;; R3
	vmovapd	zmm20, [srcreg+srcoff+d4+d2]		;; R7
	vaddpd	zmm1, zmm16, zmm20			;; R3+R7					; 2-5		n 9
	vsubpd	zmm16, zmm16, zmm20			;; R3-R7					; 2-5		n 23

	vmovapd	zmm14, [srcreg+srcoff+d1]		;; R2
	vmovapd	zmm20, [srcreg+srcoff+d4+d1]		;; R6
	vaddpd	zmm13, zmm14, zmm20			;; R2+R6					; 3-6		n 10
	vsubpd	zmm14, zmm14, zmm20			;; R2-R6					; 3-6		n 13

	vmovapd	zmm6, [srcreg+srcoff+d2+d1]		;; R4
	vmovapd	zmm20, [srcreg+srcoff+d4+d2+d1]		;; R8
	vaddpd	zmm2, zmm6, zmm20			;; R4+R8					; 4-7		n 10
	vsubpd	zmm6, zmm6, zmm20			;; R4-R8					; 4-7		n 13

	vmovapd	zmm7, [srcreg+srcoff+d2+d1+64]		;; I4
	vmovapd	zmm20, [srcreg+srcoff+d4+d2+d1+64]	;; I8
	vaddpd	zmm10, zmm7, zmm20			;; I4+I8					; 5-8		n 11
	vsubpd	zmm7, zmm7, zmm20			;; I4-I8					; 5-8		n 14

	vmovapd	zmm8, [srcreg+srcoff+d1+64]		;; I2
	vmovapd	zmm20, [srcreg+srcoff+d4+d1+64]		;; I6
	vaddpd	zmm4, zmm8, zmm20			;; I2+I6					; 6-9		n 11
	vsubpd	zmm8, zmm8, zmm20			;; I2-I6					; 6-9		n 14

	vmovapd	zmm5, [srcreg+srcoff+64]		;; I1
	vmovapd	zmm20, [srcreg+srcoff+d4+64]		;; I5
	vaddpd	zmm11, zmm5, zmm20			;; I1+I5					; 7-10		n 12
	vsubpd	zmm5, zmm5, zmm20			;; I1-I5					; 7-10		n 19

	vmovapd	zmm9, [srcreg+srcoff+d2+64]		;; I3
	vmovapd	zmm20, [srcreg+srcoff+d4+d2+64]		;; I7
	vaddpd	zmm12, zmm9, zmm20			;; I3+I7					; 8-11		n 12
	vsubpd	zmm9, zmm9, zmm20			;; I3-I7					; 8-11		n 24

	vaddpd	zmm3, zmm15, zmm1		;; r1++ = (r1+r5) + (r3+r7)				; 9-12		n 15
	vsubpd	zmm15, zmm15, zmm1		;; r1+- = (r1+r5) - (r3+r7)				; 9-12		n 21
	vmovapd zmm22, [screg+1*128]		;; sine for R3/I3 and R7/I7

	vsubpd	zmm1, zmm13, zmm2		;; r2+- = (r2+r6) - (r4+r8)				; 10-13		n 17
	vaddpd	zmm13, zmm13, zmm2		;; r2++ = (r2+r6) + (r4+r8)				; 10-13		n 15
	vmovapd zmm29, [screg+3*128+64]		;; cosine/sine for R5/I5

	vsubpd	zmm2, zmm4, zmm10		;; i2+- = (i2+i6) - (i4+i8)				; 11-14		n 17
	vaddpd	zmm4, zmm4, zmm10		;; i2++ = (i2+i6) + (i4+i8)				; 11-14		n 16
	vmovapd zmm23, [screg+0*128]		;; sine for R2/I2 and R8/I8

	vaddpd	zmm10, zmm11, zmm12		;; i1++ = (i1+i5) + (i3+i7)				; 12-15		n 16
	vsubpd	zmm11, zmm11, zmm12		;; i1+- = (i1+i5) - (i3+i7)				; 12-15		n 21
	vmovapd zmm24, [screg+2*128]		;; sine for R4/I4 and R6/I6

	vaddpd	zmm12, zmm14, zmm6		;; r2-+ = (r2-r6) + (r4-r8)				; 13-16		n 23
	vsubpd	zmm14, zmm14, zmm6		;; r2-- = (r2-r6) - (r4-r8)				; 13-16		n 18
	vmovapd zmm25, [screg+3*128]		;; sine for R5/I5

	vsubpd	zmm6, zmm8, zmm7		;; i2-- = (i2-i6) - (i4-i8)				; 14-17		n 19
	vaddpd	zmm8, zmm8, zmm7		;; i2-+ = (i2-i6) + (i4-i8)				; 14-17		n 24
	vmovapd zmm26, [screg+1*128+64]		;; cosine/sine for R3/I3 and R7/I7

	vsubpd	zmm7, zmm3, zmm13		;; R5 = (r1++) - (r2++)					; 15-18		n 20
	vaddpd	zmm3, zmm3, zmm13		;; R1 = (r1++) + (r2++)					; 15-18		n 33
	vmovapd zmm27, [screg+0*128+64]		;; cosine/sine for R2/I2 and R8/I8

	vsubpd	zmm17, zmm10, zmm4		;; I5 = (i1++) - (i2++)					; 16-19		n 20
	vaddpd	zmm10, zmm10, zmm4		;; I1 = (i1++) + (i2++)					; 16-19		n 35
	vmovapd zmm28, [screg+2*128+64]		;; cosine/sine for R4/I4 and R6/I6

	vmulpd	zmm1, zmm1, zmm22		;; r2+-s = r2+- * sine37				; 17-20		n 21
	vmulpd	zmm2, zmm2, zmm22		;; i2+-s = i2+- * sine37				; 17-20		n 21
	L1prefetchw srcreg+L1pd, L1pt

	zfmaddpd zmm4, zmm14, zmm31, zmm0	;; r1-+ = (r1-r5) + .707*(r2--)				; 18-21		n 25
	zfnmaddpd zmm14, zmm14, zmm31, zmm0	;; r1-- = (r1-r5) - .707*(r2--)				; 18-21		n 26
	L1prefetchw srcreg+64+L1pd, L1pt

	zfmaddpd zmm0, zmm6, zmm31, zmm5	;; i1-+ = (i1-i5) + .707*(i2--)				; 19-22		n 25
	zfnmaddpd zmm6, zmm6, zmm31, zmm5	;; i1-- = (i1-i5) - .707*(i2--)				; 19-22		n 26
	L1prefetchw srcreg+d1+L1pd, L1pt

	zfmsubpd zmm13, zmm7, zmm29, zmm17	;; A5 = R5 * cosine/sine - I5				; 20-23		n 27
	zfmaddpd zmm17, zmm17, zmm29, zmm7	;; B5 = I5 * cosine/sine + R5				; 20-23		n 27
	L1prefetchw srcreg+d1+64+L1pd, L1pt

	zfmsubpd zmm7, zmm15, zmm22, zmm2	;; R3s = (r1+-)*sine37 - (i2+-s)			; 21-24		n 28
	zfmaddpd zmm5, zmm11, zmm22, zmm1	;; I3s = (i1+-)*sine37 + (r2+-s)			; 21-24		n 28
	L1prefetchw srcreg+d2+L1pd, L1pt

	zfmaddpd zmm15, zmm15, zmm22, zmm2	;; R7s = (r1+-)*sine37 + (i2+-s)			; 22-25		n 29
	zfmsubpd zmm11, zmm11, zmm22, zmm1	;; I7s = (i1+-)*sine37 - (r2+-s)			; 22-25		n 29
	L1prefetchw srcreg+d2+64+L1pd, L1pt

	zfmaddpd zmm2, zmm12, zmm31, zmm16	;; r3-+ = (r3-r7) + .707*(r2-+)				; 23-26		n 30
	zfnmaddpd zmm12, zmm12, zmm31, zmm16	;; r3-- = (r3-r7) - .707*(r2-+)				; 23-26		n 31
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	zfmaddpd zmm19, zmm8, zmm31, zmm9	;; i3-+ = (i3-i7) + .707*(i2-+)				; 24-27		n 30
	zfnmaddpd zmm8, zmm8, zmm31, zmm9	;; i3-- = (i3-i7) - .707*(i2-+)				; 24-27		n 31
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt

	vmulpd	zmm4, zmm4, zmm23		;; r1-+s = r1-+ * sine28				; 25-28		n 30
	vmulpd	zmm0, zmm0, zmm23		;; i1-+s = i1-+ * sine28				; 25-28		n 30
	L1prefetchw srcreg+d4+L1pd, L1pt

	vmulpd	zmm14, zmm14, zmm24		;; r1--s = r1-- * sine46				; 26-29		n 31
	vmulpd	zmm6, zmm6, zmm24		;; i1--s = i1-- * sine46				; 26-29		n 31
	L1prefetchw srcreg+d4+64+L1pd, L1pt

	vmulpd	zmm13, zmm13, zmm25		;; A5 = A5 * sine (final R5)				; 27-30		n 33
	vmulpd	zmm17, zmm17, zmm25		;; B5 = B5 * sine (final I5)				; 27-30		n 35
	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	zfmsubpd zmm16, zmm7, zmm26, zmm5	;; R3s * cosine/sine - I3s (final R3)			; 28-31		n 37
	zfmaddpd zmm5, zmm5, zmm26, zmm7	;; I3s * cosine/sine + R3s (final I3)			; 28-31		n 39
	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt

	zfmaddpd zmm7, zmm15, zmm26, zmm11	;; R7s * cosine/sine + I7s (final R7)			; 29-32		n 37
	zfmsubpd zmm11, zmm11, zmm26, zmm15	;; I7s * cosine/sine - R7s (final I7)			; 29-32		n 39
	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	zfnmaddpd zmm9, zmm19, zmm23, zmm4	;; R2s = (r1-+s) - sine28*(i3-+)			; 30-33		n 35
	zfmaddpd zmm1, zmm2, zmm23, zmm0	;; I2s = (i1-+s) + sine28*(r3-+)			; 30-33		n 35
	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt

	zfnmaddpd zmm18, zmm8, zmm24, zmm14	;; R6s = (r1--s) - sine46*(i3--)			; 31-34		n 36
	zfmaddpd zmm15, zmm12, zmm24, zmm6	;; I6s = (i1--s) + sine46*(r3--)			; 31-34		n 36
	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	zfmaddpd zmm8, zmm8, zmm24, zmm14	;; R4s = (r1--s) + sine46*(i3--)			; 32-35		n 39
	zfnmaddpd zmm12, zmm12, zmm24, zmm6	;; I4s = (i1--s) - sine46*(r3--)			; 32-35		n 39
	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt

	;; Swap the four aparts
	vshuff64x2 zmm14, zmm3, zmm13, 01000100b;; R5_3 R5_2 R5_1 R5_0 R1_3 R1_2 R1_1 R1_0		; 33-35		n 43
	vshuff64x2 zmm3, zmm3, zmm13, 11101110b	;; R5_7	R5_6 R5_5 R5_4 R1_7 R1_6 R1_5 R1_4		; 34-36		n 43
	bump	screg, scinc

	zfmaddpd zmm19, zmm19, zmm23, zmm4	;; R8s = (r1-+s) + sine28*(i3-+)			; 33-36		n 40
	zfnmaddpd zmm2, zmm2, zmm23, zmm0	;; I8s = (i1-+s) - sine28*(r3-+)			; 34-37		n 40

	vshuff64x2 zmm0, zmm10, zmm17, 01000100b;; I5_3 I5_2 I5_1 I5_0 I1_3 I1_2 I1_1 I1_0		; 35-37		n 47
	vshuff64x2 zmm10, zmm10, zmm17, 11101110b;; I5_7 I5_6 I5_5 I5_4 I1_7 I1_6 I1_5 I1_4		; 36-38		n 47

	zfmsubpd zmm4, zmm9, zmm27, zmm1	;; R2s * cosine/sine - I2s (final R2)			; 35-38		n 41
	zfmaddpd zmm6, zmm18, zmm28, zmm15	;; R6s * cosine/sine + I6s (final R6)			; 36-39		n 41

	vshuff64x2 zmm17, zmm16, zmm7, 01000100b;; R7_3 R7_2 R7_1 R7_0 R3_3 R3_2 R3_1 R3_0		; 37-39		n 44
	vshuff64x2 zmm16, zmm16, zmm7, 11101110b;; R7_7 R7_6 R7_5 R7_4 R3_7 R3_6 R3_5 R3_4		; 38-40		n 44

	zfmaddpd zmm1, zmm1, zmm27, zmm9	;; I2s * cosine/sine + R2s (final I2)			; 37-40		n 43
	zfmsubpd zmm15, zmm15, zmm28, zmm18	;; I6s * cosine/sine - R6s (final I6)			; 38-41		n 43

	vshuff64x2 zmm7, zmm5, zmm11, 01000100b	;; I7_3 I7_2 I7_1 I7_0 I3_3 I3_2 I3_1 I3_0		; 39-41		n 48
	vshuff64x2 zmm5, zmm5, zmm11, 11101110b	;; I7_7	I7_6 I7_5 I7_4 I3_7 I3_6 I3_5 I3_4		; 40-42		n 48

	zfmsubpd zmm11, zmm8, zmm28, zmm12	;; R4s * cosine/sine - I4s (final R4)			; 39-42		n 45
	zfmaddpd zmm9, zmm19, zmm27, zmm2	;; R8s * cosine/sine + I8s (final R8)			; 40-43		n 45

	vshuff64x2 zmm18, zmm4, zmm6, 01000100b	;; R6_3 R6_2 R6_1 R6_0 R2_3 R2_2 R2_1 R2_0		; 41-43		n 51
	vshuff64x2 zmm4, zmm4, zmm6, 11101110b	;; R6_7	R6_6 R6_5 R6_4 R2_7 R2_6 R2_5 R2_4		; 42-44		n 51

	zfmaddpd zmm12, zmm12, zmm28, zmm8	;; I4s * cosine/sine + R4s (final I4)			; 41-44		n 47
	zfmsubpd zmm2, zmm2, zmm27, zmm19	;; I8s * cosine/sine - R8s (final I8)			; 42-45		n 47

	vshuff64x2 zmm6, zmm1, zmm15, 01000100b	;; I6_3 I6_2 I6_1 I6_0 I2_3 I2_2 I2_1 I2_0		; 43-45		n 53
	vshuff64x2 zmm1, zmm1, zmm15, 11101110b	;; I6_7	I6_6 I6_5 I6_4 I2_7 I2_6 I2_5 I2_4		; 44-46		n 53

	vaddpd	zmm8, zmm14, zmm3		;; add r1/r5 4-aparts					; 43-46		n 49
	vaddpd	zmm15, zmm17, zmm16		;; add r3/r7 4-aparts					; 44-47		n 49

	vshuff64x2 zmm13, zmm11, zmm9, 01000100b;; R8_3 R8_2 R8_1 R8_0 R4_3 R4_2 R4_1 R4_0		; 45-47		n 52
	vshuff64x2 zmm11, zmm11, zmm9, 11101110b;; R8_7 R8_6 R8_5 R8_4 R4_7 R4_6 R4_5 R4_4		; 46-48		n 52

	vsubpd	zmm14, zmm14, zmm3		;; sub r1/r5 4-aparts					; 45-48		n 51
	vsubpd	zmm17, zmm17, zmm16		;; sub r3/r7 4-aparts					; 46-49		n 51

	vshuff64x2 zmm9, zmm12, zmm2, 01000100b	;; I8_3 I8_2 I8_1 I8_0 I4_3 I4_2 I4_1 I4_0		; 47-49		n 54
	vshuff64x2 zmm12, zmm12, zmm2, 11101110b;; I8_7	I8_6 I8_5 I8_4 I4_7 I4_6 I4_5 I4_4		; 48-50		n 54

	vaddpd	zmm3, zmm0, zmm10		;; add i1/i5 4-aparts					; 47-50		n 53
	vaddpd	zmm16, zmm7, zmm5		;; add i3/i7 4-aparts					; 48-51		n 53

	;; Swap the two aparts
	vshuff64x2 zmm2, zmm8, zmm15, 10001000b	;; R7_1 R7_0 R3_1 R3_0 R5_1 R5_0 R1_1 R1_0 (r15/37+)	; 49-51		n 61
	vshuff64x2 zmm8, zmm8, zmm15, 11011101b	;; R7_3 R7_2 R3_3 R3_2 R5_3 R5_2 R1_3 R1_2		; 50-52		n 62

	vsubpd	zmm0, zmm0, zmm10		;; sub i1/i5 4-aparts					; 49-52		n 55
	vsubpd	zmm7, zmm7, zmm5		;; sub i3/i7 4-aparts					; 50-53		n 55

	vshuff64x2 zmm15, zmm14, zmm17, 10001000b;; R7_5 R7_4 R3_5 R3_4 R5_5 R5_4 R1_5 R1_4 (r15/37-)	; 51-53		n 73
	vshuff64x2 zmm14, zmm14, zmm17, 11011101b;; R7_7 R7_6 R3_7 R3_6 R5_7 R5_6 R1_7 R1_6		; 52-54		n 76

	vaddpd	zmm10, zmm18, zmm4		;; add r2/r6 4-aparts					; 51-54		n 57
	vaddpd	zmm5, zmm13, zmm11		;; add r4/r8 4-aparts					; 52-55		n 57

	vshuff64x2 zmm17, zmm3, zmm16, 10001000b;; I7_1 I7_0 I3_1 I3_0 I5_1 I5_0 I1_1 I1_0 (i15/37+)	; 53-55		n 65
	vshuff64x2 zmm3, zmm3, zmm16, 11011101b	;; I7_3 I7_2 I3_3 I3_2 I5_3 I5_2 I1_3 I1_2		; 54-56		n 66

	vaddpd	zmm16, zmm6, zmm1		;; add i2/i6 4-aparts					; 53-56		n 59  
	vaddpd	zmm19, zmm9, zmm12		;; add i4/i8 4-aparts					; 54-57		n 59

	vshuff64x2 zmm20, zmm0, zmm7, 10001000b	;; I7_5 I7_4 I3_5 I3_4 I5_5 I5_4 I1_5 I1_4 (i15/37-)	; 55-57		n 75
	vshuff64x2 zmm0, zmm0, zmm7, 11011101b	;; I7_7 I7_6 I3_7 I3_6 I5_7 I5_6 I1_7 I1_6		; 56-58		n 74

	vsubpd	zmm18, zmm18, zmm4		;; sub r2/r6 4-aparts					; 55-58		n 69
	vsubpd	zmm13, zmm13, zmm11		;; sub r4/r8 4-aparts					; 56-59		n 69

	vshuff64x2 zmm7, zmm10, zmm5, 10001000b	;; R8_1 R8_0 R4_1 R4_0 R6_1 R6_0 R2_1 R2_0 (r26/48+)	; 57-59		n 61
	vshuff64x2 zmm10, zmm10, zmm5, 11011101b;; R8_3 R8_2 R4_3 R4_2 R6_3 R6_2 R2_3 R2_2		; 58-60		n 62

	vsubpd	zmm6, zmm6, zmm1		;; sub i2/i6 4-aparts					; 57-60		n 71
	vsubpd	zmm9, zmm9, zmm12		;; sub i4/i8 4-aparts					; 58-61		n 71

	vshuff64x2 zmm4, zmm16, zmm19, 10001000b;; I8_1 I8_0 I4_1 I4_0 I6_1 I6_0 I2_1 I2_0 (i26/48+)	; 59-61		n 65
	vshuff64x2 zmm16, zmm16, zmm19, 11011101b;; I8_3 I8_2 I4_3 I4_2 I6_3 I6_2 I2_3 I2_2		; 60-62		n 66

	;; Swap the one aparts
	vshufpd	zmm11, zmm2, zmm7, 00000000b	;; R8_0 R7_0 R4_0 R3_0 R6_0 R5_0 R2_0 R1_0 (R1+R5 = new R1) ; 61	n 63
	vshufpd	zmm5, zmm8, zmm10, 00000000b	;; R8_2 R7_2 R4_2 R3_2 R6_2 R5_2 R2_2 R1_2 (R3+R7 = new R3) ; 62	n 63
	vshufpd	zmm2, zmm2, zmm7, 11111111b	;; R8_1 R7_1 R4_1 R3_1 R6_1 R5_1 R2_1 R1_1 (R2+R6=  new R2) ; 63	n 65
	vshufpd	zmm8, zmm8, zmm10, 11111111b	;; R8_3 R7_3 R4_3 R3_3 R6_3 R5_3 R2_3 R1_3 (R4+R8 = new R4) ; 64	n 65

	vaddpd	zmm19, zmm11, zmm5		;; R1 + R3 (newer R1)					; 63-66		n 73
	vsubpd	zmm11, zmm11, zmm5		;; R1 - R3 (newer R3)					; 64-67		n 84

	vshufpd	zmm7, zmm17, zmm4, 00000000b	;; I8_0 I7_0 I4_0 I3_0 I6_0 I5_0 I2_0 I1_0 (I1+I5 = new I1) ; 65	n 67
	vshufpd	zmm10, zmm3, zmm16, 00000000b	;; I8_2 I7_2 I4_2 I3_2 I6_2 I5_2 I2_2 I1_2 (I3+I7 = new I3) ; 66	n 67

	vaddpd	zmm5, zmm2, zmm8		;; R2 + R4 (newer R2)					; 65-68		n 71
	vsubpd	zmm2, zmm2, zmm8		;; R2 - R4 (newer R4)					; 66-69		n 86

	vshufpd	zmm17, zmm17, zmm4, 11111111b	;; I8_1 I7_1 I4_1 I3_1 I6_1 I5_1 I2_1 I1_1 (I2+I6 = new I2) ; 67	n 69
	vshufpd	zmm3, zmm3, zmm16, 11111111b	;; I8_3 I7_3 I4_3 I3_3 I6_3 I5_3 I2_3 I1_3 (I4+I8 = new I4) ; 68	n 69

	vaddpd	zmm8, zmm7, zmm10		;; I1 + I3 (newer I1)					; 67-70		n 73
	vsubpd	zmm7, zmm7, zmm10		;; I1 - I3 (newer I3)					; 68-71		n 85

	vshuff64x2 zmm4, zmm18, zmm13, 10001000b;; R8_5 R8_4 R4_5 R4_4 R6_5 R6_4 R2_5 R2_4 (r26/48-)	; 69-71		n 73
	vshuff64x2 zmm18, zmm18, zmm13, 11011101b;; R8_7 R8_6 R4_7 R4_6 R6_7 R6_6 R2_7 R2_6		; 70-72		n 76

	vaddpd	zmm16, zmm17, zmm3		;; I2 + I4 (newer I2)					; 69-72		n 73
	vsubpd	zmm17, zmm17, zmm3		;; I2 - I4 (newer I4)					; 70-73		n 84

	vshuff64x2 zmm10, zmm6, zmm9, 11011101b	;; I8_7	I8_6 I4_7 I4_6 I6_7 I6_6 I2_7 I2_6 (i26/48-)	; 71-73		n 74
	vshuff64x2 zmm6, zmm6, zmm9, 10001000b	;; I8_5 I8_4 I4_5 I4_4 I6_5 I6_4 I2_5 I2_4		; 72-74		n 75

	vaddpd	zmm13, zmm19, zmm5		;; R1 + R2 (final R1)					; 71-74		n 88
	vsubpd	zmm19, zmm19, zmm5		;; R1 - R2 (final R2)					; 72-75		n 89

	vshufpd	zmm3, zmm15, zmm4, 11111111b	;; R8_5 R7_5 R4_5 R3_5 R6_5 R5_5 R2_5 R1_5 (R2-R6 = new R6) ; 73	n 75
	vshufpd	zmm9, zmm0, zmm10, 11111111b	;; I8_7	I7_7 I4_7 I3_7 I6_7 I5_7 I2_7 I1_7 (I4-I8 = new I8) ; 74	n 75

	vaddpd	zmm5, zmm8, zmm16		;; I1 + I2 (final I1)					; 73-76		n 88
	vsubpd	zmm8, zmm8, zmm16		;; I1 - I2 (final I2)					; 74-77		n 89

	vshufpd	zmm16, zmm20, zmm6, 11111111b	;; I8_5 I7_5 I4_5 I3_5 I6_5 I5_5 I2_5 I1_5 (I2-I6 = new I6) ; 75	n 77
	vshufpd	zmm1, zmm14, zmm18, 11111111b	;; R8_7	R7_7 R4_7 R3_7 R6_7 R5_7 R2_7 R1_7 (R4-R8 = new R8) ; 76	n 77

	vsubpd	zmm12, zmm3, zmm9		;; R6 - I8 (new2 R6)					; 75-78		n 82
	vaddpd	zmm3, zmm3, zmm9		;; R6 + I8 (new2 R8)					; 76-79		n 83

	vshufpd	zmm20, zmm20, zmm6, 00000000b	;; I8_4 I7_4 I4_4 I3_4 I6_4 I5_4 I2_4 I1_4 (I1-I5 = new I5) ; 77	n 79
	vshufpd	zmm14, zmm14, zmm18, 00000000b	;; R8_6	R7_6 R4_6 R3_6 R6_6 R5_6 R2_6 R1_6 (R3-R7 = new R7) ; 78	n 79

	vaddpd	zmm9, zmm16, zmm1		;; I6 + R8 (new2 I6)					; 77-80		n 82
	vsubpd	zmm16, zmm16, zmm1		;; I6 - R8 (new2 I8)					; 78-81		n 83

	vshufpd	zmm15, zmm15, zmm4, 00000000b	;; R8_4 R7_4 R4_4 R3_4 R6_4 R5_4 R2_4 R1_4 (R1-R5 = new R5) ; 79	n 81
	vshufpd	zmm0, zmm0, zmm10, 00000000b	;; I8_6	I7_6 I4_6 I3_6 I6_6 I5_6 I2_6 I1_6 (I3-I7 = new I7) ; 80	n 81

	vaddpd	zmm6, zmm20, zmm14		;; I5 + R7 (newer I5)					; 79-82		n 86
	vsubpd	zmm20, zmm20, zmm14		;; I5 - R7 (newer I7)					; 80-83		n 87

	vsubpd	zmm18, zmm15, zmm0		;; R5 - I7 (newer R5)					; 81-83		n 91
	vaddpd	zmm15, zmm15, zmm0		;; R5 + I7 (newer R7)					; 81-84		n 98

	vaddpd	zmm4, zmm12, zmm9		;; I6 = R6 + I6 (newer I6/SQRTHALF)			; 82-85		n 86
	vsubpd	zmm12, zmm12, zmm9		;; R6 = R6 - I6 (newer R6/SQRTHALF)			; 82-85		n 91

	vsubpd	zmm10, zmm3, zmm16		;; R8 = R8 - I8 (newer R8/SQRTHALF)			; 83-86		n 87
	vaddpd	zmm3, zmm3, zmm16		;; I8 = R8 + I8 (newer I8/SQRTHALF)			; 83-86		n 98

	vsubpd	zmm14, zmm11, zmm17		;; R3 - I4 (final R3)					; 84-87		n 90
	vaddpd	zmm11, zmm11, zmm17		;; R3 + I4 (final R4)					; 84-87		n 95

	vaddpd	zmm0, zmm7, zmm2		;; I3 + R4 (final I3)					; 85-88		n 90
	vsubpd	zmm7, zmm7, zmm2		;; I3 - R4 (final I4)					; 85-88		n 95

	zfmaddpd zmm9, zmm4, zmm31, zmm6	;; I5 + I6 * SQRTHALF (final I5)			; 86-89		n 96
	zfnmaddpd zmm4, zmm4, zmm31, zmm6	;; I5 - I6 * SQRTHALF (final I6)			; 86-89		n 94

	zfmaddpd zmm2, zmm10, zmm31, zmm20	;; I7 + R8 * SQRTHALF (final I7)			; 87-90		n 102
	zfnmaddpd zmm10, zmm10, zmm31, zmm20	;; I7 - R8 * SQRTHALF (final I8)			; 87-90		n 103

	cmp	mul4_opcode, 0			;; See if we need to do more than the original type-3 FFT multiply
	je	short orig
	call	zcomplex_mult_opcode		;; Handle more difficult cases
	jmp	back_to_orig

orig:
	;; Multiply the complex numbers
	vmovapd	zmm30, [srcreg+rbp]		;; MemR1
	vmulpd	zmm20, zmm13, zmm30		;; A1 = R1 * MemR1					; 88-91		n 92
	vmulpd	zmm30, zmm5, zmm30		;; B1 = I1 * MemR1					; 88-91		n 92

	vmovapd	zmm21, [srcreg+rbp+d1]		;; MemR2
	vmulpd	zmm6, zmm19, zmm21		;; A2 = R2 * MemR2					; 89-92		n 93
	vmulpd	zmm21, zmm8, zmm21		;; B2 = I2 * MemR2					; 89-92		n 93

	vmovapd	zmm1, [srcreg+rbp+d2]		;; MemR3
	vmulpd	zmm17, zmm14, zmm1		;; A3 = R3 * MemR3					; 90-93		n 94
	vmulpd	zmm1, zmm0, zmm1		;; B3 = I3 * MemR3					; 90-93		n 94

	zfmaddpd zmm16, zmm12, zmm31, zmm18	;; R5 + R6 * SQRTHALF (final R5)			; 91-94		n 96
	zfnmaddpd zmm12, zmm12, zmm31, zmm18	;; R5 - R6 * SQRTHALF (final R6)			; 91-94		n 97

	vmovapd zmm18, [srcreg+rbp+64]		;; MemI1
	zfnmaddpd zmm20, zmm5, zmm18, zmm20	;; A1 - I1 * MemI1 (R1)					; 92-95		n 110
	zfmaddpd zmm18, zmm13, zmm18, zmm30	;; B1 + R1 * MemI1 (I1)					; 92-95		n 104

	vmovapd zmm30, [srcreg+rbp+d1+64]	;; MemI2
	zfnmaddpd zmm6, zmm8, zmm30, zmm6	;; A2 - I2 * MemI2 (R2)					; 93-96		n 110
	zfmaddpd zmm21, zmm19, zmm30, zmm21	;; B2 + R2 * MemI2 (I2)					; 93-96		n 104

	vmovapd zmm30, [srcreg+rbp+d2+64]	;; MemI3
	zfnmaddpd zmm17, zmm0, zmm30, zmm17	;; A3 - I3 * MemI3 (R3)					; 94-97		n 113
	zfmaddpd zmm1, zmm14, zmm30, zmm1	;; B3 + R3 * MemI3 (I3)					; 94-97		n 108

	vmovapd	zmm30, [srcreg+rbp+d2+d1]	;; MemR4
	vmulpd	zmm5, zmm11, zmm30		;; A4 = R4 * MemR4					; 95-98		n 99
	vmulpd	zmm13, zmm7, zmm30		;; B4 = I4 * MemR4					; 95-98		n 99

	vmovapd	zmm30, [srcreg+rbp+d4]		;; MemR5
	vmulpd	zmm8, zmm16, zmm30		;; A5 = R5 * MemR5					; 96-99		n 100
	vmulpd	zmm19, zmm9, zmm30		;; B5 = I5 * MemR5					; 96-99		n 100

	vmovapd	zmm30, [srcreg+rbp+d4+d1]	;; MemR6
	vmulpd	zmm0, zmm12, zmm30		;; A6 = R6 * MemR6					; 97-100	n 101
	vmulpd	zmm14, zmm4, zmm30		;; B6 = I6 * MemR6					; 97-100	n 101

	zfnmaddpd zmm30, zmm3, zmm31, zmm15	;; R7 - I8 * SQRTHALF (final R7)			; 98-101	n 102
	zfmaddpd zmm3, zmm3, zmm31, zmm15	;; R7 + I8 * SQRTHALF (final R8)			; 98-101	n 103

	vmovapd zmm15, [srcreg+rbp+d2+d1+64]	;; MemI4
	zfnmaddpd zmm5, zmm7, zmm15, zmm5	;; A4 - I4 * MemI4 (R4)					; 99-102	n 113
	zfmaddpd zmm13, zmm11, zmm15, zmm13	;; B4 + R4 * MemI4 (I4)					; 99-102	n 108

	vmovapd zmm15, [srcreg+rbp+d4+64]	;; MemI5
	zfnmaddpd zmm8, zmm9, zmm15, zmm8	;; A5 - I5 * MemI5 (R5)					; 100-103	n 109
	zfmaddpd zmm19, zmm16, zmm15, zmm19	;; B5 + R5 * MemI5 (I5)					; 100-103	n 105

	vmovapd zmm15, [srcreg+rbp+d4+d1+64]	;; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0	;; A6 - I6 * MemI6 (R6)					; 101-104	n 109
	zfmaddpd zmm14, zmm12, zmm15, zmm14	;; B6 + R6 * MemI6 (I6)					; 101-104	n 105

	vmovapd	zmm15, [srcreg+rbp+d4+d2]	;; MemR7
	vmulpd	zmm7, zmm30, zmm15		;; A7 = R7 * MemR7					; 102-105	n 106
	vmulpd	zmm11, zmm2, zmm15		;; B7 = I7 * MemR7					; 102-105	n 106

	vmovapd	zmm15, [srcreg+rbp+d4+d2+d1]	;; MemR8
	vmulpd	zmm9, zmm3, zmm15		;; A8 = R8 * MemR8					; 103-106	n 107
	vmulpd	zmm16, zmm10, zmm15		;; B8 = I8 * MemR8					; 103-106	n 107

	vaddpd	zmm4, zmm18, zmm21		;; I1 + I2 (new I1)					; 104-107	n 114
	vsubpd	zmm18, zmm18, zmm21		;; I1 - I2 (new I2)					; 104-107	n 121

	vaddpd	zmm12, zmm19, zmm14		;; I5 + I6 (new I5)					; 105-108	n 116
	vsubpd	zmm14, zmm19, zmm14		;; I5 - I6 (new I6)					; 105-108	n 117

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]	;; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7	;; A7 - I7 * MemI7 (R7)					; 106-109	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11	;; B7 + R7 * MemI7 (I7)					; 106-109	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]	;; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9	;; A8 - I8 * MemI8 (R8)					; 107-110	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16	;; B8 + R8 * MemI8 (I8)					; 107-110	n 112

back_to_orig:
	vaddpd	zmm21, zmm1, zmm13		;; I3 + I4 (new I3)					; 108-111	n 114
	vsubpd	zmm1, zmm1, zmm13		;; I3 - I4 (new R4)					; 108-111	n 127

	vaddpd	zmm13, zmm8, zmm0		;; R5 + R6 (new R5)					; 109-112	n 115
	vsubpd	zmm8, zmm8, zmm0		;; R5 - R6 (new R6)					; 109-112	n 117

	vaddpd	zmm10, zmm20, zmm6		;; R1 + R2 (new R1)					; 110-113	n 119
	vsubpd	zmm20, zmm20, zmm6		;; R1 - R2 (new R2)					; 110-113	n 127

	vaddpd	zmm3, zmm9, zmm7		;; R8 + R7 (new R7)					; 111-114	n 115
	vsubpd	zmm9, zmm9, zmm7		;; R8 - R7 (new I8)					; 111-114	n 118

	vaddpd	zmm15, zmm11, zmm16		;; I7 + I8 (new I7)					; 112-115	n 116
	vsubpd	zmm11, zmm11, zmm16		;; I7 - I8 (new R8)					; 112-115	n 118

 	vaddpd	zmm0, zmm5, zmm17		;; R4 + R3 (new R3)					; 113-116	n 119
	vsubpd	zmm5, zmm5, zmm17		;; R4 - R3 (new I4)					; 113-116	n 121

	vaddpd	zmm7, zmm4, zmm21		;; I1 + I3 (newer I1)					; 114-117	n 119
	vsubpd	zmm4, zmm4, zmm21		;; I1 - I3 (newer I3)					; 114-117	n 119

	vsubpd	zmm6, zmm3, zmm13		;; R7 - R5 (newer I7)					; 115-118	n 121
	vaddpd	zmm3, zmm3, zmm13		;; R7 + R5 (newer R5)					; 115-118	n 123

	vaddpd	zmm17, zmm12, zmm15		;; I5 + I7 (newer I5)					; 116-119	n 121
	vsubpd	zmm15, zmm12, zmm15		;; I5 - I7 (newer R7)					; 116-119	n 123

	;; multiply R6/I6 by SQRTHALF - i*SQRTHALF
	vsubpd	zmm21, zmm14, zmm8		;; I6 = I6 - R6						; 117-120	n 123
	vaddpd	zmm14, zmm8, zmm14		;; R6 = R6 + I6						; 117-120	n 125

	;; multiply R8/I8 by SQRTHALF - i*SQRTHALF
	vsubpd	zmm13, zmm9, zmm11		;; I8 = I8 - R8						; 118-121	n 123
	vaddpd	zmm11, zmm11, zmm9		;; R8 = R8 + I8						; 118-121	n 125

	;; shuffle inputs are:
	;; R1 = R8_0 R7_0 R4_0 R3_0 R6_0 R5_0 R2_0 R1_0	goal new R1 is  R1_7 R1_6 R1_5 R1_4 R1_3 R1_2 R1_1 R1_0
	;; R2 = R8_1 R7_1 R4_1 R3_1 R6_1 R5_1 R2_1 R1_1	goal new R2 is  R2_7 R2_6 R2_5 R2_4 R2_3 R2_2 R2_1 R2_0, etc.
	;; I1 = I8_0 I7_0 I4_0 I3_0 I6_0 I5_0 I2_0 I1_0	goal new I1 is  I1_7 I1_6 I1_5 I1_4 I1_3 I1_2 I1_1 I1_0, etc.
	;; shuffle the two aparts		(can be done before last FFT level!)
	vshuff64x2 zmm9, zmm7, zmm4, 11101110b	;; I8_2 I7_2 I4_2 I3_2 I8_0 I7_0 I4_0 I3_0 (I13H)	; 119-121	n 129
	vshuff64x2 zmm7, zmm7, zmm4, 01000100b	;; I6_2 I5_2 I2_2 I1_2 I6_0 I5_0 I2_0 I1_0 (I13L)	; 120-122	n 137

	vaddpd	zmm4, zmm10, zmm0		;; R1 + R3 (newer R1)					; 119-122	n 125
	vsubpd	zmm10, zmm10, zmm0		;; R1 - R3 (newer R3)					; 120-123	n 125

	vshuff64x2 zmm0, zmm17, zmm6, 11101110b;; I8_6 I7_6 I4_6 I3_6 I8_4 I7_4 I4_4 I3_4 (I57H)	; 121-123	n 129
	vshuff64x2 zmm17, zmm17, zmm6, 01000100b;; I6_6 I5_6 I2_6 I1_6 I6_4 I5_4 I2_4 I1_4 (I57L)	; 122-124	n 137

 	vaddpd	zmm6, zmm18, zmm5		;; I2 + I4 (newer I2)					; 121-124	n 127
	vsubpd	zmm18, zmm18, zmm5		;; I2 - I4 (newer I4)					; 122-125	n 127

	vshuff64x2 zmm8, zmm3, zmm15, 11101110b;; R8_6 R7_6 R4_6 R3_6 R8_4 R7_4 R4_4 R3_4 (R57H)	; 123-125	n 131
	vshuff64x2 zmm3, zmm3, zmm15, 01000100b;; R6_6 R5_6 R2_6 R1_6 R6_4 R5_4 R2_4 R1_4 (R57L)	; 124-126	n 139

	vaddpd	zmm15, zmm21, zmm13		;; I6 + I8 (newer I6/SQRTHALF)				; 123-126	n 129
	vsubpd	zmm21, zmm21, zmm13		;; I6 - I8 (newer R8/SQRTHALF)				; 124-127	n 131

	vshuff64x2 zmm13, zmm4, zmm10, 11101110b;; R8_2 R7_2 R4_2 R3_2 R8_0 R7_0 R4_0 R3_0 (R13H)	; 125-127	n 131
	vshuff64x2 zmm4, zmm4, zmm10, 01000100b	;; R6_2 R5_2 R2_2 R1_2 R6_0 R5_0 R2_0 R1_0 (R13L)	; 126-128	n 139

	vsubpd	zmm10, zmm11, zmm14		;; R8 - R6 (newer I8/SQRTHALF)				; 125-128	n 129
	vaddpd	zmm11, zmm11, zmm14		;; R8 + R6 (newer R6/SQRTHALF)				; 126-129	n 131

	vshuff64x2 zmm14, zmm6, zmm18, 11101110b;; I8_3 I7_3 I4_3 I3_3 I8_1 I7_1 I4_1 I3_1 (I24H)	; 127-129	n 133
	vshuff64x2 zmm6, zmm6, zmm18, 01000100b;; I6_3 I5_3 I2_3 I1_3 I6_1 I5_1 I2_1 I1_1 (I24L)	; 128-130	n 141

	vaddpd	zmm18, zmm20, zmm1		;; R2 + R4 (newer R2)					; 127-130	n 132
	vsubpd	zmm1, zmm20, zmm1		;; R2 - R4 (newer R4)					; 128-131	n 132

	vshuff64x2 zmm20, zmm15, zmm10, 11101110b;; I8_7 I7_7 I4_7 I3_7 I8_5 I7_5 I4_5 I3_5 (I68H)	; 129-131	n 133
	vshuff64x2 zmm15, zmm15, zmm10, 01000100b;; I6_7 I5_7 I2_7 I1_7 I6_5 I5_5 I2_5 I1_5 (I68L)	; 130-132	n 141

	vaddpd	zmm10, zmm9, zmm0		;; I13H + I57H (final I13H)				; 129-132	n 135
	vsubpd	zmm9, zmm9, zmm0		;; I13H - I57H (final I57H)				; 130-133	n 135

	vshuff64x2 zmm5, zmm11, zmm21, 11101110b;; R8_7 R7_7 R4_7 R3_7 R8_5 R7_5 R4_5 R3_5 (R68H)	; 131-133	n 135
	vshuff64x2 zmm12, zmm18, zmm1, 11101110b;; R8_3 R7_3 R4_3 R3_3 R8_1 R7_1 R4_1 R3_1 (R24H)	; 132-134	n 135	r132

	vaddpd	zmm0, zmm13, zmm8		;; R13H + R57H (final R13H)				; 131-134	n 137
	vsubpd	zmm13, zmm13, zmm8		;; R13H - R57H (final R57H)				; 132-135	n 137

	vshuff64x2 zmm11, zmm11, zmm21, 01000100b;; R6_7 R5_7 R2_7 R1_7 R6_5 R5_5 R2_5 R1_5 (R68L)	; 133-135	n 143
	vshuff64x2 zmm18, zmm18, zmm1, 01000100b;; R6_3 R5_3 R2_3 R1_3 R6_1 R5_1 R2_1 R1_1 (R24L)	; 134-136	n 143

	zfmaddpd zmm8, zmm20, zmm31, zmm14	;; I24H + I68H * SQRTHALF (final I24H)			; 133-136	n 138	r133
	zfnmaddpd zmm20, zmm20, zmm31, zmm14	;; I24H - I68H * SQRTHALF (final I68H)			; 134-137	n 138

	vshuff64x2 zmm21, zmm10, zmm9, 11011101b	;; I8_6 I7_6 I8_4 I7_4 I8_2 I7_2 I8_0 I7_0	; 135-137	n 143	r134
	vshuff64x2 zmm10, zmm10, zmm9, 10001000b;; I4_6 I3_6 I4_4 I3_4 I4_2 I3_2 I4_0 I3_0		; 136-138	n 147	r134

	zfmaddpd zmm1, zmm5, zmm31, zmm12	;; R24H + R68H * SQRTHALF (final R24H)			; 135-138	n 140	r135
	zfnmaddpd zmm5, zmm5, zmm31, zmm12	;; R24H - R68H * SQRTHALF (final R68H)			; 136-139	n 140

	vshuff64x2 zmm9, zmm0, zmm13, 11011101b	;; R8_6 R7_6 R8_4 R7_4 R8_2 R7_2 R8_0 R7_0		; 137-139	n 144	r136
	vshuff64x2 zmm14, zmm8, zmm20, 11011101b ;; I8_7 I7_7 I8_5 I7_5 I8_3 I7_3 I8_1 I7_1		; 138-140	n 143	r138

	vaddpd	zmm12, zmm7, zmm17		;; I13L + I57L (final I13L)				; 137-140	n 151
	vsubpd	zmm7, zmm7, zmm17		;; I13L - I57L (final I57L)				; 138-141	n 151

	vshuff64x2 zmm8, zmm8, zmm20, 10001000b	;; I4_7 I3_7 I4_5 I3_5 I4_3 I3_3 I4_1 I3_1		; 139-141	n 147	r138
	vshuff64x2 zmm17, zmm1, zmm5, 11011101b;; R8_7 R7_7 R8_5 R7_5 R8_3 R7_3 R8_1 R7_1		; 140-142	n 144	r140

	vaddpd	zmm20, zmm4, zmm3		;; R13L + R57L (final R13L)				; 139-142	n 154
	vsubpd	zmm4, zmm4, zmm3		;; R13L - R57L (final R57L)				; 140-143	n 154

	vshuff64x2 zmm0, zmm0, zmm13, 10001000b;; R4_6 R3_6 R4_4 R3_4 R4_2 R3_2 R4_0 R3_0		; 141-143	n 148	r136
	vshuff64x2 zmm1, zmm1, zmm5, 10001000b;; R4_7 R3_7 R4_5 R3_5 R4_3 R3_3 R4_1 R3_1		; 142-144	n 148	r140

	zfmaddpd zmm3, zmm15, zmm31, zmm6	;; I24L + I68L * SQRTHALF (final I24L)			; 141-144	n 152
	zfnmaddpd zmm15, zmm15, zmm31, zmm6	;; I24L - I68L * SQRTHALF (final I68L)			; 142-145	n 152

	vshufpd	zmm13, zmm21, zmm14, 11111111b	;; I8_7 I8_6 I8_5 I8_4 I8_3 I8_2 I8_1 I8_0 (next I8)	; 143		n 145
	vshufpd	zmm6, zmm9, zmm17, 11111111b	;; R8_7 R8_6 R8_5 R8_4 R8_3 R8_2 R8_1 R8_0 (next R8)	; 144		n 145

	zfmaddpd zmm5, zmm11, zmm31, zmm18	;; R24L + R68L * SQRTHALF (final R24L)			; 143-146	n 153
	zfnmaddpd zmm11, zmm11, zmm31, zmm18	;; R24L - R68L * SQRTHALF (final R68L)			; 144-147	n 153

	vshufpd	zmm21, zmm21, zmm14, 00000000b	;; I7_7 I7_6 I7_5 I7_4 I7_3 I7_2 I7_1 I7_0 (next I7)	; 145		n 147
	vshufpd	zmm9, zmm9, zmm17, 00000000b	;; R7_7 R7_6 R7_5 R7_4 R7_3 R7_2 R7_1 R7_0 (next R7)	; 146		n 147

	zfmsubpd zmm18, zmm6, zmm27, zmm13	;; A8 = R8 * cosine/sine - I8				; 145-148	n 157
	zfmaddpd zmm13, zmm13, zmm27, zmm6	;; B8 = I8 * cosine/sine + R8				; 146-149	n 158

	vshufpd	zmm14, zmm10, zmm8, 00000000b	;; I3_7 I3_6 I3_5 I3_4 I3_3 I3_2 I3_1 I3_0 (next I3)	; 147		n 149
	vshufpd	zmm17, zmm0, zmm1, 00000000b	;; R3_7 R3_6 R3_5 R3_4 R3_3 R3_2 R3_1 R3_0 (next R3)	; 148		n 149

	zfmsubpd zmm6, zmm9, zmm26, zmm21	;; R7 * cosine/sine - I7 (first R7/sine)		; 147-150	n 153
	zfmaddpd zmm21, zmm21, zmm26, zmm9	;; I7 * cosine/sine + R7 (first I7/sine)		; 148-151	n 155

	vshufpd	zmm10, zmm10, zmm8, 11111111b	;; I4_7 I4_6 I4_5 I4_4 I4_3 I4_2 I4_1 I4_0 (next I4)	; 149		n 151
 	vshufpd	zmm0, zmm0, zmm1, 11111111b	;; R4_7 R4_6 R4_5 R4_4 R4_3 R4_2 R4_1 R4_0 (next R4)	; 150		n 151

	zfmaddpd zmm9, zmm17, zmm26, zmm14	;; R3 * cosine/sine + I3 (first R3/sine)		; 149-152	n 153
	zfmsubpd zmm14, zmm14, zmm26, zmm17	;; I3 * cosine/sine - R3 (first I3/sine)		; 150-153	n 155

	;; shuffle the four aparts
	vshuff64x2 zmm8, zmm12, zmm7, 11011101b;; I6_6 I5_6 I6_4 I5_4 I6_2 I5_2 I6_0 I5_0		; 151-153	n 157
	vshuff64x2 zmm1, zmm3, zmm15, 11011101b;; I6_7 I5_7 I6_5 I5_5 I6_3 I5_3 I6_1 I5_1		; 152-154	n 157

	zfmaddpd zmm17, zmm0, zmm28, zmm10	;; R4 * cosine/sine + I4 (first R4/sine)		; 151-154	n 165
	zfmsubpd zmm10, zmm10, zmm28, zmm0	;; I4 * cosine/sine - R4 (first I4/sine)		; 152-155	n 166

	vshuff64x2 zmm0, zmm5, zmm11, 11011101b;; R6_7 R5_7 R6_5 R5_5 R6_3 R5_3 R6_1 R5_1		; 153-155	n 158
	vshuff64x2 zmm19, zmm20, zmm4, 11011101b;; R6_6 R5_6 R6_4 R5_4 R6_2 R5_2 R6_0 R5_0		; 154-156	n 158

	vaddpd	zmm2, zmm9, zmm6		;; R3/sine + R7/sine					; 153-156	n 174
	vsubpd	zmm9, zmm9, zmm6		;; R3/sine - R7/sine					; 154-157	n 171

	vshuff64x2 zmm12, zmm12, zmm7, 10001000b;; I2_6 I1_6 I2_4 I1_4 I2_2 I1_2 I2_0 I1_0		; 155-157	n 161
	vshuff64x2 zmm3, zmm3, zmm15, 10001000b;; I2_7 I1_7 I2_5 I1_5 I2_3 I1_3 I2_1 I1_1		; 156-158	n 161

	vaddpd	zmm6, zmm14, zmm21		;; I3/sine + I7/sine					; 155-158	n 170
	vsubpd	zmm14, zmm14, zmm21		;; I3/sine - I7/sine					; 156-159	n 175

	;; shufpd the one aparts
	vshufpd	zmm7, zmm8, zmm1, 00000000b	;; I5_7 I5_6 I5_5 I5_4 I5_3 I5_2 I5_1 I5_0 (next I5)	; 157		n 159
	vshufpd	zmm15, zmm19, zmm0, 00000000b	;; R5_7 R5_6 R5_5 R5_4 R5_3 R5_2 R5_1 R5_0 (next R5)	; 158		n 159

	vmulpd	zmm18, zmm18, zmm23		;; R8 = R8 * sine28					; 157-160	n 172
	vmulpd	zmm13, zmm13, zmm23		;; I8 = I8 * sine28					; 158-161	n 173

	vshufpd	zmm8, zmm8, zmm1, 11111111b	;; I6_7 I6_6 I6_5 I6_4 I6_3 I6_2 I6_1 I6_0 (next I6)	; 159		n 161
	vshufpd	zmm19, zmm19, zmm0, 11111111b	;; R6_7 R6_6 R6_5 R6_4 R6_3 R6_2 R6_1 R6_0 (next R6)	; 160		n 161

	zfmsubpd zmm21, zmm7, zmm29, zmm15	;; I5 * cosine/sine - R5 (first I5/sine)		; 159-162	n 163
	zfmaddpd zmm15, zmm15, zmm29, zmm7	;; R5 * cosine/sine + I5 (first R5/sine)		; 160-163	n 169

	vshufpd	zmm1, zmm12, zmm3, 00000000b	;; I1_7 I1_6 I1_5 I1_4 I1_3 I1_2 I1_1 I1_0 (next I1)	; 161		n 163
	vshuff64x2 zmm20, zmm20, zmm4, 10001000b;; R2_6 R1_6 R2_4 R1_4 R2_2 R1_2 R2_0 R1_0		; 162-164	n 166

	zfmsubpd zmm0, zmm19, zmm28, zmm8	;; R6 * cosine/sine - I6 (first R6/sine)		; 161-164	n 165
	zfmaddpd zmm8, zmm8, zmm28, zmm19	;; I6 * cosine/sine + R6 (first I6/sine)		; 162-165	n 166

	vshuff64x2 zmm5, zmm5, zmm11, 10001000b	;; R2_7 R1_7 R2_5 R1_5 R2_3 R1_3 R2_1 R1_1		; 163-165	n 166
	vshufpd	zmm12, zmm12, zmm3, 11111111b	;; I2_7 I2_6 I2_5 I2_4 I2_3 I2_2 I2_1 I2_0 (next I2)	; 164		n 168

	zfmaddpd zmm7, zmm21, zmm25, zmm1	;; I1 + I5 * sine					; 163-166	n 170
	zfnmaddpd zmm21, zmm21, zmm25, zmm1	;; I1 - I5 * sine					; 164-167	n 171

	vaddpd	zmm4, zmm17, zmm0		;; R4/sine + R6/sine					; 165-168	n 176
	vsubpd	zmm17, zmm17, zmm0		;; R4/sine - R6/sine					; 165-168	n 177

	vshufpd	zmm11, zmm20, zmm5, 11111111b	;; R2_7 R2_6 R2_5 R2_4 R2_3 R2_2 R2_1 R2_0 (next R2)	; 166		n 168
	vshufpd	zmm20, zmm20, zmm5, 00000000b	;; R1_7 R1_6 R1_5 R1_4 R1_3 R1_2 R1_1 R1_0 (next R1)	; 167		n 169

	vaddpd	zmm3, zmm10, zmm8		;; I4/sine + I6/sine					; 166-169	n 179
	vsubpd	zmm10, zmm10, zmm8		;; I4/sine - I6/sine					; 167-170	n 178

	zfmaddpd zmm1, zmm11, zmm27, zmm12	;; R2 * cosine/sine + I2 (first R2/sine)		; 168-171	n 172	r167
	zfmsubpd zmm12, zmm12, zmm27, zmm11	;; I2 * cosine/sine - R2 (first I2/sine)		; 168-171	n 173

	zfmaddpd zmm0, zmm15, zmm25, zmm20	;; R1 + R5 * sine					; 169-172	n 174	r168
	zfnmaddpd zmm15, zmm15, zmm25, zmm20	;; R1 - R5 * sine					; 169-172	n 175

	zfmaddpd zmm8, zmm6, zmm22, zmm7	;; i1++ = (i1+i5) + (i3+i7) * sine			; 170-173	n 184
	zfnmaddpd zmm6, zmm6, zmm22, zmm7	;; i1+- = (i1+i5) - (i3+i7) * sine			; 170-173	n 181

	zfmaddpd zmm11, zmm9, zmm22, zmm21	;; i1-+ = (i1-i5) + (r3-r7) * sine			; 171-174	n 189
	zfnmaddpd zmm9, zmm9, zmm22, zmm21	;; i1-- = (i1-i5) - (r3-r7) * sine			; 171-174	n 188

	zfmaddpd zmm20, zmm1, zmm23, zmm18	;; R2 * sine + R8					; 172-175	n 176	r171
	zfmsubpd zmm1, zmm1, zmm23, zmm18	;; R2 * sine - R8					; 172-175	n 177

	zfmaddpd zmm21, zmm12, zmm23, zmm13	;; I2 * sine + I8					; 173-176	n 179	r171
	zfmsubpd zmm12, zmm12, zmm23, zmm13	;; I2 * sine - I8					; 173-176	n 178

	zfmaddpd zmm18, zmm2, zmm22, zmm0	;; r1++ = (r1+r5) + (r3+r7) * sine			; 174-177	n 180	r173
	zfnmaddpd zmm2, zmm2, zmm22, zmm0	;; r1+- = (r1+r5) - (r3+r7) * sine			; 174-177	n 185

	zfmaddpd zmm13, zmm14, zmm22, zmm15	;; r1-+ = (r1-r5) + (i3-i7) * sine			; 175-178	n 186
	zfnmaddpd zmm14, zmm14, zmm22, zmm15	;; r1-- = (r1-r5) - (i3-i7) * sine			; 175-178	n 187

	zfmaddpd zmm0, zmm4, zmm24, zmm20	;; r2++ = (r2+r8) + (r4+r6) * sine			; 176-179	n 180
	zfnmaddpd zmm4, zmm4, zmm24, zmm20	;; r2+- = (r2+r8) - (r4+r6) * sine			; 176-179	n 182

	zfmaddpd zmm15, zmm17, zmm24, zmm1	;; r2-+ = (r2-r8) + (r4-r6) * sine			; 177-180	n 183
	zfnmaddpd zmm17, zmm17, zmm24, zmm1	;; r2-- = (r2-r8) - (r4-r6) * sine			; 177-180	n 181

	zfmaddpd zmm20, zmm10, zmm24, zmm12	;; i2-+ = (i2-i8) + (i4-i6) * sine			; 178-181	n 182
	zfnmaddpd zmm10, zmm10, zmm24, zmm12	;; i2-- = (i2-i8) - (i4-i6) * sine			; 178-181	n 185

	zfmaddpd zmm1, zmm3, zmm24, zmm21	;; i2++ = (i2+i8) + (i4+i6) * sine			; 179-182	n 184
	zfnmaddpd zmm3, zmm3, zmm24, zmm21	;; i2+- = (i2+i8) - (i4+i6) * sine			; 179-182	n 183

	vaddpd	zmm21, zmm18, zmm0		;; R1 = (r1++) + (r2++)					; 180-183		r180
	vsubpd	zmm18, zmm18, zmm0		;; R5 = (r1++) - (r2++)					; 180-183

	vsubpd	zmm0, zmm6, zmm17		;; I3 = (i1+-) - (r2--)					; 181-184		r181
	vaddpd	zmm6, zmm6, zmm17		;; I7 = (i1+-) + (r2--)					; 181-184

	vaddpd	zmm17, zmm4, zmm20		;; r2+-+ = (r2+-) + (i2-+)				; 182-185	n 186	r182
	vsubpd	zmm4, zmm4, zmm20		;; r2+-- = (r2+-) - (i2-+)				; 182-185	n 187

	vaddpd	zmm20, zmm15, zmm3		;; r2-++ = (r2-+) + (i2+-)				; 183-186	n 189	r183
	vsubpd	zmm15, zmm15, zmm3		;; r2-+- = (r2-+) - (i2+-)				; 183-186	n 188

	vaddpd	zmm3, zmm8, zmm1		;; I1 = (i1++) + (i2++)					; 184-187		r183
	vsubpd	zmm8, zmm8, zmm1		;; I5 = (i1++) - (i2++)					; 184-187
	zstore	[srcreg+r8], zmm21		;; Save R1						; 184

	vaddpd	zmm1, zmm2, zmm10		;; R3 = (r1+-) + (i2--)					; 185-188		r182
	vsubpd	zmm2, zmm2, zmm10		;; R7 = (r1+-) - (i2--)					; 185-188
	zstore	[srcreg+r8+d4], zmm18		;; Save R5						; 184+1

	zfmaddpd zmm10, zmm17, zmm31, zmm13	;; R2 = (r1-+) + .707*(r2+-+)				; 186-189		r186
	zfnmaddpd zmm17, zmm17, zmm31, zmm13	;; R6 = (r1-+) - .707*(r2+-+)				; 186-189
	zstore	[srcreg+r8+d2+64], zmm0		;; Save I3						; 185+1

	zfnmaddpd zmm13, zmm4, zmm31, zmm14	;; R4 = (r1--) - .707*(r2+--)				; 187-190		r186
	zfmaddpd zmm4, zmm4, zmm31, zmm14	;; R8 = (r1--) + .707*(r2+--)				; 187-190
	zstore	[srcreg+r8+d4+d2+64], zmm6	;; Save I7						; 185+2

	zfnmaddpd zmm14, zmm15, zmm31, zmm9	;; I2 = (i1--) - .707*(r2-+-)				; 188-191		r187
	zfmaddpd zmm15, zmm15, zmm31, zmm9	;; I6 = (i1--) + .707*(r2-+-)				; 188-191
	zstore	[srcreg+r8+64], zmm3		;; Save I1						; 188

	zfnmaddpd zmm9, zmm20, zmm31, zmm11	;; I4 = (i1-+) - .707*(r2-++)				; 189-192		r187
	zfmaddpd zmm20, zmm20, zmm31, zmm11	;; I8 = (i1-+) + .707*(r2-++)				; 189-192
	zstore	[srcreg+r8+d4+64], zmm8		;; Save I5						; 188+1

	zstore	[srcreg+r8+d2], zmm1		;; Save R3						; 189+1
	zstore	[srcreg+r8+d4+d2], zmm2		;; Save R7						; 189+2
	zstore	[srcreg+r8+d1], zmm10		;; Save R2						; 190+2
	zstore	[srcreg+r8+d4+d1], zmm17	;; Save R6						; 190+3
	zstore	[srcreg+r8+d2+d1], zmm13	;; Save R4						; 191+3
	zstore	[srcreg+r8+d4+d2+d1], zmm4	;; Save R8						; 191+4
	zstore	[srcreg+r8+d1+64], zmm14	;; Save I2						; 192+5
	zstore	[srcreg+r8+d4+d1+64], zmm15	;; Save I6						; 192+5
	zstore	[srcreg+r8+d2+d1+64], zmm9	;; Save I4						; 193+5
	zstore	[srcreg+r8+d4+d2+d1+64], zmm20	;; Save I8						; 193+6
	bump	srcreg, srcinc
	ENDM

;; Same as above code between orig and back_to_orig, except we implement mul4_opcode options

zr64_64c_mult_opcode MACRO srcreg,d1,d2,d4
	LOCAL	fma, fmasave, addmul, addmulsave, submul, submulsave, muladd, muladdhard, mulsub, mulsubhard, done

	movzx	r10, mul4_opcode		;; Load the mul4_opcode
	mov	r9, SRC2ARG			;; Distance to s2 arg from gwaddmul4 or gwsubmul4 or s3 arg in gwmuladd4 or gwmulsub4
	cmp	r10, 2				;; Test the mul4_opcode (part 1)
	jb	addmul				;; opcode == 1, addmul
	je	submul				;; opcode == 2, submul
	cmp	r10, 82h			;; Test the mul4_opcode (part 2)
	ja	fmasave				;; opcode == 0x83,0x84, muladd,mulsub with FFT save
	je	submulsave			;; opcode == 0x82, submul with FFT save
	cmp	r10, 80h			;; Test the mul4_opcode (part 3)
	ja	addmulsave			;; opcode == 0x81, addmul with FFT save
	jb	fma				;; opcode == 3,4, muladd,mulsub
						;; opcode == 0x80, plain mul with FFT save, fall through

	vmovapd	zmm30, [srcreg+rbp]		;; MemR1
	vmulpd	zmm20, zmm13, zmm30		;; A1 = R1 * MemR1					; 88-91		n 92
	vmulpd	zmm30, zmm5, zmm30		;; B1 = I1 * MemR1					; 88-91		n 92
	zstore	[srcreg], zmm13			;; R1
	zstore	[srcreg+64], zmm5		;; I1

	vmovapd	zmm21, [srcreg+rbp+d1]		;; MemR2
	vmulpd	zmm6, zmm19, zmm21		;; A2 = R2 * MemR2					; 89-92		n 93
	vmulpd	zmm21, zmm8, zmm21		;; B2 = I2 * MemR2					; 89-92		n 93
	zstore	[srcreg+d1], zmm19		;; R2
	zstore	[srcreg+d1+64], zmm8		;; I2

	vmovapd	zmm1, [srcreg+rbp+d2]		;; MemR3
	vmulpd	zmm17, zmm14, zmm1		;; A3 = R3 * MemR3					; 90-93		n 94
	vmulpd	zmm1, zmm0, zmm1		;; B3 = I3 * MemR3					; 90-93		n 94
	zstore	[srcreg+d2], zmm14		;; R3
	zstore	[srcreg+d2+64], zmm0		;; I3

	zfmaddpd zmm16, zmm12, zmm31, zmm18	;; R5 + R6 * SQRTHALF (final R5)			; 91-94		n 96
	zfnmaddpd zmm12, zmm12, zmm31, zmm18	;; R5 - R6 * SQRTHALF (final R6)			; 91-94		n 97

	vmovapd zmm18, [srcreg+rbp+64]		;; MemI1
	zfnmaddpd zmm20, zmm5, zmm18, zmm20	;; A1 - I1 * MemI1 (R1)					; 92-95		n 110
	zfmaddpd zmm18, zmm13, zmm18, zmm30	;; B1 + R1 * MemI1 (I1)					; 92-95		n 104

	vmovapd zmm30, [srcreg+rbp+d1+64]	;; MemI2
	zfnmaddpd zmm6, zmm8, zmm30, zmm6	;; A2 - I2 * MemI2 (R2)					; 93-96		n 110
	zfmaddpd zmm21, zmm19, zmm30, zmm21	;; B2 + R2 * MemI2 (I2)					; 93-96		n 104

	vmovapd zmm30, [srcreg+rbp+d2+64]	;; MemI3
	zfnmaddpd zmm17, zmm0, zmm30, zmm17	;; A3 - I3 * MemI3 (R3)					; 94-97		n 113
	zfmaddpd zmm1, zmm14, zmm30, zmm1	;; B3 + R3 * MemI3 (I3)					; 94-97		n 108

	vmovapd	zmm30, [srcreg+rbp+d2+d1]	;; MemR4
	vmulpd	zmm5, zmm11, zmm30		;; A4 = R4 * MemR4					; 95-98		n 99
	vmulpd	zmm13, zmm7, zmm30		;; B4 = I4 * MemR4					; 95-98		n 99
	zstore	[srcreg+d2+d1], zmm11		;; R4
	zstore	[srcreg+d2+d1+64], zmm7		;; I4

	vmovapd	zmm30, [srcreg+rbp+d4]		;; MemR5
	vmulpd	zmm8, zmm16, zmm30		;; A5 = R5 * MemR5					; 96-99		n 100
	vmulpd	zmm19, zmm9, zmm30		;; B5 = I5 * MemR5					; 96-99		n 100
	zstore	[srcreg+d4], zmm16		;; R5
	zstore	[srcreg+d4+64], zmm9		;; I5

	vmovapd	zmm30, [srcreg+rbp+d4+d1]	;; MemR6
	vmulpd	zmm0, zmm12, zmm30		;; A6 = R6 * MemR6					; 97-100	n 101
	vmulpd	zmm14, zmm4, zmm30		;; B6 = I6 * MemR6					; 97-100	n 101
	zstore	[srcreg+d4+d1], zmm12		;; R6
	zstore	[srcreg+d4+d1+64], zmm4		;; I6

	zfnmaddpd zmm30, zmm3, zmm31, zmm15	;; R7 - I8 * SQRTHALF (final R7)			; 98-101	n 102
	zfmaddpd zmm3, zmm3, zmm31, zmm15	;; R7 + I8 * SQRTHALF (final R8)			; 98-101	n 103

	vmovapd zmm15, [srcreg+rbp+d2+d1+64]	;; MemI4
	zfnmaddpd zmm5, zmm7, zmm15, zmm5	;; A4 - I4 * MemI4 (R4)					; 99-102	n 113
	zfmaddpd zmm13, zmm11, zmm15, zmm13	;; B4 + R4 * MemI4 (I4)					; 99-102	n 108

	vmovapd zmm15, [srcreg+rbp+d4+64]	;; MemI5
	zfnmaddpd zmm8, zmm9, zmm15, zmm8	;; A5 - I5 * MemI5 (R5)					; 100-103	n 109
	zfmaddpd zmm19, zmm16, zmm15, zmm19	;; B5 + R5 * MemI5 (I5)					; 100-103	n 105

	vmovapd zmm15, [srcreg+rbp+d4+d1+64]	;; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0	;; A6 - I6 * MemI6 (R6)					; 101-104	n 109
	zfmaddpd zmm14, zmm12, zmm15, zmm14	;; B6 + R6 * MemI6 (I6)					; 101-104	n 105

	vmovapd	zmm15, [srcreg+rbp+d4+d2]	;; MemR7
	vmulpd	zmm7, zmm30, zmm15		;; A7 = R7 * MemR7					; 102-105	n 106
	vmulpd	zmm11, zmm2, zmm15		;; B7 = I7 * MemR7					; 102-105	n 106
	zstore	[srcreg+d4+d2], zmm30		;; R7
	zstore	[srcreg+d4+d2+64], zmm2		;; I7

	vmovapd	zmm15, [srcreg+rbp+d4+d2+d1]	;; MemR8
	vmulpd	zmm9, zmm3, zmm15		;; A8 = R8 * MemR8					; 103-106	n 107
	vmulpd	zmm16, zmm10, zmm15		;; B8 = I8 * MemR8					; 103-106	n 107
	zstore	[srcreg+d4+d2+d1], zmm3		;; R8
	zstore	[srcreg+d4+d2+d1+64], zmm10	;; I8

	vaddpd	zmm4, zmm18, zmm21		;; I1 + I2 (new I1)					; 104-107	n 114
	vsubpd	zmm18, zmm18, zmm21		;; I1 - I2 (new I2)					; 104-107	n 121

	vaddpd	zmm12, zmm19, zmm14		;; I5 + I6 (new I5)					; 105-108	n 116
	vsubpd	zmm14, zmm19, zmm14		;; I5 - I6 (new I6)					; 105-108	n 117

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]	;; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7	;; A7 - I7 * MemI7 (R7)					; 106-109	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11	;; B7 + R7 * MemI7 (I7)					; 106-109	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]	;; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9	;; A8 - I8 * MemI8 (R8)					; 107-110	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16	;; B8 + R8 * MemI8 (I8)					; 107-110	n 112

	jmp	done

addmul:
	vmovapd	zmm30, [srcreg+rbp]		;; MemR1
	vaddpd	zmm30, zmm30, [srcreg+r9]	;; MemR1
	vmulpd	zmm20, zmm13, zmm30		;; A1 = R1 * MemR1					; 88-91		n 92
	vmulpd	zmm30, zmm5, zmm30		;; B1 = I1 * MemR1					; 88-91		n 92

	vmovapd	zmm21, [srcreg+rbp+d1]		;; MemR2
	vaddpd	zmm21, zmm21, [srcreg+r9+d1]	;; MemR2
	vmulpd	zmm6, zmm19, zmm21		;; A2 = R2 * MemR2					; 89-92		n 93
	vmulpd	zmm21, zmm8, zmm21		;; B2 = I2 * MemR2					; 89-92		n 93

	vmovapd	zmm1, [srcreg+rbp+d2]		;; MemR3
	vaddpd	zmm1, zmm1, [srcreg+r9+d2]	;; MemR3
	vmulpd	zmm17, zmm14, zmm1		;; A3 = R3 * MemR3					; 90-93		n 94
	vmulpd	zmm1, zmm0, zmm1		;; B3 = I3 * MemR3					; 90-93		n 94

	zfmaddpd zmm16, zmm12, zmm31, zmm18	;; R5 + R6 * SQRTHALF (final R5)			; 91-94		n 96
	zfnmaddpd zmm12, zmm12, zmm31, zmm18	;; R5 - R6 * SQRTHALF (final R6)			; 91-94		n 97

	vmovapd zmm18, [srcreg+rbp+64]		;; MemI1
	vaddpd zmm18, zmm18, [srcreg+r9+64]	;; MemI1
	zfnmaddpd zmm20, zmm5, zmm18, zmm20	;; A1 - I1 * MemI1 (R1)					; 92-95		n 110
	zfmaddpd zmm18, zmm13, zmm18, zmm30	;; B1 + R1 * MemI1 (I1)					; 92-95		n 104

	vmovapd zmm30, [srcreg+rbp+d1+64]	;; MemI2
	vaddpd zmm30, zmm30, [srcreg+r9+d1+64]	;; MemI2
	zfnmaddpd zmm6, zmm8, zmm30, zmm6	;; A2 - I2 * MemI2 (R2)					; 93-96		n 110
	zfmaddpd zmm21, zmm19, zmm30, zmm21	;; B2 + R2 * MemI2 (I2)					; 93-96		n 104

	vmovapd zmm30, [srcreg+rbp+d2+64]	;; MemI3
	vaddpd zmm30, zmm30, [srcreg+r9+d2+64]	;; MemI3
	zfnmaddpd zmm17, zmm0, zmm30, zmm17	;; A3 - I3 * MemI3 (R3)					; 94-97		n 113
	zfmaddpd zmm1, zmm14, zmm30, zmm1	;; B3 + R3 * MemI3 (I3)					; 94-97		n 108

	vmovapd	zmm30, [srcreg+rbp+d2+d1]	;; MemR4
	vaddpd	zmm30, zmm30, [srcreg+r9+d2+d1]	;; MemR4
	vmulpd	zmm5, zmm11, zmm30		;; A4 = R4 * MemR4					; 95-98		n 99
	vmulpd	zmm13, zmm7, zmm30		;; B4 = I4 * MemR4					; 95-98		n 99

	vmovapd	zmm30, [srcreg+rbp+d4]		;; MemR5
	vaddpd	zmm30, zmm30, [srcreg+r9+d4]	;; MemR5
	vmulpd	zmm8, zmm16, zmm30		;; A5 = R5 * MemR5					; 96-99		n 100
	vmulpd	zmm19, zmm9, zmm30		;; B5 = I5 * MemR5					; 96-99		n 100

	vmovapd	zmm30, [srcreg+rbp+d4+d1]	;; MemR6
	vaddpd	zmm30, zmm30, [srcreg+r9+d4+d1]	;; MemR6
	vmulpd	zmm0, zmm12, zmm30		;; A6 = R6 * MemR6					; 97-100	n 101
	vmulpd	zmm14, zmm4, zmm30		;; B6 = I6 * MemR6					; 97-100	n 101

	zfnmaddpd zmm30, zmm3, zmm31, zmm15	;; R7 - I8 * SQRTHALF (final R7)			; 98-101	n 102
	zfmaddpd zmm3, zmm3, zmm31, zmm15	;; R7 + I8 * SQRTHALF (final R8)			; 98-101	n 103

	vmovapd zmm15, [srcreg+rbp+d2+d1+64]	;; MemI4
	vaddpd zmm15, zmm15, [srcreg+r9+d2+d1+64];; MemI4
	zfnmaddpd zmm5, zmm7, zmm15, zmm5	;; A4 - I4 * MemI4 (R4)					; 99-102	n 113
	zfmaddpd zmm13, zmm11, zmm15, zmm13	;; B4 + R4 * MemI4 (I4)					; 99-102	n 108

	vmovapd zmm15, [srcreg+rbp+d4+64]	;; MemI5
	vaddpd zmm15, zmm15, [srcreg+r9+d4+64]	;; MemI5
	zfnmaddpd zmm8, zmm9, zmm15, zmm8	;; A5 - I5 * MemI5 (R5)					; 100-103	n 109
	zfmaddpd zmm19, zmm16, zmm15, zmm19	;; B5 + R5 * MemI5 (I5)					; 100-103	n 105

	vmovapd zmm15, [srcreg+rbp+d4+d1+64]	;; MemI6
	vaddpd zmm15, zmm15, [srcreg+r9+d4+d1+64];; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0	;; A6 - I6 * MemI6 (R6)					; 101-104	n 109
	zfmaddpd zmm14, zmm12, zmm15, zmm14	;; B6 + R6 * MemI6 (I6)					; 101-104	n 105

	vmovapd	zmm15, [srcreg+rbp+d4+d2]	;; MemR7
	vaddpd	zmm15, zmm15, [srcreg+r9+d4+d2]	;; MemR7
	vmulpd	zmm7, zmm30, zmm15		;; A7 = R7 * MemR7					; 102-105	n 106
	vmulpd	zmm11, zmm2, zmm15		;; B7 = I7 * MemR7					; 102-105	n 106

	vmovapd	zmm15, [srcreg+rbp+d4+d2+d1]	;; MemR8
	vaddpd	zmm15, zmm15, [srcreg+r9+d4+d2+d1];; MemR8
	vmulpd	zmm9, zmm3, zmm15		;; A8 = R8 * MemR8					; 103-106	n 107
	vmulpd	zmm16, zmm10, zmm15		;; B8 = I8 * MemR8					; 103-106	n 107

	vaddpd	zmm4, zmm18, zmm21		;; I1 + I2 (new I1)					; 104-107	n 114
	vsubpd	zmm18, zmm18, zmm21		;; I1 - I2 (new I2)					; 104-107	n 121

	vaddpd	zmm12, zmm19, zmm14		;; I5 + I6 (new I5)					; 105-108	n 116
	vsubpd	zmm14, zmm19, zmm14		;; I5 - I6 (new I6)					; 105-108	n 117

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]	;; MemI7
	vaddpd zmm15, zmm15, [srcreg+r9+d4+d2+64];; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7	;; A7 - I7 * MemI7 (R7)					; 106-109	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11	;; B7 + R7 * MemI7 (I7)					; 106-109	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]	;; MemI8
	vaddpd zmm15, zmm15, [srcreg+r9+d4+d2+d1+64];; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9	;; A8 - I8 * MemI8 (R8)					; 107-110	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16	;; B8 + R8 * MemI8 (I8)					; 107-110	n 112

	jmp	done

addmulsave:
	vmovapd	zmm30, [srcreg+rbp]		;; MemR1
	vaddpd	zmm30, zmm30, [srcreg+r9]	;; MemR1
	vmulpd	zmm20, zmm13, zmm30		;; A1 = R1 * MemR1					; 88-91		n 92
	vmulpd	zmm30, zmm5, zmm30		;; B1 = I1 * MemR1					; 88-91		n 92
	zstore	[srcreg], zmm13			;; R1
	zstore	[srcreg+64], zmm5		;; I1

	vmovapd	zmm21, [srcreg+rbp+d1]		;; MemR2
	vaddpd	zmm21, zmm21, [srcreg+r9+d1]	;; MemR2
	vmulpd	zmm6, zmm19, zmm21		;; A2 = R2 * MemR2					; 89-92		n 93
	vmulpd	zmm21, zmm8, zmm21		;; B2 = I2 * MemR2					; 89-92		n 93
	zstore	[srcreg+d1], zmm19		;; R2
	zstore	[srcreg+d1+64], zmm8		;; I2

	vmovapd	zmm1, [srcreg+rbp+d2]		;; MemR3
	vaddpd	zmm1, zmm1, [srcreg+r9+d2]	;; MemR3
	vmulpd	zmm17, zmm14, zmm1		;; A3 = R3 * MemR3					; 90-93		n 94
	vmulpd	zmm1, zmm0, zmm1		;; B3 = I3 * MemR3					; 90-93		n 94
	zstore	[srcreg+d2], zmm14		;; R3
	zstore	[srcreg+d2+64], zmm0		;; I3

	zfmaddpd zmm16, zmm12, zmm31, zmm18	;; R5 + R6 * SQRTHALF (final R5)			; 91-94		n 96
	zfnmaddpd zmm12, zmm12, zmm31, zmm18	;; R5 - R6 * SQRTHALF (final R6)			; 91-94		n 97

	vmovapd zmm18, [srcreg+rbp+64]		;; MemI1
	vaddpd zmm18, zmm18, [srcreg+r9+64]	;; MemI1
	zfnmaddpd zmm20, zmm5, zmm18, zmm20	;; A1 - I1 * MemI1 (R1)					; 92-95		n 110
	zfmaddpd zmm18, zmm13, zmm18, zmm30	;; B1 + R1 * MemI1 (I1)					; 92-95		n 104

	vmovapd zmm30, [srcreg+rbp+d1+64]	;; MemI2
	vaddpd zmm30, zmm30, [srcreg+r9+d1+64]	;; MemI2
	zfnmaddpd zmm6, zmm8, zmm30, zmm6	;; A2 - I2 * MemI2 (R2)					; 93-96		n 110
	zfmaddpd zmm21, zmm19, zmm30, zmm21	;; B2 + R2 * MemI2 (I2)					; 93-96		n 104

	vmovapd zmm30, [srcreg+rbp+d2+64]	;; MemI3
	vaddpd zmm30, zmm30, [srcreg+r9+d2+64]	;; MemI3
	zfnmaddpd zmm17, zmm0, zmm30, zmm17	;; A3 - I3 * MemI3 (R3)					; 94-97		n 113
	zfmaddpd zmm1, zmm14, zmm30, zmm1	;; B3 + R3 * MemI3 (I3)					; 94-97		n 108

	vmovapd	zmm30, [srcreg+rbp+d2+d1]	;; MemR4
	vaddpd	zmm30, zmm30, [srcreg+r9+d2+d1]	;; MemR4
	vmulpd	zmm5, zmm11, zmm30		;; A4 = R4 * MemR4					; 95-98		n 99
	vmulpd	zmm13, zmm7, zmm30		;; B4 = I4 * MemR4					; 95-98		n 99
	zstore	[srcreg+d2+d1], zmm11		;; R4
	zstore	[srcreg+d2+d1+64], zmm7		;; I4

	vmovapd	zmm30, [srcreg+rbp+d4]		;; MemR5
	vaddpd	zmm30, zmm30, [srcreg+r9+d4]	;; MemR5
	vmulpd	zmm8, zmm16, zmm30		;; A5 = R5 * MemR5					; 96-99		n 100
	vmulpd	zmm19, zmm9, zmm30		;; B5 = I5 * MemR5					; 96-99		n 100
	zstore	[srcreg+d4], zmm16		;; R5
	zstore	[srcreg+d4+64], zmm9		;; I5

	vmovapd	zmm30, [srcreg+rbp+d4+d1]	;; MemR6
	vaddpd	zmm30, zmm30, [srcreg+r9+d4+d1]	;; MemR6
	vmulpd	zmm0, zmm12, zmm30		;; A6 = R6 * MemR6					; 97-100	n 101
	vmulpd	zmm14, zmm4, zmm30		;; B6 = I6 * MemR6					; 97-100	n 101
	zstore	[srcreg+d4+d1], zmm12		;; R6
	zstore	[srcreg+d4+d1+64], zmm4		;; I6

	zfnmaddpd zmm30, zmm3, zmm31, zmm15	;; R7 - I8 * SQRTHALF (final R7)			; 98-101	n 102
	zfmaddpd zmm3, zmm3, zmm31, zmm15	;; R7 + I8 * SQRTHALF (final R8)			; 98-101	n 103

	vmovapd zmm15, [srcreg+rbp+d2+d1+64]	;; MemI4
	vaddpd zmm15, zmm15, [srcreg+r9+d2+d1+64];; MemI4
	zfnmaddpd zmm5, zmm7, zmm15, zmm5	;; A4 - I4 * MemI4 (R4)					; 99-102	n 113
	zfmaddpd zmm13, zmm11, zmm15, zmm13	;; B4 + R4 * MemI4 (I4)					; 99-102	n 108

	vmovapd zmm15, [srcreg+rbp+d4+64]	;; MemI5
	vaddpd zmm15, zmm15, [srcreg+r9+d4+64]	;; MemI5
	zfnmaddpd zmm8, zmm9, zmm15, zmm8	;; A5 - I5 * MemI5 (R5)					; 100-103	n 109
	zfmaddpd zmm19, zmm16, zmm15, zmm19	;; B5 + R5 * MemI5 (I5)					; 100-103	n 105

	vmovapd zmm15, [srcreg+rbp+d4+d1+64]	;; MemI6
	vaddpd zmm15, zmm15, [srcreg+r9+d4+d1+64];; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0	;; A6 - I6 * MemI6 (R6)					; 101-104	n 109
	zfmaddpd zmm14, zmm12, zmm15, zmm14	;; B6 + R6 * MemI6 (I6)					; 101-104	n 105

	vmovapd	zmm15, [srcreg+rbp+d4+d2]	;; MemR7
	vaddpd	zmm15, zmm15, [srcreg+r9+d4+d2]	;; MemR7
	vmulpd	zmm7, zmm30, zmm15		;; A7 = R7 * MemR7					; 102-105	n 106
	vmulpd	zmm11, zmm2, zmm15		;; B7 = I7 * MemR7					; 102-105	n 106
	zstore	[srcreg+d4+d2], zmm30		;; R7
	zstore	[srcreg+d4+d2+64], zmm2		;; I7

	vmovapd	zmm15, [srcreg+rbp+d4+d2+d1]	;; MemR8
	vaddpd	zmm15, zmm15, [srcreg+r9+d4+d2+d1];; MemR8
	vmulpd	zmm9, zmm3, zmm15		;; A8 = R8 * MemR8					; 103-106	n 107
	vmulpd	zmm16, zmm10, zmm15		;; B8 = I8 * MemR8					; 103-106	n 107
	zstore	[srcreg+d4+d2+d1], zmm3		;; R8
	zstore	[srcreg+d4+d2+d1+64], zmm10	;; I8

	vaddpd	zmm4, zmm18, zmm21		;; I1 + I2 (new I1)					; 104-107	n 114
	vsubpd	zmm18, zmm18, zmm21		;; I1 - I2 (new I2)					; 104-107	n 121

	vaddpd	zmm12, zmm19, zmm14		;; I5 + I6 (new I5)					; 105-108	n 116
	vsubpd	zmm14, zmm19, zmm14		;; I5 - I6 (new I6)					; 105-108	n 117

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]	;; MemI7
	vaddpd zmm15, zmm15, [srcreg+r9+d4+d2+64];; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7	;; A7 - I7 * MemI7 (R7)					; 106-109	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11	;; B7 + R7 * MemI7 (I7)					; 106-109	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]	;; MemI8
	vaddpd zmm15, zmm15, [srcreg+r9+d4+d2+d1+64];; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9	;; A8 - I8 * MemI8 (R8)					; 107-110	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16	;; B8 + R8 * MemI8 (I8)					; 107-110	n 112

	jmp	done

submul:
	vmovapd	zmm30, [srcreg+rbp]		;; MemR1
	vsubpd	zmm30, zmm30, [srcreg+r9]	;; MemR1
	vmulpd	zmm20, zmm13, zmm30		;; A1 = R1 * MemR1					; 88-91		n 92
	vmulpd	zmm30, zmm5, zmm30		;; B1 = I1 * MemR1					; 88-91		n 92

	vmovapd	zmm21, [srcreg+rbp+d1]		;; MemR2
	vsubpd	zmm21, zmm21, [srcreg+r9+d1]	;; MemR2
	vmulpd	zmm6, zmm19, zmm21		;; A2 = R2 * MemR2					; 89-92		n 93
	vmulpd	zmm21, zmm8, zmm21		;; B2 = I2 * MemR2					; 89-92		n 93

	vmovapd	zmm1, [srcreg+rbp+d2]		;; MemR3
	vsubpd	zmm1, zmm1, [srcreg+r9+d2]	;; MemR3
	vmulpd	zmm17, zmm14, zmm1		;; A3 = R3 * MemR3					; 90-93		n 94
	vmulpd	zmm1, zmm0, zmm1		;; B3 = I3 * MemR3					; 90-93		n 94

	zfmaddpd zmm16, zmm12, zmm31, zmm18	;; R5 + R6 * SQRTHALF (final R5)			; 91-94		n 96
	zfnmaddpd zmm12, zmm12, zmm31, zmm18	;; R5 - R6 * SQRTHALF (final R6)			; 91-94		n 97

	vmovapd zmm18, [srcreg+rbp+64]		;; MemI1
	vsubpd zmm18, zmm18, [srcreg+r9+64]	;; MemI1
	zfnmaddpd zmm20, zmm5, zmm18, zmm20	;; A1 - I1 * MemI1 (R1)					; 92-95		n 110
	zfmaddpd zmm18, zmm13, zmm18, zmm30	;; B1 + R1 * MemI1 (I1)					; 92-95		n 104

	vmovapd zmm30, [srcreg+rbp+d1+64]	;; MemI2
	vsubpd zmm30, zmm30, [srcreg+r9+d1+64]	;; MemI2
	zfnmaddpd zmm6, zmm8, zmm30, zmm6	;; A2 - I2 * MemI2 (R2)					; 93-96		n 110
	zfmaddpd zmm21, zmm19, zmm30, zmm21	;; B2 + R2 * MemI2 (I2)					; 93-96		n 104

	vmovapd zmm30, [srcreg+rbp+d2+64]	;; MemI3
	vsubpd zmm30, zmm30, [srcreg+r9+d2+64]	;; MemI3
	zfnmaddpd zmm17, zmm0, zmm30, zmm17	;; A3 - I3 * MemI3 (R3)					; 94-97		n 113
	zfmaddpd zmm1, zmm14, zmm30, zmm1	;; B3 + R3 * MemI3 (I3)					; 94-97		n 108

	vmovapd	zmm30, [srcreg+rbp+d2+d1]	;; MemR4
	vsubpd	zmm30, zmm30, [srcreg+r9+d2+d1]	;; MemR4
	vmulpd	zmm5, zmm11, zmm30		;; A4 = R4 * MemR4					; 95-98		n 99
	vmulpd	zmm13, zmm7, zmm30		;; B4 = I4 * MemR4					; 95-98		n 99

	vmovapd	zmm30, [srcreg+rbp+d4]		;; MemR5
	vsubpd	zmm30, zmm30, [srcreg+r9+d4]	;; MemR5
	vmulpd	zmm8, zmm16, zmm30		;; A5 = R5 * MemR5					; 96-99		n 100
	vmulpd	zmm19, zmm9, zmm30		;; B5 = I5 * MemR5					; 96-99		n 100

	vmovapd	zmm30, [srcreg+rbp+d4+d1]	;; MemR6
	vsubpd	zmm30, zmm30, [srcreg+r9+d4+d1]	;; MemR6
	vmulpd	zmm0, zmm12, zmm30		;; A6 = R6 * MemR6					; 97-100	n 101
	vmulpd	zmm14, zmm4, zmm30		;; B6 = I6 * MemR6					; 97-100	n 101

	zfnmaddpd zmm30, zmm3, zmm31, zmm15	;; R7 - I8 * SQRTHALF (final R7)			; 98-101	n 102
	zfmaddpd zmm3, zmm3, zmm31, zmm15	;; R7 + I8 * SQRTHALF (final R8)			; 98-101	n 103

	vmovapd zmm15, [srcreg+rbp+d2+d1+64]	;; MemI4
	vsubpd zmm15, zmm15, [srcreg+r9+d2+d1+64];; MemI4
	zfnmaddpd zmm5, zmm7, zmm15, zmm5	;; A4 - I4 * MemI4 (R4)					; 99-102	n 113
	zfmaddpd zmm13, zmm11, zmm15, zmm13	;; B4 + R4 * MemI4 (I4)					; 99-102	n 108

	vmovapd zmm15, [srcreg+rbp+d4+64]	;; MemI5
	vsubpd zmm15, zmm15, [srcreg+r9+d4+64]	;; MemI5
	zfnmaddpd zmm8, zmm9, zmm15, zmm8	;; A5 - I5 * MemI5 (R5)					; 100-103	n 109
	zfmaddpd zmm19, zmm16, zmm15, zmm19	;; B5 + R5 * MemI5 (I5)					; 100-103	n 105

	vmovapd zmm15, [srcreg+rbp+d4+d1+64]	;; MemI6
	vsubpd zmm15, zmm15, [srcreg+r9+d4+d1+64];; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0	;; A6 - I6 * MemI6 (R6)					; 101-104	n 109
	zfmaddpd zmm14, zmm12, zmm15, zmm14	;; B6 + R6 * MemI6 (I6)					; 101-104	n 105

	vmovapd	zmm15, [srcreg+rbp+d4+d2]	;; MemR7
	vsubpd	zmm15, zmm15, [srcreg+r9+d4+d2];; MemR7
	vmulpd	zmm7, zmm30, zmm15		;; A7 = R7 * MemR7					; 102-105	n 106
	vmulpd	zmm11, zmm2, zmm15		;; B7 = I7 * MemR7					; 102-105	n 106

	vmovapd	zmm15, [srcreg+rbp+d4+d2+d1]	;; MemR8
	vsubpd	zmm15, zmm15, [srcreg+r9+d4+d2+d1];; MemR8
	vmulpd	zmm9, zmm3, zmm15		;; A8 = R8 * MemR8					; 103-106	n 107
	vmulpd	zmm16, zmm10, zmm15		;; B8 = I8 * MemR8					; 103-106	n 107

	vaddpd	zmm4, zmm18, zmm21		;; I1 + I2 (new I1)					; 104-107	n 114
	vsubpd	zmm18, zmm18, zmm21		;; I1 - I2 (new I2)					; 104-107	n 121

	vaddpd	zmm12, zmm19, zmm14		;; I5 + I6 (new I5)					; 105-108	n 116
	vsubpd	zmm14, zmm19, zmm14		;; I5 - I6 (new I6)					; 105-108	n 117

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]	;; MemI7
	vsubpd zmm15, zmm15, [srcreg+r9+d4+d2+64];; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7	;; A7 - I7 * MemI7 (R7)					; 106-109	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11	;; B7 + R7 * MemI7 (I7)					; 106-109	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]	;; MemI8
	vsubpd zmm15, zmm15, [srcreg+r9+d4+d2+d1+64];; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9	;; A8 - I8 * MemI8 (R8)					; 107-110	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16	;; B8 + R8 * MemI8 (I8)					; 107-110	n 112

	jmp	done

submulsave:
	vmovapd	zmm30, [srcreg+rbp]		;; MemR1
	vsubpd	zmm30, zmm30, [srcreg+r9]	;; MemR1
	vmulpd	zmm20, zmm13, zmm30		;; A1 = R1 * MemR1					; 88-91		n 92
	vmulpd	zmm30, zmm5, zmm30		;; B1 = I1 * MemR1					; 88-91		n 92
	zstore	[srcreg], zmm13			;; R1
	zstore	[srcreg+64], zmm5		;; I1

	vmovapd	zmm21, [srcreg+rbp+d1]		;; MemR2
	vsubpd	zmm21, zmm21, [srcreg+r9+d1]	;; MemR2
	vmulpd	zmm6, zmm19, zmm21		;; A2 = R2 * MemR2					; 89-92		n 93
	vmulpd	zmm21, zmm8, zmm21		;; B2 = I2 * MemR2					; 89-92		n 93
	zstore	[srcreg+d1], zmm19		;; R2
	zstore	[srcreg+d1+64], zmm8		;; I2

	vmovapd	zmm1, [srcreg+rbp+d2]		;; MemR3
	vsubpd	zmm1, zmm1, [srcreg+r9+d2]	;; MemR3
	vmulpd	zmm17, zmm14, zmm1		;; A3 = R3 * MemR3					; 90-93		n 94
	vmulpd	zmm1, zmm0, zmm1		;; B3 = I3 * MemR3					; 90-93		n 94
	zstore	[srcreg+d2], zmm14		;; R3
	zstore	[srcreg+d2+64], zmm0		;; I3

	zfmaddpd zmm16, zmm12, zmm31, zmm18	;; R5 + R6 * SQRTHALF (final R5)			; 91-94		n 96
	zfnmaddpd zmm12, zmm12, zmm31, zmm18	;; R5 - R6 * SQRTHALF (final R6)			; 91-94		n 97

	vmovapd zmm18, [srcreg+rbp+64]		;; MemI1
	vsubpd zmm18, zmm18, [srcreg+r9+64]	;; MemI1
	zfnmaddpd zmm20, zmm5, zmm18, zmm20	;; A1 - I1 * MemI1 (R1)					; 92-95		n 110
	zfmaddpd zmm18, zmm13, zmm18, zmm30	;; B1 + R1 * MemI1 (I1)					; 92-95		n 104

	vmovapd zmm30, [srcreg+rbp+d1+64]	;; MemI2
	vsubpd zmm30, zmm30, [srcreg+r9+d1+64]	;; MemI2
	zfnmaddpd zmm6, zmm8, zmm30, zmm6	;; A2 - I2 * MemI2 (R2)					; 93-96		n 110
	zfmaddpd zmm21, zmm19, zmm30, zmm21	;; B2 + R2 * MemI2 (I2)					; 93-96		n 104

	vmovapd zmm30, [srcreg+rbp+d2+64]	;; MemI3
	vsubpd zmm30, zmm30, [srcreg+r9+d2+64]	;; MemI3
	zfnmaddpd zmm17, zmm0, zmm30, zmm17	;; A3 - I3 * MemI3 (R3)					; 94-97		n 113
	zfmaddpd zmm1, zmm14, zmm30, zmm1	;; B3 + R3 * MemI3 (I3)					; 94-97		n 108

	vmovapd	zmm30, [srcreg+rbp+d2+d1]	;; MemR4
	vsubpd	zmm30, zmm30, [srcreg+r9+d2+d1]	;; MemR4
	vmulpd	zmm5, zmm11, zmm30		;; A4 = R4 * MemR4					; 95-98		n 99
	vmulpd	zmm13, zmm7, zmm30		;; B4 = I4 * MemR4					; 95-98		n 99
	zstore	[srcreg+d2+d1], zmm11		;; R4
	zstore	[srcreg+d2+d1+64], zmm7		;; I4

	vmovapd	zmm30, [srcreg+rbp+d4]		;; MemR5
	vsubpd	zmm30, zmm30, [srcreg+r9+d4]	;; MemR5
	vmulpd	zmm8, zmm16, zmm30		;; A5 = R5 * MemR5					; 96-99		n 100
	vmulpd	zmm19, zmm9, zmm30		;; B5 = I5 * MemR5					; 96-99		n 100
	zstore	[srcreg+d4], zmm16		;; R5
	zstore	[srcreg+d4+64], zmm9		;; I5

	vmovapd	zmm30, [srcreg+rbp+d4+d1]	;; MemR6
	vsubpd	zmm30, zmm30, [srcreg+r9+d4+d1]	;; MemR6
	vmulpd	zmm0, zmm12, zmm30		;; A6 = R6 * MemR6					; 97-100	n 101
	vmulpd	zmm14, zmm4, zmm30		;; B6 = I6 * MemR6					; 97-100	n 101
	zstore	[srcreg+d4+d1], zmm12		;; R6
	zstore	[srcreg+d4+d1+64], zmm4		;; I6

	zfnmaddpd zmm30, zmm3, zmm31, zmm15	;; R7 - I8 * SQRTHALF (final R7)			; 98-101	n 102
	zfmaddpd zmm3, zmm3, zmm31, zmm15	;; R7 + I8 * SQRTHALF (final R8)			; 98-101	n 103

	vmovapd zmm15, [srcreg+rbp+d2+d1+64]	;; MemI4
	vsubpd zmm15, zmm15, [srcreg+r9+d2+d1+64];; MemI4
	zfnmaddpd zmm5, zmm7, zmm15, zmm5	;; A4 - I4 * MemI4 (R4)					; 99-102	n 113
	zfmaddpd zmm13, zmm11, zmm15, zmm13	;; B4 + R4 * MemI4 (I4)					; 99-102	n 108

	vmovapd zmm15, [srcreg+rbp+d4+64]	;; MemI5
	vsubpd zmm15, zmm15, [srcreg+r9+d4+64]	;; MemI5
	zfnmaddpd zmm8, zmm9, zmm15, zmm8	;; A5 - I5 * MemI5 (R5)					; 100-103	n 109
	zfmaddpd zmm19, zmm16, zmm15, zmm19	;; B5 + R5 * MemI5 (I5)					; 100-103	n 105

	vmovapd zmm15, [srcreg+rbp+d4+d1+64]	;; MemI6
	vsubpd zmm15, zmm15, [srcreg+r9+d4+d1+64];; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0	;; A6 - I6 * MemI6 (R6)					; 101-104	n 109
	zfmaddpd zmm14, zmm12, zmm15, zmm14	;; B6 + R6 * MemI6 (I6)					; 101-104	n 105

	vmovapd	zmm15, [srcreg+rbp+d4+d2]	;; MemR7
	vsubpd	zmm15, zmm15, [srcreg+r9+d4+d2]	;; MemR7
	vmulpd	zmm7, zmm30, zmm15		;; A7 = R7 * MemR7					; 102-105	n 106
	vmulpd	zmm11, zmm2, zmm15		;; B7 = I7 * MemR7					; 102-105	n 106
	zstore	[srcreg+d4+d2], zmm30		;; R7
	zstore	[srcreg+d4+d2+64], zmm2		;; I7

	vmovapd	zmm15, [srcreg+rbp+d4+d2+d1]	;; MemR8
	vsubpd	zmm15, zmm15, [srcreg+r9+d4+d2+d1];; MemR8
	vmulpd	zmm9, zmm3, zmm15		;; A8 = R8 * MemR8					; 103-106	n 107
	vmulpd	zmm16, zmm10, zmm15		;; B8 = I8 * MemR8					; 103-106	n 107
	zstore	[srcreg+d4+d2+d1], zmm3		;; R8
	zstore	[srcreg+d4+d2+d1+64], zmm10	;; I8

	vaddpd	zmm4, zmm18, zmm21		;; I1 + I2 (new I1)					; 104-107	n 114
	vsubpd	zmm18, zmm18, zmm21		;; I1 - I2 (new I2)					; 104-107	n 121

	vaddpd	zmm12, zmm19, zmm14		;; I5 + I6 (new I5)					; 105-108	n 116
	vsubpd	zmm14, zmm19, zmm14		;; I5 - I6 (new I6)					; 105-108	n 117

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]	;; MemI7
	vsubpd zmm15, zmm15, [srcreg+r9+d4+d2+64];; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7	;; A7 - I7 * MemI7 (R7)					; 106-109	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11	;; B7 + R7 * MemI7 (I7)					; 106-109	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]	;; MemI8
	vsubpd zmm15, zmm15, [srcreg+r9+d4+d2+d1+64];; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9	;; A8 - I8 * MemI8 (R8)					; 107-110	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16	;; B8 + R8 * MemI8 (I8)					; 107-110	n 112

	jmp	done

fmasave:
	vmovapd	zmm30, [srcreg+rbp]		;; MemR1
	vmulpd	zmm20, zmm13, zmm30		;; A1 = R1 * MemR1					; 88-91		n 92
	vmulpd	zmm30, zmm5, zmm30		;; B1 = I1 * MemR1					; 88-91		n 92
	zstore	[srcreg], zmm13			;; R1
	zstore	[srcreg+64], zmm5		;; I1

	vmovapd	zmm21, [srcreg+rbp+d1]		;; MemR2
	vmulpd	zmm6, zmm19, zmm21		;; A2 = R2 * MemR2					; 89-92		n 93
	vmulpd	zmm21, zmm8, zmm21		;; B2 = I2 * MemR2					; 89-92		n 93
	zstore	[srcreg+d1], zmm19		;; R2
	zstore	[srcreg+d1+64], zmm8		;; I2

	vmovapd	zmm1, [srcreg+rbp+d2]		;; MemR3
	vmulpd	zmm17, zmm14, zmm1		;; A3 = R3 * MemR3					; 90-93		n 94
	vmulpd	zmm1, zmm0, zmm1		;; B3 = I3 * MemR3					; 90-93		n 94
	zstore	[srcreg+d2], zmm14		;; R3
	zstore	[srcreg+d2+64], zmm0		;; I3

	zfmaddpd zmm16, zmm12, zmm31, zmm18	;; R5 + R6 * SQRTHALF (final R5)			; 91-94		n 96
	zfnmaddpd zmm12, zmm12, zmm31, zmm18	;; R5 - R6 * SQRTHALF (final R6)			; 91-94		n 97

	vmovapd zmm18, [srcreg+rbp+64]		;; MemI1
	zfnmaddpd zmm20, zmm5, zmm18, zmm20	;; A1 - I1 * MemI1 (R1)					; 92-95		n 110
	zfmaddpd zmm18, zmm13, zmm18, zmm30	;; B1 + R1 * MemI1 (I1)					; 92-95		n 104

	vmovapd zmm30, [srcreg+rbp+d1+64]	;; MemI2
	zfnmaddpd zmm6, zmm8, zmm30, zmm6	;; A2 - I2 * MemI2 (R2)					; 93-96		n 110
	zfmaddpd zmm21, zmm19, zmm30, zmm21	;; B2 + R2 * MemI2 (I2)					; 93-96		n 104

	vmovapd zmm30, [srcreg+rbp+d2+64]	;; MemI3
	zfnmaddpd zmm17, zmm0, zmm30, zmm17	;; A3 - I3 * MemI3 (R3)					; 94-97		n 113
	zfmaddpd zmm1, zmm14, zmm30, zmm1	;; B3 + R3 * MemI3 (I3)					; 94-97		n 108

	vmovapd	zmm30, [srcreg+rbp+d2+d1]	;; MemR4
	vmulpd	zmm5, zmm11, zmm30		;; A4 = R4 * MemR4					; 95-98		n 99
	vmulpd	zmm13, zmm7, zmm30		;; B4 = I4 * MemR4					; 95-98		n 99
	zstore	[srcreg+d2+d1], zmm11		;; R4
	zstore	[srcreg+d2+d1+64], zmm7		;; I4

	vmovapd	zmm30, [srcreg+rbp+d4]		;; MemR5
	vmulpd	zmm8, zmm16, zmm30		;; A5 = R5 * MemR5					; 96-99		n 100
	vmulpd	zmm19, zmm9, zmm30		;; B5 = I5 * MemR5					; 96-99		n 100
	zstore	[srcreg+d4], zmm16		;; R5
	zstore	[srcreg+d4+64], zmm9		;; I5

	vmovapd	zmm30, [srcreg+rbp+d4+d1]	;; MemR6
	vmulpd	zmm0, zmm12, zmm30		;; A6 = R6 * MemR6					; 97-100	n 101
	vmulpd	zmm14, zmm4, zmm30		;; B6 = I6 * MemR6					; 97-100	n 101
	zstore	[srcreg+d4+d1], zmm12		;; R6
	zstore	[srcreg+d4+d1+64], zmm4		;; I6

	zfnmaddpd zmm30, zmm3, zmm31, zmm15	;; R7 - I8 * SQRTHALF (final R7)			; 98-101	n 102
	zfmaddpd zmm3, zmm3, zmm31, zmm15	;; R7 + I8 * SQRTHALF (final R8)			; 98-101	n 103

	vmovapd zmm15, [srcreg+rbp+d2+d1+64]	;; MemI4
	zfnmaddpd zmm5, zmm7, zmm15, zmm5	;; A4 - I4 * MemI4 (R4)					; 99-102	n 113
	zfmaddpd zmm13, zmm11, zmm15, zmm13	;; B4 + R4 * MemI4 (I4)					; 99-102	n 108

	vmovapd zmm15, [srcreg+rbp+d4+64]	;; MemI5
	zfnmaddpd zmm8, zmm9, zmm15, zmm8	;; A5 - I5 * MemI5 (R5)					; 100-103	n 109
	zfmaddpd zmm19, zmm16, zmm15, zmm19	;; B5 + R5 * MemI5 (I5)					; 100-103	n 105

	vmovapd zmm15, [srcreg+rbp+d4+d1+64]	;; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0	;; A6 - I6 * MemI6 (R6)					; 101-104	n 109
	zfmaddpd zmm14, zmm12, zmm15, zmm14	;; B6 + R6 * MemI6 (I6)					; 101-104	n 105

	vmovapd	zmm15, [srcreg+rbp+d4+d2]	;; MemR7
	vmulpd	zmm7, zmm30, zmm15		;; A7 = R7 * MemR7					; 102-105	n 106
	vmulpd	zmm11, zmm2, zmm15		;; B7 = I7 * MemR7					; 102-105	n 106
	zstore	[srcreg+d4+d2], zmm30		;; R7
	zstore	[srcreg+d4+d2+64], zmm2		;; I7

	vmovapd	zmm15, [srcreg+rbp+d4+d2+d1]	;; MemR8
	vmulpd	zmm9, zmm3, zmm15		;; A8 = R8 * MemR8					; 103-106	n 107
	vmulpd	zmm16, zmm10, zmm15		;; B8 = I8 * MemR8					; 103-106	n 107
	zstore	[srcreg+d4+d2+d1], zmm3		;; R8
	zstore	[srcreg+d4+d2+d1+64], zmm10	;; I8

	cmp	r10, 84h			;; Case off opcode
	mov	r10, SRC3ARG			;; Load distance to FFT(1)
	jb	muladd				;; 83=muladd
	je	mulsub				;; 84=mulsub

fma:	vmovapd	zmm30, [srcreg+rbp]		;; MemR1
	vmulpd	zmm20, zmm13, zmm30		;; A1 = R1 * MemR1					; 88-91		n 92
	vmulpd	zmm30, zmm5, zmm30		;; B1 = I1 * MemR1					; 88-91		n 92

	vmovapd	zmm21, [srcreg+rbp+d1]		;; MemR2
	vmulpd	zmm6, zmm19, zmm21		;; A2 = R2 * MemR2					; 89-92		n 93
	vmulpd	zmm21, zmm8, zmm21		;; B2 = I2 * MemR2					; 89-92		n 93

	vmovapd	zmm1, [srcreg+rbp+d2]		;; MemR3
	vmulpd	zmm17, zmm14, zmm1		;; A3 = R3 * MemR3					; 90-93		n 94
	vmulpd	zmm1, zmm0, zmm1		;; B3 = I3 * MemR3					; 90-93		n 94

	zfmaddpd zmm16, zmm12, zmm31, zmm18	;; R5 + R6 * SQRTHALF (final R5)			; 91-94		n 96
	zfnmaddpd zmm12, zmm12, zmm31, zmm18	;; R5 - R6 * SQRTHALF (final R6)			; 91-94		n 97

	vmovapd zmm18, [srcreg+rbp+64]		;; MemI1
	zfnmaddpd zmm20, zmm5, zmm18, zmm20	;; A1 - I1 * MemI1 (R1)					; 92-95		n 110
	zfmaddpd zmm18, zmm13, zmm18, zmm30	;; B1 + R1 * MemI1 (I1)					; 92-95		n 104

	vmovapd zmm30, [srcreg+rbp+d1+64]	;; MemI2
	zfnmaddpd zmm6, zmm8, zmm30, zmm6	;; A2 - I2 * MemI2 (R2)					; 93-96		n 110
	zfmaddpd zmm21, zmm19, zmm30, zmm21	;; B2 + R2 * MemI2 (I2)					; 93-96		n 104

	vmovapd zmm30, [srcreg+rbp+d2+64]	;; MemI3
	zfnmaddpd zmm17, zmm0, zmm30, zmm17	;; A3 - I3 * MemI3 (R3)					; 94-97		n 113
	zfmaddpd zmm1, zmm14, zmm30, zmm1	;; B3 + R3 * MemI3 (I3)					; 94-97		n 108

	vmovapd	zmm30, [srcreg+rbp+d2+d1]	;; MemR4
	vmulpd	zmm5, zmm11, zmm30		;; A4 = R4 * MemR4					; 95-98		n 99
	vmulpd	zmm13, zmm7, zmm30		;; B4 = I4 * MemR4					; 95-98		n 99

	vmovapd	zmm30, [srcreg+rbp+d4]		;; MemR5
	vmulpd	zmm8, zmm16, zmm30		;; A5 = R5 * MemR5					; 96-99		n 100
	vmulpd	zmm19, zmm9, zmm30		;; B5 = I5 * MemR5					; 96-99		n 100

	vmovapd	zmm30, [srcreg+rbp+d4+d1]	;; MemR6
	vmulpd	zmm0, zmm12, zmm30		;; A6 = R6 * MemR6					; 97-100	n 101
	vmulpd	zmm14, zmm4, zmm30		;; B6 = I6 * MemR6					; 97-100	n 101

	zfnmaddpd zmm30, zmm3, zmm31, zmm15	;; R7 - I8 * SQRTHALF (final R7)			; 98-101	n 102
	zfmaddpd zmm3, zmm3, zmm31, zmm15	;; R7 + I8 * SQRTHALF (final R8)			; 98-101	n 103

	vmovapd zmm15, [srcreg+rbp+d2+d1+64]	;; MemI4
	zfnmaddpd zmm5, zmm7, zmm15, zmm5	;; A4 - I4 * MemI4 (R4)					; 99-102	n 113
	zfmaddpd zmm13, zmm11, zmm15, zmm13	;; B4 + R4 * MemI4 (I4)					; 99-102	n 108

	vmovapd zmm15, [srcreg+rbp+d4+64]	;; MemI5
	zfnmaddpd zmm8, zmm9, zmm15, zmm8	;; A5 - I5 * MemI5 (R5)					; 100-103	n 109
	zfmaddpd zmm19, zmm16, zmm15, zmm19	;; B5 + R5 * MemI5 (I5)					; 100-103	n 105

	vmovapd zmm15, [srcreg+rbp+d4+d1+64]	;; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0	;; A6 - I6 * MemI6 (R6)					; 101-104	n 109
	zfmaddpd zmm14, zmm12, zmm15, zmm14	;; B6 + R6 * MemI6 (I6)					; 101-104	n 105

	vmovapd	zmm15, [srcreg+rbp+d4+d2]	;; MemR7
	vmulpd	zmm7, zmm30, zmm15		;; A7 = R7 * MemR7					; 102-105	n 106
	vmulpd	zmm11, zmm2, zmm15		;; B7 = I7 * MemR7					; 102-105	n 106

	vmovapd	zmm15, [srcreg+rbp+d4+d2+d1]	;; MemR8
	vmulpd	zmm9, zmm3, zmm15		;; A8 = R8 * MemR8					; 103-106	n 107
	vmulpd	zmm16, zmm10, zmm15		;; B8 = I8 * MemR8					; 103-106	n 107

	cmp	r10, 4				;; Case off opcode
	mov	r10, SRC3ARG			;; Load distance to FFT(1)
	;jb	muladd				;; 3=muladd, fall through
	je	mulsub				;; 4=mulsub

muladd:	cmp	r10, 1				;; Check if we need to multiply add-in by FFT(1)
	jne	muladdhard			;; Yes, do it the hard way

	vaddpd	zmm20, zmm20, [srcreg+r9]		;; R1 = R1 + MemR1
	vaddpd	zmm18, zmm18, [srcreg+r9+64]		;; I1 = I1 + MemI1
	vaddpd	zmm6, zmm6, [srcreg+r9+d1]		;; R2 = R2 + MemR2
	vaddpd	zmm21, zmm21, [srcreg+r9+d1+64]		;; I2 = I2 + MemI2
	vaddpd	zmm17, zmm17, [srcreg+r9+d2]		;; R3 = R3 + MemR3
	vaddpd	zmm1, zmm1, [srcreg+r9+d2+64]		;; I3 = I3 + MemI3
	vaddpd	zmm5, zmm5, [srcreg+r9+d2+d1]		;; R4 = R4 + MemR4
	vaddpd	zmm13, zmm13, [srcreg+r9+d2+d1+64]	;; I4 = I4 + MemI4
	vaddpd	zmm8, zmm8, [srcreg+r9+d4]		;; R5 = R5 + MemR5
	vaddpd	zmm19, zmm19, [srcreg+r9+d4+64]		;; I5 = I5 + MemI5
	vaddpd	zmm0, zmm0, [srcreg+r9+d4+d1]		;; R6 = R6 + MemR6
	vaddpd	zmm14, zmm14, [srcreg+r9+d4+d1+64]	;; I6 = I6 + MemI6

	vaddpd	zmm4, zmm18, zmm21			;; I1 + I2 (new I1)					; 104-107	n 114
	vsubpd	zmm18, zmm18, zmm21			;; I1 - I2 (new I2)					; 104-107	n 121

	vaddpd	zmm12, zmm19, zmm14			;; I5 + I6 (new I5)					; 105-108	n 116
	vsubpd	zmm14, zmm19, zmm14			;; I5 - I6 (new I6)					; 105-108	n 117

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]		;; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7		;; A7 - I7 * MemI7 (R7)					; 106-109	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11		;; B7 + R7 * MemI7 (I7)					; 106-109	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]		;; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9		;; A8 - I8 * MemI8 (R8)					; 107-110	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16		;; B8 + R8 * MemI8 (I8)					; 107-110	n 112

	vaddpd	zmm7, zmm7, [srcreg+r9+d4+d2]		;; R7 = R7 + MemR7
	vaddpd	zmm11, zmm11, [srcreg+r9+d4+d2+64]	;; I7 = I7 + MemI7
	vaddpd	zmm9, zmm9, [srcreg+r9+d4+d2+d1]	;; R8 = R8 + MemR8
	vaddpd	zmm16, zmm16, [srcreg+r9+d4+d2+d1+64]	;; I8 = I8 + MemI8

	jmp	done

muladdhard:
	vmovapd	zmm4, [srcreg+r9]			;; MemR1
	vmovapd	zmm12, [srcreg+r10]			;; MemR1#2
	vmovapd	zmm15, [srcreg+r10+64]			;; MemI1#2
	zfmaddpd zmm20, zmm4, zmm12, zmm20		;; R1 = R1 + MemR1*MemR1#2
	zfmaddpd zmm18, zmm4, zmm15, zmm18		;; I1 = I1 + MemR1*MemI1#2
	vmovapd	zmm4, [srcreg+r9+64]			;; MemI1
	zfnmaddpd zmm20, zmm4, zmm15, zmm20		;; R1 = R1 - MemI1*MemI1#2
	zfmaddpd zmm18, zmm4, zmm12, zmm18		;; I1 = I1 + MemI1*MemR1#2

	vmovapd	zmm4, [srcreg+r9+d1]			;; MemR2
	vmovapd	zmm12, [srcreg+r10+d1]			;; MemR2#2
	vmovapd	zmm15, [srcreg+r10+d1+64]		;; MemI2#2
	zfmaddpd zmm6, zmm4, zmm12, zmm6		;; R2 = R2 + MemR2*MemR2#2
	zfmaddpd zmm21, zmm4, zmm15, zmm21		;; I2 = I2 + MemR2*MemI2#2
	vmovapd	zmm4, [srcreg+r9+d1+64]			;; MemI2
	zfnmaddpd zmm6, zmm4, zmm15, zmm6		;; R2 = R2 - MemI2*MemI2#2
	zfmaddpd zmm21, zmm4, zmm12, zmm21		;; I2 = I2 + MemI2*MemR2#2

	vmovapd	zmm4, [srcreg+r9+d2]			;; MemR3
	vmovapd	zmm12, [srcreg+r10+d2]			;; MemR3#2
	vmovapd	zmm15, [srcreg+r10+d2+64]		;; MemI3#2
	zfmaddpd zmm17, zmm4, zmm12, zmm17		;; R3 = R3 + MemR3*MemR3#2
	zfmaddpd zmm1, zmm4, zmm15, zmm1		;; I3 = I3 + MemR3*MemI3#2
	vmovapd	zmm4, [srcreg+r9+d2+64]			;; MemI3
	zfnmaddpd zmm17, zmm4, zmm15, zmm17		;; R3 = R3 - MemI3*MemI3#2
	zfmaddpd zmm1, zmm4, zmm12, zmm1		;; I3 = I3 + MemI3*MemR3#2

	vmovapd	zmm4, [srcreg+r9+d2+d1]			;; MemR4
	vmovapd	zmm12, [srcreg+r10+d2+d1]		;; MemR4#2
	vmovapd	zmm15, [srcreg+r10+d2+d1+64]		;; MemI4#2
	zfmaddpd zmm5, zmm4, zmm12, zmm5		;; R4 = R4 + MemR4*MemR4#2
	zfmaddpd zmm13, zmm4, zmm15, zmm13		;; I4 = I4 + MemR4*MemI4#2
	vmovapd	zmm4, [srcreg+r9+d2+d1+64]		;; MemI4
	zfnmaddpd zmm5, zmm4, zmm15, zmm5		;; R4 = R4 - MemI4*MemI4#2
	zfmaddpd zmm13, zmm4, zmm12, zmm13		;; I4 = I4 + MemI4*MemR4#2

	vmovapd	zmm4, [srcreg+r9+d4]			;; MemR5
	vmovapd	zmm12, [srcreg+r10+d4]			;; MemR5#2
	vmovapd	zmm15, [srcreg+r10+d4+64]		;; MemI5#2
	zfmaddpd zmm8, zmm4, zmm12, zmm8		;; R5 = R5 + MemR5*MemR5#2
	zfmaddpd zmm19, zmm4, zmm15, zmm19		;; I5 = I5 + MemR5*MemI5#2
	vmovapd	zmm4, [srcreg+r9+d4+64]			;; MemI5
	zfnmaddpd zmm8, zmm4, zmm15, zmm8		;; R5 = R5 - MemI5*MemI5#2
	zfmaddpd zmm19, zmm4, zmm12, zmm19		;; I5 = I5 + MemI5*MemR5#2

	vmovapd	zmm4, [srcreg+r9+d4+d1]			;; MemR6
	vmovapd	zmm12, [srcreg+r10+d4+d1]		;; MemR6#2
	vmovapd	zmm15, [srcreg+r10+d4+d1+64]		;; MemI6#2
	zfmaddpd zmm0, zmm4, zmm12, zmm0		;; R6 = R6 + MemR6*MemR6#2
	zfmaddpd zmm14, zmm4, zmm15, zmm14		;; I6 = I6 + MemR6*MemI6#2
	vmovapd	zmm4, [srcreg+r9+d4+d1+64]		;; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0		;; R6 = R6 - MemI6*MemI6#2
	zfmaddpd zmm14, zmm4, zmm12, zmm14		;; I6 = I6 + MemI6*MemR6#2

	vaddpd	zmm4, zmm18, zmm21			;; I1 + I2 (new I1)					; 104-107	n 114
	vsubpd	zmm18, zmm18, zmm21			;; I1 - I2 (new I2)					; 104-107	n 121

	vaddpd	zmm12, zmm19, zmm14			;; I5 + I6 (new I5)					; 105-108	n 116
	vsubpd	zmm14, zmm19, zmm14			;; I5 - I6 (new I6)					; 105-108	n 117

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]		;; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7		;; A7 - I7 * MemI7 (R7)					; 106-109	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11		;; B7 + R7 * MemI7 (I7)					; 106-109	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]		;; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9		;; A8 - I8 * MemI8 (R8)					; 107-110	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16		;; B8 + R8 * MemI8 (I8)					; 107-110	n 112

	vmovapd	zmm2, [srcreg+r9+d4+d2]			;; MemR7
	vmovapd	zmm30, [srcreg+r10+d4+d2]		;; MemR7#2
	vmovapd	zmm15, [srcreg+r10+d4+d2+64]		;; MemI7#2
	zfmaddpd zmm7, zmm2, zmm30, zmm7		;; R7 = R7 + MemR7*MemR7#2
	zfmaddpd zmm11, zmm2, zmm15, zmm11		;; I7 = I7 + MemR7*MemI7#2
	vmovapd	zmm2, [srcreg+r9+d4+d2+64]		;; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7		;; R7 = R7 - MemI7*MemI7#2
	zfmaddpd zmm11, zmm2, zmm30, zmm11		;; I7 = I7 + MemI7*MemR7#2

	vmovapd	zmm2, [srcreg+r9+d4+d2+d1]		;; MemR8
	vmovapd	zmm30, [srcreg+r10+d4+d2+d1]		;; MemR8#2
	vmovapd	zmm15, [srcreg+r10+d4+d2+d1+64]		;; MemI8#2
	zfmaddpd zmm9, zmm2, zmm30, zmm9		;; R8 = R8 + MemR8*MemR8#2
	zfmaddpd zmm16, zmm2, zmm15, zmm16		;; I8 = I8 + MemR8*MemI8#2
	vmovapd	zmm2, [srcreg+r9+d4+d2+d1+64]		;; MemI8
	zfnmaddpd zmm9, zmm2, zmm15, zmm9		;; R8 = R8 - MemI8*MemI8#2
	zfmaddpd zmm16, zmm2, zmm30, zmm16		;; I8 = I8 + MemI8*MemR8#2

	jmp	done

mulsub:	cmp	r10, 1				;; Check if we need to multiply add-in by FFT(1)
	jne	mulsubhard			;; Yes, do it the hard way

	vsubpd	zmm20, zmm20, [srcreg+r9]		;; R1 = R1 - MemR1
	vsubpd	zmm18, zmm18, [srcreg+r9+64]		;; I1 = I1 - MemI1
	vsubpd	zmm6, zmm6, [srcreg+r9+d1]		;; R2 = R2 - MemR2
	vsubpd	zmm21, zmm21, [srcreg+r9+d1+64]		;; I2 = I2 - MemI2
	vsubpd	zmm17, zmm17, [srcreg+r9+d2]		;; R3 = R3 - MemR3
	vsubpd	zmm1, zmm1, [srcreg+r9+d2+64]		;; I3 = I3 - MemI3
	vsubpd	zmm5, zmm5, [srcreg+r9+d2+d1]		;; R4 = R4 - MemR4
	vsubpd	zmm13, zmm13, [srcreg+r9+d2+d1+64]	;; I4 = I4 - MemI4
	vsubpd	zmm8, zmm8, [srcreg+r9+d4]		;; R5 = R5 - MemR5
	vsubpd	zmm19, zmm19, [srcreg+r9+d4+64]		;; I5 = I5 - MemI5
	vsubpd	zmm0, zmm0, [srcreg+r9+d4+d1]		;; R6 = R6 - MemR6
	vsubpd	zmm14, zmm14, [srcreg+r9+d4+d1+64]	;; I6 = I6 - MemI6

	vaddpd	zmm4, zmm18, zmm21			;; I1 + I2 (new I1)					; 104-107	n 114
	vsubpd	zmm18, zmm18, zmm21			;; I1 - I2 (new I2)					; 104-107	n 121

	vaddpd	zmm12, zmm19, zmm14			;; I5 + I6 (new I5)					; 105-108	n 116
	vsubpd	zmm14, zmm19, zmm14			;; I5 - I6 (new I6)					; 105-108	n 117

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]		;; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7		;; A7 - I7 * MemI7 (R7)					; 106-109	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11		;; B7 + R7 * MemI7 (I7)					; 106-109	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]		;; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9		;; A8 - I8 * MemI8 (R8)					; 107-110	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16		;; B8 + R8 * MemI8 (I8)					; 107-110	n 112

	vsubpd	zmm7, zmm7, [srcreg+r9+d4+d2]		;; R7 = R7 - MemR7
	vsubpd	zmm11, zmm11, [srcreg+r9+d4+d2+64]	;; I7 = I7 - MemI7
	vsubpd	zmm9, zmm9, [srcreg+r9+d4+d2+d1]	;; R8 = R8 - MemR8
	vsubpd	zmm16, zmm16, [srcreg+r9+d4+d2+d1+64]	;; I8 = I8 - MemI8

	jmp	done

mulsubhard:
	vmovapd	zmm4, [srcreg+r9]			;; MemR1
	vmovapd	zmm12, [srcreg+r10]			;; MemR1#2
	vmovapd	zmm15, [srcreg+r10+64]			;; MemI1#2
	zfnmaddpd zmm20, zmm4, zmm12, zmm20		;; R1 = R1 - MemR1*MemR1#2
	zfnmaddpd zmm18, zmm4, zmm15, zmm18		;; I1 = I1 - MemR1*MemI1#2
	vmovapd	zmm4, [srcreg+r9+64]			;; MemI1
	zfmaddpd zmm20, zmm4, zmm15, zmm20		;; R1 = R1 + MemI1*MemI1#2
	zfnmaddpd zmm18, zmm4, zmm12, zmm18		;; I1 = I1 - MemI1*MemR1#2

	vmovapd	zmm4, [srcreg+r9+d1]			;; MemR2
	vmovapd	zmm12, [srcreg+r10+d1]			;; MemR2#2
	vmovapd	zmm15, [srcreg+r10+d1+64]		;; MemI2#2
	zfnmaddpd zmm6, zmm4, zmm12, zmm6		;; R2 = R2 - MemR2*MemR2#2
	zfnmaddpd zmm21, zmm4, zmm15, zmm21		;; I2 = I2 - MemR2*MemI2#2
	vmovapd	zmm4, [srcreg+r9+d1+64]			;; MemI2
	zfmaddpd zmm6, zmm4, zmm15, zmm6		;; R2 = R2 + MemI2*MemI2#2
	zfnmaddpd zmm21, zmm4, zmm12, zmm21		;; I2 = I2 - MemI2*MemR2#2

	vmovapd	zmm4, [srcreg+r9+d2]			;; MemR3
	vmovapd	zmm12, [srcreg+r10+d2]			;; MemR3#2
	vmovapd	zmm15, [srcreg+r10+d2+64]		;; MemI3#2
	zfnmaddpd zmm17, zmm4, zmm12, zmm17		;; R3 = R3 - MemR3*MemR3#2
	zfnmaddpd zmm1, zmm4, zmm15, zmm1		;; I3 = I3 - MemR3*MemI3#2
	vmovapd	zmm4, [srcreg+r9+d2+64]			;; MemI3
	zfmaddpd zmm17, zmm4, zmm15, zmm17		;; R3 = R3 + MemI3*MemI3#2
	zfnmaddpd zmm1, zmm4, zmm12, zmm1		;; I3 = I3 - MemI3*MemR3#2

	vmovapd	zmm4, [srcreg+r9+d2+d1]			;; MemR4
	vmovapd	zmm12, [srcreg+r10+d2+d1]		;; MemR4#2
	vmovapd	zmm15, [srcreg+r10+d2+d1+64]		;; MemI4#2
	zfnmaddpd zmm5, zmm4, zmm12, zmm5		;; R4 = R4 - MemR4*MemR4#2
	zfnmaddpd zmm13, zmm4, zmm15, zmm13		;; I4 = I4 - MemR4*MemI4#2
	vmovapd	zmm4, [srcreg+r9+d2+d1+64]		;; MemI4
	zfmaddpd zmm5, zmm4, zmm15, zmm5		;; R4 = R4 + MemI4*MemI4#2
	zfnmaddpd zmm13, zmm4, zmm12, zmm13		;; I4 = I4 - MemI4*MemR4#2

	vmovapd	zmm4, [srcreg+r9+d4]			;; MemR5
	vmovapd	zmm12, [srcreg+r10+d4]			;; MemR5#2
	vmovapd	zmm15, [srcreg+r10+d4+64]		;; MemI5#2
	zfnmaddpd zmm8, zmm4, zmm12, zmm8		;; R5 = R5 - MemR5*MemR5#2
	zfnmaddpd zmm19, zmm4, zmm15, zmm19		;; I5 = I5 - MemR5*MemI5#2
	vmovapd	zmm4, [srcreg+r9+d4+64]			;; MemI5
	zfmaddpd zmm8, zmm4, zmm15, zmm8		;; R5 = R5 + MemI5*MemI5#2
	zfnmaddpd zmm19, zmm4, zmm12, zmm19		;; I5 = I5 - MemI5*MemR5#2

	vmovapd	zmm4, [srcreg+r9+d4+d1]			;; MemR6
	vmovapd	zmm12, [srcreg+r10+d4+d1]		;; MemR6#2
	vmovapd	zmm15, [srcreg+r10+d4+d1+64]		;; MemI6#2
	zfnmaddpd zmm0, zmm4, zmm12, zmm0		;; R6 = R6 - MemR6*MemR6#2
	zfnmaddpd zmm14, zmm4, zmm15, zmm14		;; I6 = I6 - MemR6*MemI6#2
	vmovapd	zmm4, [srcreg+r9+d4+d1+64]		;; MemI6
	zfmaddpd zmm0, zmm4, zmm15, zmm0		;; R6 = R6 + MemI6*MemI6#2
	zfnmaddpd zmm14, zmm4, zmm12, zmm14		;; I6 = I6 - MemI6*MemR6#2

	vaddpd	zmm4, zmm18, zmm21			;; I1 + I2 (new I1)					; 104-107	n 114
	vsubpd	zmm18, zmm18, zmm21			;; I1 - I2 (new I2)					; 104-107	n 121

	vaddpd	zmm12, zmm19, zmm14			;; I5 + I6 (new I5)					; 105-108	n 116
	vsubpd	zmm14, zmm19, zmm14			;; I5 - I6 (new I6)					; 105-108	n 117

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]		;; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7		;; A7 - I7 * MemI7 (R7)					; 106-109	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11		;; B7 + R7 * MemI7 (I7)					; 106-109	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]		;; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9		;; A8 - I8 * MemI8 (R8)					; 107-110	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16		;; B8 + R8 * MemI8 (I8)					; 107-110	n 112

	vmovapd	zmm2, [srcreg+r9+d4+d2]			;; MemR7
	vmovapd	zmm30, [srcreg+r10+d4+d2]		;; MemR7#2
	vmovapd	zmm15, [srcreg+r10+d4+d2+64]		;; MemI7#2
	zfnmaddpd zmm7, zmm2, zmm30, zmm7		;; R7 = R7 - MemR7*MemR7#2
	zfnmaddpd zmm11, zmm2, zmm15, zmm11		;; I7 = I7 - MemR7*MemI7#2
	vmovapd	zmm2, [srcreg+r9+d4+d2+64]		;; MemI7
	zfmaddpd zmm7, zmm2, zmm15, zmm7		;; R7 = R7 + MemI7*MemI7#2
	zfnmaddpd zmm11, zmm2, zmm30, zmm11		;; I7 = I7 - MemI7*MemR7#2

	vmovapd	zmm2, [srcreg+r9+d4+d2+d1]		;; MemR8
	vmovapd	zmm30, [srcreg+r10+d4+d2+d1]		;; MemR8#2
	vmovapd	zmm15, [srcreg+r10+d4+d2+d1+64]		;; MemI8#2
	zfnmaddpd zmm9, zmm2, zmm30, zmm9		;; R8 = R8 - MemR8*MemR8#2
	zfnmaddpd zmm16, zmm2, zmm15, zmm16		;; I8 = I8 - MemR8*MemI8#2
	vmovapd	zmm2, [srcreg+r9+d4+d2+d1+64]		;; MemI8
	zfmaddpd zmm9, zmm2, zmm15, zmm9		;; R8 = R8 + MemI8*MemI8#2
	zfnmaddpd zmm16, zmm2, zmm30, zmm16		;; I8 = I8 - MemI8*MemR8#2

done:	
	ENDM


zr64f_sixtyfour_complex_with_mulf_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRTHALF
	ENDM
zr64f_sixtyfour_complex_with_mulf MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	LOCAL	orig, back_to_orig

	cmp	mul4_opcode, 0			;; See if we need to do more than the original type-4 FFT multiply
	je	short orig
	call	zcomplex_mulf_opcode		;; Handle more difficult cases
	jmp	back_to_orig
orig:

	;; Multiply the complex numbers
	vmovapd	zmm16, [srcreg][rbx]		;; R1
	vmovapd	zmm9, [srcreg+64][rbx]		;; I1
	vmovapd	zmm18, [srcreg+rbp]		;; MemR1
	vmulpd	zmm20, zmm16, zmm18		;; A1 = R1 * MemR1					; 90-93		n 94
	vmulpd	zmm18, zmm9, zmm18		;; B1 = I1 * MemR1					; 90-93		n 94

	vmovapd	zmm19, [srcreg+d1][rbx]		;; R2
	vmovapd	zmm8, [srcreg+d1+64][rbx]	;; I2
	vmovapd	zmm21, [srcreg+rbp+d1]		;; MemR2
	vmulpd	zmm6, zmm19, zmm21		;; A2 = R2 * MemR2					; 91-94		n 95
	vmulpd	zmm21, zmm8, zmm21		;; B2 = I2 * MemR2					; 91-94		n 95

	vmovapd	zmm14, [srcreg+d2][rbx]		;; R3
	vmovapd	zmm0, [srcreg+d2+64][rbx]	;; I3
	vmovapd	zmm1, [srcreg+rbp+d2]		;; MemR3
	vmulpd	zmm17, zmm14, zmm1		;; A3 = R3 * MemR3					; 92-95		n 96
	vmulpd	zmm1, zmm0, zmm1		;; B3 = I3 * MemR3					; 92-95		n 96

	vmovapd	zmm11, [srcreg+d2+d1][rbx]	;; R4
	vmovapd	zmm7, [srcreg+d2+d1+64][rbx]	;; I4
	vmovapd	zmm13, [srcreg+rbp+d2+d1]	;; MemR4
	vmulpd	zmm5, zmm11, zmm13		;; A4 = R4 * MemR4					; 93-96		n 97
	vmulpd	zmm13, zmm7, zmm13		;; B4 = I4 * MemR4					; 93-96		n 97

	vmovapd zmm30, [srcreg+rbp+64]		;; MemI1
	zfnmaddpd zmm20, zmm9, zmm30, zmm20	;; A1 - I1 * MemI1 (R1)					; 94-97		n 110
	zfmaddpd zmm18, zmm16, zmm30, zmm18	;; B1 + R1 * MemI1 (I1)					; 94-97		n 106

	vmovapd zmm30, [srcreg+rbp+d1+64]	;; MemI2
	zfnmaddpd zmm6, zmm8, zmm30, zmm6	;; A2 - I2 * MemI2 (R2)					; 95-98		n 110
	zfmaddpd zmm21, zmm19, zmm30, zmm21	;; B2 + R2 * MemI2 (I2)					; 95-98		n 106

	vmovapd zmm30, [srcreg+rbp+d2+64]	;; MemI3
	zfnmaddpd zmm17, zmm0, zmm30, zmm17	;; A3 - I3 * MemI3 (R3)					; 96-99		n 113
	zfmaddpd zmm1, zmm14, zmm30, zmm1	;; B3 + R3 * MemI3 (I3)					; 96-99		n 108

	vmovapd zmm30, [srcreg+rbp+d2+d1+64]	;; MemI4
	zfnmaddpd zmm5, zmm7, zmm30, zmm5	;; A4 - I4 * MemI4 (R4)					; 97-100	n 113
	zfmaddpd zmm13, zmm11, zmm30, zmm13	;; B4 + R4 * MemI4 (I4)					; 97-100	n 108

	vmovapd	zmm22, [srcreg+d4][rbx]		;; R5
	vmovapd	zmm23, [srcreg+d4+64][rbx]	;; I5
	vmovapd	zmm19, [srcreg+rbp+d4]		;; MemR5
	vmulpd	zmm8, zmm22, zmm19		;; A5 = R5 * MemR5					; 98-101	n 102
	vmulpd	zmm19, zmm23, zmm19		;; B5 = I5 * MemR5					; 98-101	n 102

	vmovapd	zmm12, [srcreg+d4+d1][rbx]	;; R6
	vmovapd	zmm4, [srcreg+d4+d1+64][rbx]	;; I6
	vmovapd	zmm14, [srcreg+rbp+d4+d1]	;; MemR6
	vmulpd	zmm0, zmm12, zmm14		;; A6 = R6 * MemR6					; 99-102	n 103
	vmulpd	zmm14, zmm4, zmm14		;; B6 = I6 * MemR6					; 99-102	n 103

	vmovapd	zmm30, [srcreg+d4+d2][rbx]	;; R7
	vmovapd	zmm2, [srcreg+d4+d2+64][rbx]	;; I7
	vmovapd	zmm11, [srcreg+rbp+d4+d2]	;; MemR7
	vmulpd	zmm7, zmm30, zmm11		;; A7 = R7 * MemR7					; 100-103	n 104
	vmulpd	zmm11, zmm2, zmm11		;; B7 = I7 * MemR7					; 100-103	n 104

	vmovapd	zmm3, [srcreg+d4+d2+d1][rbx]	;; R8
	vmovapd	zmm10, [srcreg+d4+d2+d1+64][rbx];; I8
	vmovapd	zmm16, [srcreg+rbp+d4+d2+d1]	;; MemR8
	vmulpd	zmm9, zmm3, zmm16		;; A8 = R8 * MemR8					; 101-104	n 105
	vmulpd	zmm16, zmm10, zmm16		;; B8 = I8 * MemR8					; 101-104	n 105

	vmovapd zmm15, [srcreg+rbp+d4+64]	;; MemI5
	zfnmaddpd zmm8, zmm23, zmm15, zmm8	;; A5 - I5 * MemI5 (R5)					; 102-105	n 109
	zfmaddpd zmm19, zmm22, zmm15, zmm19	;; B5 + R5 * MemI5 (I5)					; 102-105	n 107

	vmovapd zmm15, [srcreg+rbp+d4+d1+64]	;; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0	;; A6 - I6 * MemI6 (R6)					; 103-106	n 109
	zfmaddpd zmm14, zmm12, zmm15, zmm14	;; B6 + R6 * MemI6 (I6)					; 103-106	n 107

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]	;; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7	;; A7 - I7 * MemI7 (R7)					; 104-107	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11	;; B7 + R7 * MemI7 (I7)					; 104-107	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]	;; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9	;; A8 - I8 * MemI8 (R8)					; 105-108	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16	;; B8 + R8 * MemI8 (I8)					; 105-108	n 112

back_to_orig:
	vaddpd	zmm4, zmm18, zmm21		;; I1 + I2 (new I1)					; 106-109	n 114
	vsubpd	zmm18, zmm18, zmm21		;; I1 - I2 (new I2)					; 106-109	n 121
	vmovapd zmm27, [screg+0*128+64]		;; cosine/sine for R2/I2 and R8/I8

	vaddpd	zmm12, zmm19, zmm14		;; I5 + I6 (new I5)					; 107-110	n 116
	vsubpd	zmm14, zmm19, zmm14		;; I5 - I6 (new I6)					; 107-110	n 117
	vmovapd zmm26, [screg+1*128+64]		;; cosine/sine for R3/I3 and R7/I7

	vaddpd	zmm21, zmm1, zmm13		;; I3 + I4 (new I3)					; 108-111	n 114
	vsubpd	zmm1, zmm1, zmm13		;; I3 - I4 (new R4)					; 108-111	n 127
	vmovapd zmm28, [screg+2*128+64]		;; cosine/sine for R4/I4 and R6/I6

	vaddpd	zmm13, zmm8, zmm0		;; R5 + R6 (new R5)					; 109-112	n 115
	vsubpd	zmm8, zmm8, zmm0		;; R5 - R6 (new R6)					; 109-112	n 117
	vmovapd zmm23, [screg+0*128]		;; sine for R2/I2 and R8/I8

	vaddpd	zmm10, zmm20, zmm6		;; R1 + R2 (new R1)					; 110-113	n 119
	vsubpd	zmm20, zmm20, zmm6		;; R1 - R2 (new R2)					; 110-113	n 127
	vmovapd zmm29, [screg+3*128+64]		;; cosine/sine for R5/I5

	vaddpd	zmm3, zmm9, zmm7		;; R8 + R7 (new R7)					; 111-114	n 115
	vsubpd	zmm9, zmm9, zmm7		;; R8 - R7 (new I8)					; 111-114	n 118
	vmovapd zmm25, [screg+3*128]		;; sine for R5/I5

	vaddpd	zmm15, zmm11, zmm16		;; I7 + I8 (new I7)					; 112-115	n 116
	vsubpd	zmm11, zmm11, zmm16		;; I7 - I8 (new R8)					; 112-115	n 118
	vmovapd zmm22, [screg+1*128]		;; sine for R3/I3 and R7/I7

 	vaddpd	zmm0, zmm5, zmm17		;; R4 + R3 (new R3)					; 113-116	n 119
	vsubpd	zmm5, zmm5, zmm17		;; R4 - R3 (new I4)					; 113-116	n 121
	vmovapd zmm24, [screg+2*128]		;; sine for R4/I4 and R6/I6

	vaddpd	zmm7, zmm4, zmm21		;; I1 + I3 (newer I1)					; 114-117	n 119
	vsubpd	zmm4, zmm4, zmm21		;; I1 - I3 (newer I3)					; 114-117	n 119
	bump	screg, scinc

	vsubpd	zmm6, zmm3, zmm13		;; R7 - R5 (newer I7)					; 115-118	n 121
	vaddpd	zmm3, zmm3, zmm13		;; R7 + R5 (newer R5)					; 115-118	n 123
	L1prefetchw srcreg+L1pd, L1pt

	vaddpd	zmm17, zmm12, zmm15		;; I5 + I7 (newer I5)					; 116-119	n 121
	vsubpd	zmm15, zmm12, zmm15		;; I5 - I7 (newer R7)					; 116-119	n 123
	L1prefetchw srcreg+64+L1pd, L1pt

	;; multiply R6/I6 by SQRTHALF - i*SQRTHALF
	vsubpd	zmm21, zmm14, zmm8		;; I6 = I6 - R6						; 117-120	n 123
	vaddpd	zmm14, zmm8, zmm14		;; R6 = R6 + I6						; 117-120	n 125
	L1prefetchw srcreg+d1+L1pd, L1pt

	;; multiply R8/I8 by SQRTHALF - i*SQRTHALF
	vsubpd	zmm13, zmm9, zmm11		;; I8 = I8 - R8						; 118-121	n 123
	vaddpd	zmm11, zmm11, zmm9		;; R8 = R8 + I8						; 118-121	n 125
	L1prefetchw srcreg+d1+64+L1pd, L1pt

	;; shuffle inputs are:
	;; R1 = R8_0 R7_0 R4_0 R3_0 R6_0 R5_0 R2_0 R1_0	goal new R1 is  R1_7 R1_6 R1_5 R1_4 R1_3 R1_2 R1_1 R1_0
	;; R2 = R8_1 R7_1 R4_1 R3_1 R6_1 R5_1 R2_1 R1_1	goal new R2 is  R2_7 R2_6 R2_5 R2_4 R2_3 R2_2 R2_1 R2_0, etc.
	;; I1 = I8_0 I7_0 I4_0 I3_0 I6_0 I5_0 I2_0 I1_0	goal new I1 is  I1_7 I1_6 I1_5 I1_4 I1_3 I1_2 I1_1 I1_0, etc.
	;; shuffle the two aparts		(can be done before last FFT level!)
	vshuff64x2 zmm9, zmm7, zmm4, 11101110b	;; I8_2 I7_2 I4_2 I3_2 I8_0 I7_0 I4_0 I3_0 (I13H)	; 119-121	n 129
	vshuff64x2 zmm7, zmm7, zmm4, 01000100b	;; I6_2 I5_2 I2_2 I1_2 I6_0 I5_0 I2_0 I1_0 (I13L)	; 120-122	n 137
	L1prefetchw srcreg+d2+L1pd, L1pt

	vaddpd	zmm4, zmm10, zmm0		;; R1 + R3 (newer R1)					; 119-122	n 125
	vsubpd	zmm10, zmm10, zmm0		;; R1 - R3 (newer R3)					; 120-123	n 125
	L1prefetchw srcreg+d2+64+L1pd, L1pt

	vshuff64x2 zmm0, zmm17, zmm6, 11101110b;; I8_6 I7_6 I4_6 I3_6 I8_4 I7_4 I4_4 I3_4 (I57H)	; 121-123	n 129
	vshuff64x2 zmm17, zmm17, zmm6, 01000100b;; I6_6 I5_6 I2_6 I1_6 I6_4 I5_4 I2_4 I1_4 (I57L)	; 122-124	n 137
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

 	vaddpd	zmm6, zmm18, zmm5		;; I2 + I4 (newer I2)					; 121-124	n 127
	vsubpd	zmm18, zmm18, zmm5		;; I2 - I4 (newer I4)					; 122-125	n 127
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt

	vshuff64x2 zmm8, zmm3, zmm15, 11101110b;; R8_6 R7_6 R4_6 R3_6 R8_4 R7_4 R4_4 R3_4 (R57H)	; 123-125	n 131
	vshuff64x2 zmm3, zmm3, zmm15, 01000100b;; R6_6 R5_6 R2_6 R1_6 R6_4 R5_4 R2_4 R1_4 (R57L)	; 124-126	n 139
	L1prefetchw srcreg+d4+L1pd, L1pt

	vaddpd	zmm15, zmm21, zmm13		;; I6 + I8 (newer I6/SQRTHALF)				; 123-126	n 129
	vsubpd	zmm21, zmm21, zmm13		;; I6 - I8 (newer R8/SQRTHALF)				; 124-127	n 131
	L1prefetchw srcreg+d4+64+L1pd, L1pt

	vshuff64x2 zmm13, zmm4, zmm10, 11101110b;; R8_2 R7_2 R4_2 R3_2 R8_0 R7_0 R4_0 R3_0 (R13H)	; 125-127	n 131
	vshuff64x2 zmm4, zmm4, zmm10, 01000100b	;; R6_2 R5_2 R2_2 R1_2 R6_0 R5_0 R2_0 R1_0 (R13L)	; 126-128	n 139
	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	vsubpd	zmm10, zmm11, zmm14		;; R8 - R6 (newer I8/SQRTHALF)				; 125-128	n 129
	vaddpd	zmm11, zmm11, zmm14		;; R8 + R6 (newer R6/SQRTHALF)				; 126-129	n 131
	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt

	vshuff64x2 zmm14, zmm6, zmm18, 11101110b;; I8_3 I7_3 I4_3 I3_3 I8_1 I7_1 I4_1 I3_1 (I24H)	; 127-129	n 133
	vshuff64x2 zmm6, zmm6, zmm18, 01000100b;; I6_3 I5_3 I2_3 I1_3 I6_1 I5_1 I2_1 I1_1 (I24L)	; 128-130	n 141
	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	vaddpd	zmm18, zmm20, zmm1		;; R2 + R4 (newer R2)					; 127-130	n 132
	vsubpd	zmm1, zmm20, zmm1		;; R2 - R4 (newer R4)					; 128-131	n 132
	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt

	vshuff64x2 zmm20, zmm15, zmm10, 11101110b;; I8_7 I7_7 I4_7 I3_7 I8_5 I7_5 I4_5 I3_5 (I68H)	; 129-131	n 133
	vshuff64x2 zmm15, zmm15, zmm10, 01000100b;; I6_7 I5_7 I2_7 I1_7 I6_5 I5_5 I2_5 I1_5 (I68L)	; 130-132	n 141
	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	vaddpd	zmm10, zmm9, zmm0		;; I13H + I57H (final I13H)				; 129-132	n 135
	vsubpd	zmm9, zmm9, zmm0		;; I13H - I57H (final I57H)				; 130-133	n 135
	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt

	vshuff64x2 zmm5, zmm11, zmm21, 11101110b;; R8_7 R7_7 R4_7 R3_7 R8_5 R7_5 R4_5 R3_5 (R68H)	; 131-133	n 135
	vshuff64x2 zmm12, zmm18, zmm1, 11101110b;; R8_3 R7_3 R4_3 R3_3 R8_1 R7_1 R4_1 R3_1 (R24H)	; 132-134	n 135	r132

	vaddpd	zmm0, zmm13, zmm8		;; R13H + R57H (final R13H)				; 131-134	n 137
	vsubpd	zmm13, zmm13, zmm8		;; R13H - R57H (final R57H)				; 132-135	n 137

	vshuff64x2 zmm11, zmm11, zmm21, 01000100b;; R6_7 R5_7 R2_7 R1_7 R6_5 R5_5 R2_5 R1_5 (R68L)	; 133-135	n 143
	vshuff64x2 zmm18, zmm18, zmm1, 01000100b;; R6_3 R5_3 R2_3 R1_3 R6_1 R5_1 R2_1 R1_1 (R24L)	; 134-136	n 143

	zfmaddpd zmm8, zmm20, zmm31, zmm14	;; I24H + I68H * SQRTHALF (final I24H)			; 133-136	n 138	r133
	zfnmaddpd zmm20, zmm20, zmm31, zmm14	;; I24H - I68H * SQRTHALF (final I68H)			; 134-137	n 138

	vshuff64x2 zmm21, zmm10, zmm9, 11011101b	;; I8_6 I7_6 I8_4 I7_4 I8_2 I7_2 I8_0 I7_0	; 135-137	n 143	r134
	vshuff64x2 zmm10, zmm10, zmm9, 10001000b;; I4_6 I3_6 I4_4 I3_4 I4_2 I3_2 I4_0 I3_0		; 136-138	n 147	r134

	zfmaddpd zmm1, zmm5, zmm31, zmm12	;; R24H + R68H * SQRTHALF (final R24H)			; 135-138	n 140	r135
	zfnmaddpd zmm5, zmm5, zmm31, zmm12	;; R24H - R68H * SQRTHALF (final R68H)			; 136-139	n 140

	vshuff64x2 zmm9, zmm0, zmm13, 11011101b	;; R8_6 R7_6 R8_4 R7_4 R8_2 R7_2 R8_0 R7_0		; 137-139	n 144	r136
	vshuff64x2 zmm14, zmm8, zmm20, 11011101b ;; I8_7 I7_7 I8_5 I7_5 I8_3 I7_3 I8_1 I7_1		; 138-140	n 143	r138

	vaddpd	zmm12, zmm7, zmm17		;; I13L + I57L (final I13L)				; 137-140	n 151
	vsubpd	zmm7, zmm7, zmm17		;; I13L - I57L (final I57L)				; 138-141	n 151

	vshuff64x2 zmm8, zmm8, zmm20, 10001000b	;; I4_7 I3_7 I4_5 I3_5 I4_3 I3_3 I4_1 I3_1		; 139-141	n 147	r138
	vshuff64x2 zmm17, zmm1, zmm5, 11011101b;; R8_7 R7_7 R8_5 R7_5 R8_3 R7_3 R8_1 R7_1		; 140-142	n 144	r140

	vaddpd	zmm20, zmm4, zmm3		;; R13L + R57L (final R13L)				; 139-142	n 154
	vsubpd	zmm4, zmm4, zmm3		;; R13L - R57L (final R57L)				; 140-143	n 154

	vshuff64x2 zmm0, zmm0, zmm13, 10001000b;; R4_6 R3_6 R4_4 R3_4 R4_2 R3_2 R4_0 R3_0		; 141-143	n 148	r136
	vshuff64x2 zmm1, zmm1, zmm5, 10001000b;; R4_7 R3_7 R4_5 R3_5 R4_3 R3_3 R4_1 R3_1		; 142-144	n 148	r140

	zfmaddpd zmm3, zmm15, zmm31, zmm6	;; I24L + I68L * SQRTHALF (final I24L)			; 141-144	n 152
	zfnmaddpd zmm15, zmm15, zmm31, zmm6	;; I24L - I68L * SQRTHALF (final I68L)			; 142-145	n 152

	vshufpd	zmm13, zmm21, zmm14, 11111111b	;; I8_7 I8_6 I8_5 I8_4 I8_3 I8_2 I8_1 I8_0 (next I8)	; 143		n 145
	vshufpd	zmm6, zmm9, zmm17, 11111111b	;; R8_7 R8_6 R8_5 R8_4 R8_3 R8_2 R8_1 R8_0 (next R8)	; 144		n 145

	zfmaddpd zmm5, zmm11, zmm31, zmm18	;; R24L + R68L * SQRTHALF (final R24L)			; 143-146	n 153
	zfnmaddpd zmm11, zmm11, zmm31, zmm18	;; R24L - R68L * SQRTHALF (final R68L)			; 144-147	n 153

	vshufpd	zmm21, zmm21, zmm14, 00000000b	;; I7_7 I7_6 I7_5 I7_4 I7_3 I7_2 I7_1 I7_0 (next I7)	; 145		n 147
	vshufpd	zmm9, zmm9, zmm17, 00000000b	;; R7_7 R7_6 R7_5 R7_4 R7_3 R7_2 R7_1 R7_0 (next R7)	; 146		n 147

	zfmsubpd zmm18, zmm6, zmm27, zmm13	;; A8 = R8 * cosine/sine - I8				; 145-148	n 157
	zfmaddpd zmm13, zmm13, zmm27, zmm6	;; B8 = I8 * cosine/sine + R8				; 146-149	n 158

	vshufpd	zmm14, zmm10, zmm8, 00000000b	;; I3_7 I3_6 I3_5 I3_4 I3_3 I3_2 I3_1 I3_0 (next I3)	; 147		n 149
	vshufpd	zmm17, zmm0, zmm1, 00000000b	;; R3_7 R3_6 R3_5 R3_4 R3_3 R3_2 R3_1 R3_0 (next R3)	; 148		n 149

	zfmsubpd zmm6, zmm9, zmm26, zmm21	;; R7 * cosine/sine - I7 (first R7/sine)		; 147-150	n 153
	zfmaddpd zmm21, zmm21, zmm26, zmm9	;; I7 * cosine/sine + R7 (first I7/sine)		; 148-151	n 155

	vshufpd	zmm10, zmm10, zmm8, 11111111b	;; I4_7 I4_6 I4_5 I4_4 I4_3 I4_2 I4_1 I4_0 (next I4)	; 149		n 151
 	vshufpd	zmm0, zmm0, zmm1, 11111111b	;; R4_7 R4_6 R4_5 R4_4 R4_3 R4_2 R4_1 R4_0 (next R4)	; 150		n 151

	zfmaddpd zmm9, zmm17, zmm26, zmm14	;; R3 * cosine/sine + I3 (first R3/sine)		; 149-152	n 153
	zfmsubpd zmm14, zmm14, zmm26, zmm17	;; I3 * cosine/sine - R3 (first I3/sine)		; 150-153	n 155

	;; shuffle the four aparts
	vshuff64x2 zmm8, zmm12, zmm7, 11011101b;; I6_6 I5_6 I6_4 I5_4 I6_2 I5_2 I6_0 I5_0		; 151-153	n 157
	vshuff64x2 zmm1, zmm3, zmm15, 11011101b;; I6_7 I5_7 I6_5 I5_5 I6_3 I5_3 I6_1 I5_1		; 152-154	n 157

	zfmaddpd zmm17, zmm0, zmm28, zmm10	;; R4 * cosine/sine + I4 (first R4/sine)		; 151-154	n 165
	zfmsubpd zmm10, zmm10, zmm28, zmm0	;; I4 * cosine/sine - R4 (first I4/sine)		; 152-155	n 166

	vshuff64x2 zmm0, zmm5, zmm11, 11011101b;; R6_7 R5_7 R6_5 R5_5 R6_3 R5_3 R6_1 R5_1		; 153-155	n 158
	vshuff64x2 zmm19, zmm20, zmm4, 11011101b;; R6_6 R5_6 R6_4 R5_4 R6_2 R5_2 R6_0 R5_0		; 154-156	n 158

	vaddpd	zmm2, zmm9, zmm6		;; R3/sine + R7/sine					; 153-156	n 174
	vsubpd	zmm9, zmm9, zmm6		;; R3/sine - R7/sine					; 154-157	n 171

	vshuff64x2 zmm12, zmm12, zmm7, 10001000b;; I2_6 I1_6 I2_4 I1_4 I2_2 I1_2 I2_0 I1_0		; 155-157	n 161
	vshuff64x2 zmm3, zmm3, zmm15, 10001000b;; I2_7 I1_7 I2_5 I1_5 I2_3 I1_3 I2_1 I1_1		; 156-158	n 161

	vaddpd	zmm6, zmm14, zmm21		;; I3/sine + I7/sine					; 155-158	n 170
	vsubpd	zmm14, zmm14, zmm21		;; I3/sine - I7/sine					; 156-159	n 175

	;; shufpd the one aparts
	vshufpd	zmm7, zmm8, zmm1, 00000000b	;; I5_7 I5_6 I5_5 I5_4 I5_3 I5_2 I5_1 I5_0 (next I5)	; 157		n 159
	vshufpd	zmm15, zmm19, zmm0, 00000000b	;; R5_7 R5_6 R5_5 R5_4 R5_3 R5_2 R5_1 R5_0 (next R5)	; 158		n 159

	vmulpd	zmm18, zmm18, zmm23		;; R8 = R8 * sine28					; 157-160	n 172
	vmulpd	zmm13, zmm13, zmm23		;; I8 = I8 * sine28					; 158-161	n 173

	vshufpd	zmm8, zmm8, zmm1, 11111111b	;; I6_7 I6_6 I6_5 I6_4 I6_3 I6_2 I6_1 I6_0 (next I6)	; 159		n 161
	vshufpd	zmm19, zmm19, zmm0, 11111111b	;; R6_7 R6_6 R6_5 R6_4 R6_3 R6_2 R6_1 R6_0 (next R6)	; 160		n 161

	zfmsubpd zmm21, zmm7, zmm29, zmm15	;; I5 * cosine/sine - R5 (first I5/sine)		; 159-162	n 163
	zfmaddpd zmm15, zmm15, zmm29, zmm7	;; R5 * cosine/sine + I5 (first R5/sine)		; 160-163	n 169

	vshufpd	zmm1, zmm12, zmm3, 00000000b	;; I1_7 I1_6 I1_5 I1_4 I1_3 I1_2 I1_1 I1_0 (next I1)	; 161		n 163
	vshuff64x2 zmm20, zmm20, zmm4, 10001000b;; R2_6 R1_6 R2_4 R1_4 R2_2 R1_2 R2_0 R1_0		; 162-164	n 166

	zfmsubpd zmm0, zmm19, zmm28, zmm8	;; R6 * cosine/sine - I6 (first R6/sine)		; 161-164	n 165
	zfmaddpd zmm8, zmm8, zmm28, zmm19	;; I6 * cosine/sine + R6 (first I6/sine)		; 162-165	n 166

	vshuff64x2 zmm5, zmm5, zmm11, 10001000b	;; R2_7 R1_7 R2_5 R1_5 R2_3 R1_3 R2_1 R1_1		; 163-165	n 166
	vshufpd	zmm12, zmm12, zmm3, 11111111b	;; I2_7 I2_6 I2_5 I2_4 I2_3 I2_2 I2_1 I2_0 (next I2)	; 164		n 168

	zfmaddpd zmm7, zmm21, zmm25, zmm1	;; I1 + I5 * sine					; 163-166	n 170
	zfnmaddpd zmm21, zmm21, zmm25, zmm1	;; I1 - I5 * sine					; 164-167	n 171

	vaddpd	zmm4, zmm17, zmm0		;; R4/sine + R6/sine					; 165-168	n 176
	vsubpd	zmm17, zmm17, zmm0		;; R4/sine - R6/sine					; 165-168	n 177

	vshufpd	zmm11, zmm20, zmm5, 11111111b	;; R2_7 R2_6 R2_5 R2_4 R2_3 R2_2 R2_1 R2_0 (next R2)	; 166		n 168
	vshufpd	zmm20, zmm20, zmm5, 00000000b	;; R1_7 R1_6 R1_5 R1_4 R1_3 R1_2 R1_1 R1_0 (next R1)	; 167		n 169

	vaddpd	zmm3, zmm10, zmm8		;; I4/sine + I6/sine					; 166-169	n 179
	vsubpd	zmm10, zmm10, zmm8		;; I4/sine - I6/sine					; 167-170	n 178

	zfmaddpd zmm1, zmm11, zmm27, zmm12	;; R2 * cosine/sine + I2 (first R2/sine)		; 168-171	n 172	r167
	zfmsubpd zmm12, zmm12, zmm27, zmm11	;; I2 * cosine/sine - R2 (first I2/sine)		; 168-171	n 173

	zfmaddpd zmm0, zmm15, zmm25, zmm20	;; R1 + R5 * sine					; 169-172	n 174	r168
	zfnmaddpd zmm15, zmm15, zmm25, zmm20	;; R1 - R5 * sine					; 169-172	n 175

	zfmaddpd zmm8, zmm6, zmm22, zmm7	;; i1++ = (i1+i5) + (i3+i7) * sine			; 170-173	n 184
	zfnmaddpd zmm6, zmm6, zmm22, zmm7	;; i1+- = (i1+i5) - (i3+i7) * sine			; 170-173	n 181

	zfmaddpd zmm11, zmm9, zmm22, zmm21	;; i1-+ = (i1-i5) + (r3-r7) * sine			; 171-174	n 189
	zfnmaddpd zmm9, zmm9, zmm22, zmm21	;; i1-- = (i1-i5) - (r3-r7) * sine			; 171-174	n 188

	zfmaddpd zmm20, zmm1, zmm23, zmm18	;; R2 * sine + R8					; 172-175	n 176	r171
	zfmsubpd zmm1, zmm1, zmm23, zmm18	;; R2 * sine - R8					; 172-175	n 177

	zfmaddpd zmm21, zmm12, zmm23, zmm13	;; I2 * sine + I8					; 173-176	n 179	r171
	zfmsubpd zmm12, zmm12, zmm23, zmm13	;; I2 * sine - I8					; 173-176	n 178

	zfmaddpd zmm18, zmm2, zmm22, zmm0	;; r1++ = (r1+r5) + (r3+r7) * sine			; 174-177	n 180	r173
	zfnmaddpd zmm2, zmm2, zmm22, zmm0	;; r1+- = (r1+r5) - (r3+r7) * sine			; 174-177	n 185

	zfmaddpd zmm13, zmm14, zmm22, zmm15	;; r1-+ = (r1-r5) + (i3-i7) * sine			; 175-178	n 186
	zfnmaddpd zmm14, zmm14, zmm22, zmm15	;; r1-- = (r1-r5) - (i3-i7) * sine			; 175-178	n 187

	zfmaddpd zmm0, zmm4, zmm24, zmm20	;; r2++ = (r2+r8) + (r4+r6) * sine			; 176-179	n 180
	zfnmaddpd zmm4, zmm4, zmm24, zmm20	;; r2+- = (r2+r8) - (r4+r6) * sine			; 176-179	n 182

	zfmaddpd zmm15, zmm17, zmm24, zmm1	;; r2-+ = (r2-r8) + (r4-r6) * sine			; 177-180	n 183
	zfnmaddpd zmm17, zmm17, zmm24, zmm1	;; r2-- = (r2-r8) - (r4-r6) * sine			; 177-180	n 181

	zfmaddpd zmm20, zmm10, zmm24, zmm12	;; i2-+ = (i2-i8) + (i4-i6) * sine			; 178-181	n 182
	zfnmaddpd zmm10, zmm10, zmm24, zmm12	;; i2-- = (i2-i8) - (i4-i6) * sine			; 178-181	n 185

	zfmaddpd zmm1, zmm3, zmm24, zmm21	;; i2++ = (i2+i8) + (i4+i6) * sine			; 179-182	n 184
	zfnmaddpd zmm3, zmm3, zmm24, zmm21	;; i2+- = (i2+i8) - (i4+i6) * sine			; 179-182	n 183

	vaddpd	zmm21, zmm18, zmm0		;; R1 = (r1++) + (r2++)					; 180-183		r180
	vsubpd	zmm18, zmm18, zmm0		;; R5 = (r1++) - (r2++)					; 180-183

	vsubpd	zmm0, zmm6, zmm17		;; I3 = (i1+-) - (r2--)					; 181-184		r181
	vaddpd	zmm6, zmm6, zmm17		;; I7 = (i1+-) + (r2--)					; 181-184

	vaddpd	zmm17, zmm4, zmm20		;; r2+-+ = (r2+-) + (i2-+)				; 182-185	n 186	r182
	vsubpd	zmm4, zmm4, zmm20		;; r2+-- = (r2+-) - (i2-+)				; 182-185	n 187

	vaddpd	zmm20, zmm15, zmm3		;; r2-++ = (r2-+) + (i2+-)				; 183-186	n 189	r183
	vsubpd	zmm15, zmm15, zmm3		;; r2-+- = (r2-+) - (i2+-)				; 183-186	n 188

	vaddpd	zmm3, zmm8, zmm1		;; I1 = (i1++) + (i2++)					; 184-187		r183
	vsubpd	zmm8, zmm8, zmm1		;; I5 = (i1++) - (i2++)					; 184-187
	zstore	[srcreg], zmm21			;; Save R1						; 184

	vaddpd	zmm1, zmm2, zmm10		;; R3 = (r1+-) + (i2--)					; 185-188		r182
	vsubpd	zmm2, zmm2, zmm10		;; R7 = (r1+-) - (i2--)					; 185-188
	zstore	[srcreg+d4], zmm18		;; Save R5						; 184+1

	zfmaddpd zmm10, zmm17, zmm31, zmm13	;; R2 = (r1-+) + .707*(r2+-+)				; 186-189		r186
	zfnmaddpd zmm17, zmm17, zmm31, zmm13	;; R6 = (r1-+) - .707*(r2+-+)				; 186-189
	zstore	[srcreg+d2+64], zmm0		;; Save I3						; 185+1

	zfnmaddpd zmm13, zmm4, zmm31, zmm14	;; R4 = (r1--) - .707*(r2+--)				; 187-190		r186
	zfmaddpd zmm4, zmm4, zmm31, zmm14	;; R8 = (r1--) + .707*(r2+--)				; 187-190
	zstore	[srcreg+d4+d2+64], zmm6		;; Save I7						; 185+2

	zfnmaddpd zmm14, zmm15, zmm31, zmm9	;; I2 = (i1--) - .707*(r2-+-)				; 188-191		r187
	zfmaddpd zmm15, zmm15, zmm31, zmm9	;; I6 = (i1--) + .707*(r2-+-)				; 188-191
	zstore	[srcreg+64], zmm3		;; Save I1						; 188

	zfnmaddpd zmm9, zmm20, zmm31, zmm11	;; I4 = (i1-+) - .707*(r2-++)				; 189-192		r187
	zfmaddpd zmm20, zmm20, zmm31, zmm11	;; I8 = (i1-+) + .707*(r2-++)				; 189-192
	zstore	[srcreg+d4+64], zmm8		;; Save I5						; 188+1

	zstore	[srcreg+d2], zmm1		;; Save R3						; 189+1
	zstore	[srcreg+d4+d2], zmm2		;; Save R7						; 189+2
	zstore	[srcreg+d1], zmm10		;; Save R2						; 190+2
	zstore	[srcreg+d4+d1], zmm17		;; Save R6						; 190+3
	zstore	[srcreg+d2+d1], zmm13		;; Save R4						; 191+3
	zstore	[srcreg+d4+d2+d1], zmm4		;; Save R8						; 191+4
	zstore	[srcreg+d1+64], zmm14		;; Save I2						; 192+5
	zstore	[srcreg+d4+d1+64], zmm15	;; Save I6						; 192+5
	zstore	[srcreg+d2+d1+64], zmm9		;; Save I4						; 193+5
	zstore	[srcreg+d4+d2+d1+64], zmm20	;; Save I8						; 193+6
	bump	srcreg, srcinc
	ENDM

;; Same as above code between orig and back_to_orig, except we implement mul4_opcode options

zr64_64c_mulf_opcode MACRO srcreg,d1,d2,d4
	LOCAL	submul, fma, muladd, muladdhard, mulsub, mulsubhard, done

	movzx	r10, mul4_opcode		;; Load the mul4_opcode
	mov	r9, SRC2ARG			;; Distance to s2 arg from gwaddmul4 or gwsubmul4 or s3 arg in gwmuladd4 or gwmulsub4
	cmp	r10, 2				;; Case off opcode
	jg	fma				;; 3,4=muladd,mulsub
	je	submul				;; 2=submul

	;; Multiply the complex numbers
	vmovapd	zmm16, [srcreg][rbx]		;; R1
	vmovapd	zmm9, [srcreg+64][rbx]		;; I1
	vmovapd	zmm18, [srcreg+rbp]		;; MemR1
	vaddpd	zmm18, zmm18, [srcreg+r9]	;; MemR1
	vmulpd	zmm20, zmm16, zmm18		;; A1 = R1 * MemR1					; 90-93		n 94
	vmulpd	zmm18, zmm9, zmm18		;; B1 = I1 * MemR1					; 90-93		n 94

	vmovapd	zmm19, [srcreg+d1][rbx]		;; R2
	vmovapd	zmm8, [srcreg+d1+64][rbx]	;; I2
	vmovapd	zmm21, [srcreg+rbp+d1]		;; MemR2
	vaddpd	zmm21, zmm21, [srcreg+r9+d1]	;; MemR2
	vmulpd	zmm6, zmm19, zmm21		;; A2 = R2 * MemR2					; 91-94		n 95
	vmulpd	zmm21, zmm8, zmm21		;; B2 = I2 * MemR2					; 91-94		n 95

	vmovapd	zmm14, [srcreg+d2][rbx]		;; R3
	vmovapd	zmm0, [srcreg+d2+64][rbx]	;; I3
	vmovapd	zmm1, [srcreg+rbp+d2]		;; MemR3
	vaddpd	zmm1, zmm1, [srcreg+r9+d2]	;; MemR3
	vmulpd	zmm17, zmm14, zmm1		;; A3 = R3 * MemR3					; 92-95		n 96
	vmulpd	zmm1, zmm0, zmm1		;; B3 = I3 * MemR3					; 92-95		n 96

	vmovapd	zmm11, [srcreg+d2+d1][rbx]	;; R4
	vmovapd	zmm7, [srcreg+d2+d1+64][rbx]	;; I4
	vmovapd	zmm13, [srcreg+rbp+d2+d1]	;; MemR4
	vaddpd	zmm13, zmm13, [srcreg+r9+d2+d1];; MemR4
	vmulpd	zmm5, zmm11, zmm13		;; A4 = R4 * MemR4					; 93-96		n 97
	vmulpd	zmm13, zmm7, zmm13		;; B4 = I4 * MemR4					; 93-96		n 97

	vmovapd zmm30, [srcreg+rbp+64]		;; MemI1
	vaddpd	zmm30, zmm30, [srcreg+r9+64]	;; MemI1
	zfnmaddpd zmm20, zmm9, zmm30, zmm20	;; A1 - I1 * MemI1 (R1)					; 94-97		n 110
	zfmaddpd zmm18, zmm16, zmm30, zmm18	;; B1 + R1 * MemI1 (I1)					; 94-97		n 106

	vmovapd zmm30, [srcreg+rbp+d1+64]	;; MemI2
	vaddpd	zmm30, zmm30, [srcreg+r9+d1+64];; MemI2
	zfnmaddpd zmm6, zmm8, zmm30, zmm6	;; A2 - I2 * MemI2 (R2)					; 95-98		n 110
	zfmaddpd zmm21, zmm19, zmm30, zmm21	;; B2 + R2 * MemI2 (I2)					; 95-98		n 106

	vmovapd zmm30, [srcreg+rbp+d2+64]	;; MemI3
	vaddpd	zmm30, zmm30, [srcreg+r9+d2+64];; MemI3
	zfnmaddpd zmm17, zmm0, zmm30, zmm17	;; A3 - I3 * MemI3 (R3)					; 96-99		n 113
	zfmaddpd zmm1, zmm14, zmm30, zmm1	;; B3 + R3 * MemI3 (I3)					; 96-99		n 108

	vmovapd zmm30, [srcreg+rbp+d2+d1+64]	;; MemI4
	vaddpd	zmm30, zmm30, [srcreg+r9+d2+d1+64];; MemI4
	zfnmaddpd zmm5, zmm7, zmm30, zmm5	;; A4 - I4 * MemI4 (R4)					; 97-100	n 113
	zfmaddpd zmm13, zmm11, zmm30, zmm13	;; B4 + R4 * MemI4 (I4)					; 97-100	n 108

	vmovapd	zmm22, [srcreg+d4][rbx]		;; R5
	vmovapd	zmm23, [srcreg+d4+64][rbx]	;; I5
	vmovapd	zmm19, [srcreg+rbp+d4]		;; MemR5
	vaddpd	zmm19, zmm19, [srcreg+r9+d4]	;; MemR5
	vmulpd	zmm8, zmm22, zmm19		;; A5 = R5 * MemR5					; 98-101	n 102
	vmulpd	zmm19, zmm23, zmm19		;; B5 = I5 * MemR5					; 98-101	n 102

	vmovapd	zmm12, [srcreg+d4+d1][rbx]	;; R6
	vmovapd	zmm4, [srcreg+d4+d1+64][rbx]	;; I6
	vmovapd	zmm14, [srcreg+rbp+d4+d1]	;; MemR6
	vaddpd	zmm14, zmm14, [srcreg+r9+d4+d1];; MemR6
	vmulpd	zmm0, zmm12, zmm14		;; A6 = R6 * MemR6					; 99-102	n 103
	vmulpd	zmm14, zmm4, zmm14		;; B6 = I6 * MemR6					; 99-102	n 103

	vmovapd	zmm30, [srcreg+d4+d2][rbx]	;; R7
	vmovapd	zmm2, [srcreg+d4+d2+64][rbx]	;; I7
	vmovapd	zmm11, [srcreg+rbp+d4+d2]	;; MemR7
	vaddpd	zmm11, zmm11, [srcreg+r9+d4+d2];; MemR7
	vmulpd	zmm7, zmm30, zmm11		;; A7 = R7 * MemR7					; 100-103	n 104
	vmulpd	zmm11, zmm2, zmm11		;; B7 = I7 * MemR7					; 100-103	n 104

	vmovapd	zmm3, [srcreg+d4+d2+d1][rbx]	;; R8
	vmovapd	zmm10, [srcreg+d4+d2+d1+64][rbx];; I8
	vmovapd	zmm16, [srcreg+rbp+d4+d2+d1]	;; MemR8
	vaddpd	zmm16, zmm16, [srcreg+r9+d4+d2+d1];; MemR8
	vmulpd	zmm9, zmm3, zmm16		;; A8 = R8 * MemR8					; 101-104	n 105
	vmulpd	zmm16, zmm10, zmm16		;; B8 = I8 * MemR8					; 101-104	n 105

	vmovapd zmm15, [srcreg+rbp+d4+64]	;; MemI5
	vaddpd	zmm15, zmm15, [srcreg+r9+d4+64];; MemI5
	zfnmaddpd zmm8, zmm23, zmm15, zmm8	;; A5 - I5 * MemI5 (R5)					; 102-105	n 109
	zfmaddpd zmm19, zmm22, zmm15, zmm19	;; B5 + R5 * MemI5 (I5)					; 102-105	n 107

	vmovapd zmm15, [srcreg+rbp+d4+d1+64]	;; MemI6
	vaddpd	zmm15, zmm15, [srcreg+r9+d4+d1+64];; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0	;; A6 - I6 * MemI6 (R6)					; 103-106	n 109
	zfmaddpd zmm14, zmm12, zmm15, zmm14	;; B6 + R6 * MemI6 (I6)					; 103-106	n 107

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]	;; MemI7
	vaddpd	zmm15, zmm15, [srcreg+r9+d4+d2+64];; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7	;; A7 - I7 * MemI7 (R7)					; 104-107	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11	;; B7 + R7 * MemI7 (I7)					; 104-107	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]	;; MemI8
	vaddpd	zmm15, zmm15, [srcreg+r9+d4+d2+d1+64];; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9	;; A8 - I8 * MemI8 (R8)					; 105-108	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16	;; B8 + R8 * MemI8 (I8)					; 105-108	n 112

	jmp	done

submul:
	vmovapd	zmm16, [srcreg][rbx]		;; R1
	vmovapd	zmm9, [srcreg+64][rbx]		;; I1
	vmovapd	zmm18, [srcreg+rbp]		;; MemR1
	vsubpd	zmm18, zmm18, [srcreg+r9]	;; MemR1
	vmulpd	zmm20, zmm16, zmm18		;; A1 = R1 * MemR1					; 90-93		n 94
	vmulpd	zmm18, zmm9, zmm18		;; B1 = I1 * MemR1					; 90-93		n 94

	vmovapd	zmm19, [srcreg+d1][rbx]		;; R2
	vmovapd	zmm8, [srcreg+d1+64][rbx]	;; I2
	vmovapd	zmm21, [srcreg+rbp+d1]		;; MemR2
	vsubpd	zmm21, zmm21, [srcreg+r9+d1]	;; MemR2
	vmulpd	zmm6, zmm19, zmm21		;; A2 = R2 * MemR2					; 91-94		n 95
	vmulpd	zmm21, zmm8, zmm21		;; B2 = I2 * MemR2					; 91-94		n 95

	vmovapd	zmm14, [srcreg+d2][rbx]		;; R3
	vmovapd	zmm0, [srcreg+d2+64][rbx]	;; I3
	vmovapd	zmm1, [srcreg+rbp+d2]		;; MemR3
	vsubpd	zmm1, zmm1, [srcreg+r9+d2]	;; MemR3
	vmulpd	zmm17, zmm14, zmm1		;; A3 = R3 * MemR3					; 92-95		n 96
	vmulpd	zmm1, zmm0, zmm1		;; B3 = I3 * MemR3					; 92-95		n 96

	vmovapd	zmm11, [srcreg+d2+d1][rbx]	;; R4
	vmovapd	zmm7, [srcreg+d2+d1+64][rbx]	;; I4
	vmovapd	zmm13, [srcreg+rbp+d2+d1]	;; MemR4
	vsubpd	zmm13, zmm13, [srcreg+r9+d2+d1];; MemR4
	vmulpd	zmm5, zmm11, zmm13		;; A4 = R4 * MemR4					; 93-96		n 97
	vmulpd	zmm13, zmm7, zmm13		;; B4 = I4 * MemR4					; 93-96		n 97

	vmovapd zmm30, [srcreg+rbp+64]		;; MemI1
	vsubpd	zmm30, zmm30, [srcreg+r9+64]	;; MemI1
	zfnmaddpd zmm20, zmm9, zmm30, zmm20	;; A1 - I1 * MemI1 (R1)					; 94-97		n 110
	zfmaddpd zmm18, zmm16, zmm30, zmm18	;; B1 + R1 * MemI1 (I1)					; 94-97		n 106

	vmovapd zmm30, [srcreg+rbp+d1+64]	;; MemI2
	vsubpd	zmm30, zmm30, [srcreg+r9+d1+64];; MemI2
	zfnmaddpd zmm6, zmm8, zmm30, zmm6	;; A2 - I2 * MemI2 (R2)					; 95-98		n 110
	zfmaddpd zmm21, zmm19, zmm30, zmm21	;; B2 + R2 * MemI2 (I2)					; 95-98		n 106

	vmovapd zmm30, [srcreg+rbp+d2+64]	;; MemI3
	vsubpd	zmm30, zmm30, [srcreg+r9+d2+64];; MemI3
	zfnmaddpd zmm17, zmm0, zmm30, zmm17	;; A3 - I3 * MemI3 (R3)					; 96-99		n 113
	zfmaddpd zmm1, zmm14, zmm30, zmm1	;; B3 + R3 * MemI3 (I3)					; 96-99		n 108

	vmovapd zmm30, [srcreg+rbp+d2+d1+64]	;; MemI4
	vsubpd	zmm30, zmm30, [srcreg+r9+d2+d1+64];; MemI4
	zfnmaddpd zmm5, zmm7, zmm30, zmm5	;; A4 - I4 * MemI4 (R4)					; 97-100	n 113
	zfmaddpd zmm13, zmm11, zmm30, zmm13	;; B4 + R4 * MemI4 (I4)					; 97-100	n 108

	vmovapd	zmm22, [srcreg+d4][rbx]		;; R5
	vmovapd	zmm23, [srcreg+d4+64][rbx]	;; I5
	vmovapd	zmm19, [srcreg+rbp+d4]		;; MemR5
	vsubpd	zmm19, zmm19, [srcreg+r9+d4]	;; MemR5
	vmulpd	zmm8, zmm22, zmm19		;; A5 = R5 * MemR5					; 98-101	n 102
	vmulpd	zmm19, zmm23, zmm19		;; B5 = I5 * MemR5					; 98-101	n 102

	vmovapd	zmm12, [srcreg+d4+d1][rbx]	;; R6
	vmovapd	zmm4, [srcreg+d4+d1+64][rbx]	;; I6
	vmovapd	zmm14, [srcreg+rbp+d4+d1]	;; MemR6
	vsubpd	zmm14, zmm14, [srcreg+r9+d4+d1];; MemR6
	vmulpd	zmm0, zmm12, zmm14		;; A6 = R6 * MemR6					; 99-102	n 103
	vmulpd	zmm14, zmm4, zmm14		;; B6 = I6 * MemR6					; 99-102	n 103

	vmovapd	zmm30, [srcreg+d4+d2][rbx]	;; R7
	vmovapd	zmm2, [srcreg+d4+d2+64][rbx]	;; I7
	vmovapd	zmm11, [srcreg+rbp+d4+d2]	;; MemR7
	vsubpd	zmm11, zmm11, [srcreg+r9+d4+d2];; MemR7
	vmulpd	zmm7, zmm30, zmm11		;; A7 = R7 * MemR7					; 100-103	n 104
	vmulpd	zmm11, zmm2, zmm11		;; B7 = I7 * MemR7					; 100-103	n 104

	vmovapd	zmm3, [srcreg+d4+d2+d1][rbx]	;; R8
	vmovapd	zmm10, [srcreg+d4+d2+d1+64][rbx];; I8
	vmovapd	zmm16, [srcreg+rbp+d4+d2+d1]	;; MemR8
	vsubpd	zmm16, zmm16, [srcreg+r9+d4+d2+d1];; MemR8
	vmulpd	zmm9, zmm3, zmm16		;; A8 = R8 * MemR8					; 101-104	n 105
	vmulpd	zmm16, zmm10, zmm16		;; B8 = I8 * MemR8					; 101-104	n 105

	vmovapd zmm15, [srcreg+rbp+d4+64]	;; MemI5
	vsubpd	zmm15, zmm15, [srcreg+r9+d4+64];; MemI5
	zfnmaddpd zmm8, zmm23, zmm15, zmm8	;; A5 - I5 * MemI5 (R5)					; 102-105	n 109
	zfmaddpd zmm19, zmm22, zmm15, zmm19	;; B5 + R5 * MemI5 (I5)					; 102-105	n 107

	vmovapd zmm15, [srcreg+rbp+d4+d1+64]	;; MemI6
	vsubpd	zmm15, zmm15, [srcreg+r9+d4+d1+64];; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0	;; A6 - I6 * MemI6 (R6)					; 103-106	n 109
	zfmaddpd zmm14, zmm12, zmm15, zmm14	;; B6 + R6 * MemI6 (I6)					; 103-106	n 107

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]	;; MemI7
	vsubpd	zmm15, zmm15, [srcreg+r9+d4+d2+64];; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7	;; A7 - I7 * MemI7 (R7)					; 104-107	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11	;; B7 + R7 * MemI7 (I7)					; 104-107	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]	;; MemI8
	vsubpd	zmm15, zmm15, [srcreg+r9+d4+d2+d1+64];; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9	;; A8 - I8 * MemI8 (R8)					; 105-108	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16	;; B8 + R8 * MemI8 (I8)					; 105-108	n 112

	jmp	done

fma:
	vmovapd	zmm16, [srcreg][rbx]		;; R1
	vmovapd	zmm9, [srcreg+64][rbx]		;; I1
	vmovapd	zmm18, [srcreg+rbp]		;; MemR1
	vmulpd	zmm20, zmm16, zmm18		;; A1 = R1 * MemR1					; 90-93		n 94
	vmulpd	zmm18, zmm9, zmm18		;; B1 = I1 * MemR1					; 90-93		n 94

	vmovapd	zmm19, [srcreg+d1][rbx]		;; R2
	vmovapd	zmm8, [srcreg+d1+64][rbx]	;; I2
	vmovapd	zmm21, [srcreg+rbp+d1]		;; MemR2
	vmulpd	zmm6, zmm19, zmm21		;; A2 = R2 * MemR2					; 91-94		n 95
	vmulpd	zmm21, zmm8, zmm21		;; B2 = I2 * MemR2					; 91-94		n 95

	vmovapd	zmm14, [srcreg+d2][rbx]		;; R3
	vmovapd	zmm0, [srcreg+d2+64][rbx]	;; I3
	vmovapd	zmm1, [srcreg+rbp+d2]		;; MemR3
	vmulpd	zmm17, zmm14, zmm1		;; A3 = R3 * MemR3					; 92-95		n 96
	vmulpd	zmm1, zmm0, zmm1		;; B3 = I3 * MemR3					; 92-95		n 96

	vmovapd	zmm11, [srcreg+d2+d1][rbx]	;; R4
	vmovapd	zmm7, [srcreg+d2+d1+64][rbx]	;; I4
	vmovapd	zmm13, [srcreg+rbp+d2+d1]	;; MemR4
	vmulpd	zmm5, zmm11, zmm13		;; A4 = R4 * MemR4					; 93-96		n 97
	vmulpd	zmm13, zmm7, zmm13		;; B4 = I4 * MemR4					; 93-96		n 97

	vmovapd zmm30, [srcreg+rbp+64]		;; MemI1
	zfnmaddpd zmm20, zmm9, zmm30, zmm20	;; A1 - I1 * MemI1 (R1)					; 94-97		n 110
	zfmaddpd zmm18, zmm16, zmm30, zmm18	;; B1 + R1 * MemI1 (I1)					; 94-97		n 106

	vmovapd zmm30, [srcreg+rbp+d1+64]	;; MemI2
	zfnmaddpd zmm6, zmm8, zmm30, zmm6	;; A2 - I2 * MemI2 (R2)					; 95-98		n 110
	zfmaddpd zmm21, zmm19, zmm30, zmm21	;; B2 + R2 * MemI2 (I2)					; 95-98		n 106

	vmovapd zmm30, [srcreg+rbp+d2+64]	;; MemI3
	zfnmaddpd zmm17, zmm0, zmm30, zmm17	;; A3 - I3 * MemI3 (R3)					; 96-99		n 113
	zfmaddpd zmm1, zmm14, zmm30, zmm1	;; B3 + R3 * MemI3 (I3)					; 96-99		n 108

	vmovapd zmm30, [srcreg+rbp+d2+d1+64]	;; MemI4
	zfnmaddpd zmm5, zmm7, zmm30, zmm5	;; A4 - I4 * MemI4 (R4)					; 97-100	n 113
	zfmaddpd zmm13, zmm11, zmm30, zmm13	;; B4 + R4 * MemI4 (I4)					; 97-100	n 108

	vmovapd	zmm22, [srcreg+d4][rbx]		;; R5
	vmovapd	zmm23, [srcreg+d4+64][rbx]	;; I5
	vmovapd	zmm19, [srcreg+rbp+d4]		;; MemR5
	vmulpd	zmm8, zmm22, zmm19		;; A5 = R5 * MemR5					; 98-101	n 102
	vmulpd	zmm19, zmm23, zmm19		;; B5 = I5 * MemR5					; 98-101	n 102

	vmovapd	zmm12, [srcreg+d4+d1][rbx]	;; R6
	vmovapd	zmm4, [srcreg+d4+d1+64][rbx]	;; I6
	vmovapd	zmm14, [srcreg+rbp+d4+d1]	;; MemR6
	vmulpd	zmm0, zmm12, zmm14		;; A6 = R6 * MemR6					; 99-102	n 103
	vmulpd	zmm14, zmm4, zmm14		;; B6 = I6 * MemR6					; 99-102	n 103

	vmovapd	zmm30, [srcreg+d4+d2][rbx]	;; R7
	vmovapd	zmm2, [srcreg+d4+d2+64][rbx]	;; I7
	vmovapd	zmm11, [srcreg+rbp+d4+d2]	;; MemR7
	vmulpd	zmm7, zmm30, zmm11		;; A7 = R7 * MemR7					; 100-103	n 104
	vmulpd	zmm11, zmm2, zmm11		;; B7 = I7 * MemR7					; 100-103	n 104

	vmovapd	zmm3, [srcreg+d4+d2+d1][rbx]	;; R8
	vmovapd	zmm10, [srcreg+d4+d2+d1+64][rbx];; I8
	vmovapd	zmm16, [srcreg+rbp+d4+d2+d1]	;; MemR8
	vmulpd	zmm9, zmm3, zmm16		;; A8 = R8 * MemR8					; 101-104	n 105
	vmulpd	zmm16, zmm10, zmm16		;; B8 = I8 * MemR8					; 101-104	n 105

	vmovapd zmm15, [srcreg+rbp+d4+64]	;; MemI5
	zfnmaddpd zmm8, zmm23, zmm15, zmm8	;; A5 - I5 * MemI5 (R5)					; 102-105	n 109
	zfmaddpd zmm19, zmm22, zmm15, zmm19	;; B5 + R5 * MemI5 (I5)					; 102-105	n 107

	vmovapd zmm15, [srcreg+rbp+d4+d1+64]	;; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0	;; A6 - I6 * MemI6 (R6)					; 103-106	n 109
	zfmaddpd zmm14, zmm12, zmm15, zmm14	;; B6 + R6 * MemI6 (I6)					; 103-106	n 107

	vmovapd zmm15, [srcreg+rbp+d4+d2+64]	;; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7	;; A7 - I7 * MemI7 (R7)					; 104-107	n 111
	zfmaddpd zmm11, zmm30, zmm15, zmm11	;; B7 + R7 * MemI7 (I7)					; 104-107	n 112

	vmovapd zmm15, [srcreg+rbp+d4+d2+d1+64]	;; MemI8
	zfnmaddpd zmm9, zmm10, zmm15, zmm9	;; A8 - I8 * MemI8 (R8)					; 105-108	n 111
	zfmaddpd zmm16, zmm3, zmm15, zmm16	;; B8 + R8 * MemI8 (I8)					; 105-108	n 112

	cmp	r10, 4				;; Case off opcode
	mov	r10, SRC3ARG			;; Load distance to FFT(1)
	;jb	muladd				;; 3=muladd, fall through
	je	mulsub				;; 4=mulsub

muladd:	cmp	r10, 1				;; Check if we need to multiply add-in by FFT(1)
	jne	muladdhard			;; Yes, do it the hard way

	vaddpd	zmm20, zmm20, [srcreg+r9]		;; R1 = R1 + MemR1
	vaddpd	zmm18, zmm18, [srcreg+r9+64]		;; I1 = I1 + MemI1
	vaddpd	zmm6, zmm6, [srcreg+r9+d1]		;; R2 = R2 + MemR2
	vaddpd	zmm21, zmm21, [srcreg+r9+d1+64]		;; I2 = I2 + MemI2
	vaddpd	zmm17, zmm17, [srcreg+r9+d2]		;; R3 = R3 + MemR3
	vaddpd	zmm1, zmm1, [srcreg+r9+d2+64]		;; I3 = I3 + MemI3
	vaddpd	zmm5, zmm5, [srcreg+r9+d2+d1]		;; R4 = R4 + MemR4
	vaddpd	zmm13, zmm13, [srcreg+r9+d2+d1+64]	;; I4 = I4 + MemI4
	vaddpd	zmm8, zmm8, [srcreg+r9+d4]		;; R5 = R5 + MemR5
	vaddpd	zmm19, zmm19, [srcreg+r9+d4+64]		;; I5 = I5 + MemI5
	vaddpd	zmm0, zmm0, [srcreg+r9+d4+d1]		;; R6 = R6 + MemR6
	vaddpd	zmm14, zmm14, [srcreg+r9+d4+d1+64]	;; I6 = I6 + MemI6
	vaddpd	zmm7, zmm7, [srcreg+r9+d4+d2]		;; R7 = R7 + MemR7
	vaddpd	zmm11, zmm11, [srcreg+r9+d4+d2+64]	;; I7 = I7 + MemI7
	vaddpd	zmm9, zmm9, [srcreg+r9+d4+d2+d1]	;; R8 = R8 + MemR8
	vaddpd	zmm16, zmm16, [srcreg+r9+d4+d2+d1+64]	;; I8 = I8 + MemI8

	jmp	done

muladdhard:
	vmovapd	zmm4, [srcreg+r9]			;; MemR1
	vmovapd	zmm12, [srcreg+r10]			;; MemR1#2
	vmovapd	zmm15, [srcreg+r10+64]			;; MemI1#2
	zfmaddpd zmm20, zmm4, zmm12, zmm20		;; R1 = R1 + MemR1*MemR1#2
	zfmaddpd zmm18, zmm4, zmm15, zmm18		;; I1 = I1 + MemR1*MemI1#2
	vmovapd	zmm4, [srcreg+r9+64]			;; MemI1
	zfnmaddpd zmm20, zmm4, zmm15, zmm20		;; R1 = R1 - MemI1*MemI1#2
	zfmaddpd zmm18, zmm4, zmm12, zmm18		;; I1 = I1 + MemI1*MemR1#2

	vmovapd	zmm4, [srcreg+r9+d1]			;; MemR2
	vmovapd	zmm12, [srcreg+r10+d1]			;; MemR2#2
	vmovapd	zmm15, [srcreg+r10+d1+64]		;; MemI2#2
	zfmaddpd zmm6, zmm4, zmm12, zmm6		;; R2 = R2 + MemR2*MemR2#2
	zfmaddpd zmm21, zmm4, zmm15, zmm21		;; I2 = I2 + MemR2*MemI2#2
	vmovapd	zmm4, [srcreg+r9+d1+64]			;; MemI2
	zfnmaddpd zmm6, zmm4, zmm15, zmm6		;; R2 = R2 - MemI2*MemI2#2
	zfmaddpd zmm21, zmm4, zmm12, zmm21		;; I2 = I2 + MemI2*MemR2#2

	vmovapd	zmm4, [srcreg+r9+d2]			;; MemR3
	vmovapd	zmm12, [srcreg+r10+d2]			;; MemR3#2
	vmovapd	zmm15, [srcreg+r10+d2+64]		;; MemI3#2
	zfmaddpd zmm17, zmm4, zmm12, zmm17		;; R3 = R3 + MemR3*MemR3#2
	zfmaddpd zmm1, zmm4, zmm15, zmm1		;; I3 = I3 + MemR3*MemI3#2
	vmovapd	zmm4, [srcreg+r9+d2+64]			;; MemI3
	zfnmaddpd zmm17, zmm4, zmm15, zmm17		;; R3 = R3 - MemI3*MemI3#2
	zfmaddpd zmm1, zmm4, zmm12, zmm1		;; I3 = I3 + MemI3*MemR3#2

	vmovapd	zmm4, [srcreg+r9+d2+d1]			;; MemR4
	vmovapd	zmm12, [srcreg+r10+d2+d1]		;; MemR4#2
	vmovapd	zmm15, [srcreg+r10+d2+d1+64]		;; MemI4#2
	zfmaddpd zmm5, zmm4, zmm12, zmm5		;; R4 = R4 + MemR4*MemR4#2
	zfmaddpd zmm13, zmm4, zmm15, zmm13		;; I4 = I4 + MemR4*MemI4#2
	vmovapd	zmm4, [srcreg+r9+d2+d1+64]		;; MemI4
	zfnmaddpd zmm5, zmm4, zmm15, zmm5		;; R4 = R4 - MemI4*MemI4#2
	zfmaddpd zmm13, zmm4, zmm12, zmm13		;; I4 = I4 + MemI4*MemR4#2

	vmovapd	zmm4, [srcreg+r9+d4]			;; MemR5
	vmovapd	zmm12, [srcreg+r10+d4]			;; MemR5#2
	vmovapd	zmm15, [srcreg+r10+d4+64]		;; MemI5#2
	zfmaddpd zmm8, zmm4, zmm12, zmm8		;; R5 = R5 + MemR5*MemR5#2
	zfmaddpd zmm19, zmm4, zmm15, zmm19		;; I5 = I5 + MemR5*MemI5#2
	vmovapd	zmm4, [srcreg+r9+d4+64]			;; MemI5
	zfnmaddpd zmm8, zmm4, zmm15, zmm8		;; R5 = R5 - MemI5*MemI5#2
	zfmaddpd zmm19, zmm4, zmm12, zmm19		;; I5 = I5 + MemI5*MemR5#2

	vmovapd	zmm4, [srcreg+r9+d4+d1]			;; MemR6
	vmovapd	zmm12, [srcreg+r10+d4+d1]		;; MemR6#2
	vmovapd	zmm15, [srcreg+r10+d4+d1+64]		;; MemI6#2
	zfmaddpd zmm0, zmm4, zmm12, zmm0		;; R6 = R6 + MemR6*MemR6#2
	zfmaddpd zmm14, zmm4, zmm15, zmm14		;; I6 = I6 + MemR6*MemI6#2
	vmovapd	zmm4, [srcreg+r9+d4+d1+64]		;; MemI6
	zfnmaddpd zmm0, zmm4, zmm15, zmm0		;; R6 = R6 - MemI6*MemI6#2
	zfmaddpd zmm14, zmm4, zmm12, zmm14		;; I6 = I6 + MemI6*MemR6#2

	vmovapd	zmm2, [srcreg+r9+d4+d2]			;; MemR7
	vmovapd	zmm30, [srcreg+r10+d4+d2]		;; MemR7#2
	vmovapd	zmm15, [srcreg+r10+d4+d2+64]		;; MemI7#2
	zfmaddpd zmm7, zmm2, zmm30, zmm7		;; R7 = R7 + MemR7*MemR7#2
	zfmaddpd zmm11, zmm2, zmm15, zmm11		;; I7 = I7 + MemR7*MemI7#2
	vmovapd	zmm2, [srcreg+r9+d4+d2+64]		;; MemI7
	zfnmaddpd zmm7, zmm2, zmm15, zmm7		;; R7 = R7 - MemI7*MemI7#2
	zfmaddpd zmm11, zmm2, zmm30, zmm11		;; I7 = I7 + MemI7*MemR7#2

	vmovapd	zmm2, [srcreg+r9+d4+d2+d1]		;; MemR8
	vmovapd	zmm30, [srcreg+r10+d4+d2+d1]		;; MemR8#2
	vmovapd	zmm15, [srcreg+r10+d4+d2+d1+64]		;; MemI8#2
	zfmaddpd zmm9, zmm2, zmm30, zmm9		;; R8 = R8 + MemR8*MemR8#2
	zfmaddpd zmm16, zmm2, zmm15, zmm16		;; I8 = I8 + MemR8*MemI8#2
	vmovapd	zmm2, [srcreg+r9+d4+d2+d1+64]		;; MemI8
	zfnmaddpd zmm9, zmm2, zmm15, zmm9		;; R8 = R8 - MemI8*MemI8#2
	zfmaddpd zmm16, zmm2, zmm30, zmm16		;; I8 = I8 + MemI8*MemR8#2

	jmp	done

mulsub:	cmp	r10, 1				;; Check if we need to multiply add-in by FFT(1)
	jne	mulsubhard			;; Yes, do it the hard way

	vsubpd	zmm20, zmm20, [srcreg+r9]		;; R1 = R1 - MemR1
	vsubpd	zmm18, zmm18, [srcreg+r9+64]		;; I1 = I1 - MemI1
	vsubpd	zmm6, zmm6, [srcreg+r9+d1]		;; R2 = R2 - MemR2
	vsubpd	zmm21, zmm21, [srcreg+r9+d1+64]		;; I2 = I2 - MemI2
	vsubpd	zmm17, zmm17, [srcreg+r9+d2]		;; R3 = R3 - MemR3
	vsubpd	zmm1, zmm1, [srcreg+r9+d2+64]		;; I3 = I3 - MemI3
	vsubpd	zmm5, zmm5, [srcreg+r9+d2+d1]		;; R4 = R4 - MemR4
	vsubpd	zmm13, zmm13, [srcreg+r9+d2+d1+64]	;; I4 = I4 - MemI4
	vsubpd	zmm8, zmm8, [srcreg+r9+d4]		;; R5 = R5 - MemR5
	vsubpd	zmm19, zmm19, [srcreg+r9+d4+64]		;; I5 = I5 - MemI5
	vsubpd	zmm0, zmm0, [srcreg+r9+d4+d1]		;; R6 = R6 - MemR6
	vsubpd	zmm14, zmm14, [srcreg+r9+d4+d1+64]	;; I6 = I6 - MemI6
	vsubpd	zmm7, zmm7, [srcreg+r9+d4+d2]		;; R7 = R7 - MemR7
	vsubpd	zmm11, zmm11, [srcreg+r9+d4+d2+64]	;; I7 = I7 - MemI7
	vsubpd	zmm9, zmm9, [srcreg+r9+d4+d2+d1]	;; R8 = R8 - MemR8
	vsubpd	zmm16, zmm16, [srcreg+r9+d4+d2+d1+64]	;; I8 = I8 - MemI8

	jmp	done

mulsubhard:
	vmovapd	zmm4, [srcreg+r9]			;; MemR1
	vmovapd	zmm12, [srcreg+r10]			;; MemR1#2
	vmovapd	zmm15, [srcreg+r10+64]			;; MemI1#2
	zfnmaddpd zmm20, zmm4, zmm12, zmm20		;; R1 = R1 - MemR1*MemR1#2
	zfnmaddpd zmm18, zmm4, zmm15, zmm18		;; I1 = I1 - MemR1*MemI1#2
	vmovapd	zmm4, [srcreg+r9+64]			;; MemI1
	zfmaddpd zmm20, zmm4, zmm15, zmm20		;; R1 = R1 + MemI1*MemI1#2
	zfnmaddpd zmm18, zmm4, zmm12, zmm18		;; I1 = I1 - MemI1*MemR1#2

	vmovapd	zmm4, [srcreg+r9+d1]			;; MemR2
	vmovapd	zmm12, [srcreg+r10+d1]			;; MemR2#2
	vmovapd	zmm15, [srcreg+r10+d1+64]		;; MemI2#2
	zfnmaddpd zmm6, zmm4, zmm12, zmm6		;; R2 = R2 - MemR2*MemR2#2
	zfnmaddpd zmm21, zmm4, zmm15, zmm21		;; I2 = I2 - MemR2*MemI2#2
	vmovapd	zmm4, [srcreg+r9+d1+64]			;; MemI2
	zfmaddpd zmm6, zmm4, zmm15, zmm6		;; R2 = R2 + MemI2*MemI2#2
	zfnmaddpd zmm21, zmm4, zmm12, zmm21		;; I2 = I2 - MemI2*MemR2#2

	vmovapd	zmm4, [srcreg+r9+d2]			;; MemR3
	vmovapd	zmm12, [srcreg+r10+d2]			;; MemR3#2
	vmovapd	zmm15, [srcreg+r10+d2+64]		;; MemI3#2
	zfnmaddpd zmm17, zmm4, zmm12, zmm17		;; R3 = R3 - MemR3*MemR3#2
	zfnmaddpd zmm1, zmm4, zmm15, zmm1		;; I3 = I3 - MemR3*MemI3#2
	vmovapd	zmm4, [srcreg+r9+d2+64]			;; MemI3
	zfmaddpd zmm17, zmm4, zmm15, zmm17		;; R3 = R3 + MemI3*MemI3#2
	zfnmaddpd zmm1, zmm4, zmm12, zmm1		;; I3 = I3 - MemI3*MemR3#2

	vmovapd	zmm4, [srcreg+r9+d2+d1]			;; MemR4
	vmovapd	zmm12, [srcreg+r10+d2+d1]		;; MemR4#2
	vmovapd	zmm15, [srcreg+r10+d2+d1+64]		;; MemI4#2
	zfnmaddpd zmm5, zmm4, zmm12, zmm5		;; R4 = R4 - MemR4*MemR4#2
	zfnmaddpd zmm13, zmm4, zmm15, zmm13		;; I4 = I4 - MemR4*MemI4#2
	vmovapd	zmm4, [srcreg+r9+d2+d1+64]		;; MemI4
	zfmaddpd zmm5, zmm4, zmm15, zmm5		;; R4 = R4 + MemI4*MemI4#2
	zfnmaddpd zmm13, zmm4, zmm12, zmm13		;; I4 = I4 - MemI4*MemR4#2

	vmovapd	zmm4, [srcreg+r9+d4]			;; MemR5
	vmovapd	zmm12, [srcreg+r10+d4]			;; MemR5#2
	vmovapd	zmm15, [srcreg+r10+d4+64]		;; MemI5#2
	zfnmaddpd zmm8, zmm4, zmm12, zmm8		;; R5 = R5 - MemR5*MemR5#2
	zfnmaddpd zmm19, zmm4, zmm15, zmm19		;; I5 = I5 - MemR5*MemI5#2
	vmovapd	zmm4, [srcreg+r9+d4+64]			;; MemI5
	zfmaddpd zmm8, zmm4, zmm15, zmm8		;; R5 = R5 + MemI5*MemI5#2
	zfnmaddpd zmm19, zmm4, zmm12, zmm19		;; I5 = I5 - MemI5*MemR5#2

	vmovapd	zmm4, [srcreg+r9+d4+d1]			;; MemR6
	vmovapd	zmm12, [srcreg+r10+d4+d1]		;; MemR6#2
	vmovapd	zmm15, [srcreg+r10+d4+d1+64]		;; MemI6#2
	zfnmaddpd zmm0, zmm4, zmm12, zmm0		;; R6 = R6 - MemR6*MemR6#2
	zfnmaddpd zmm14, zmm4, zmm15, zmm14		;; I6 = I6 - MemR6*MemI6#2
	vmovapd	zmm4, [srcreg+r9+d4+d1+64]		;; MemI6
	zfmaddpd zmm0, zmm4, zmm15, zmm0		;; R6 = R6 + MemI6*MemI6#2
	zfnmaddpd zmm14, zmm4, zmm12, zmm14		;; I6 = I6 - MemI6*MemR6#2

	vmovapd	zmm2, [srcreg+r9+d4+d2]			;; MemR7
	vmovapd	zmm30, [srcreg+r10+d4+d2]		;; MemR7#2
	vmovapd	zmm15, [srcreg+r10+d4+d2+64]		;; MemI7#2
	zfnmaddpd zmm7, zmm2, zmm30, zmm7		;; R7 = R7 - MemR7*MemR7#2
	zfnmaddpd zmm11, zmm2, zmm15, zmm11		;; I7 = I7 - MemR7*MemI7#2
	vmovapd	zmm2, [srcreg+r9+d4+d2+64]		;; MemI7
	zfmaddpd zmm7, zmm2, zmm15, zmm7		;; R7 = R7 + MemI7*MemI7#2
	zfnmaddpd zmm11, zmm2, zmm30, zmm11		;; I7 = I7 - MemI7*MemR7#2

	vmovapd	zmm2, [srcreg+r9+d4+d2+d1]		;; MemR8
	vmovapd	zmm30, [srcreg+r10+d4+d2+d1]		;; MemR8#2
	vmovapd	zmm15, [srcreg+r10+d4+d2+d1+64]		;; MemI8#2
	zfnmaddpd zmm9, zmm2, zmm30, zmm9		;; R8 = R8 - MemR8*MemR8#2
	zfnmaddpd zmm16, zmm2, zmm15, zmm16		;; I8 = I8 - MemR8*MemI8#2
	vmovapd	zmm2, [srcreg+r9+d4+d2+d1+64]		;; MemI8
	zfmaddpd zmm9, zmm2, zmm15, zmm9		;; R8 = R8 + MemI8*MemI8#2
	zfnmaddpd zmm16, zmm2, zmm30, zmm16		;; I8 = I8 - MemI8*MemR8#2

done:
	ENDM


;;************************************* hundred-twenty-eight-reals macros *************************************


zr64_hundredtwentyeight_real_fft_final_preload MACRO
	zr64_128r_fft_cmn_preload
	ENDM
zr64_hundredtwentyeight_real_fft_final MACRO srcreg,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	zr64_128r_fft_cmn srcreg,0,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM
zr64f_hundredtwentyeight_real_fft_final_preload MACRO
	zr64_128r_fft_cmn_preload
	ENDM
zr64f_hundredtwentyeight_real_fft_final MACRO srcreg,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	zr64_128r_fft_cmn srcreg,rbx,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

zr64_128r_fft_cmn_preload MACRO
	;; preload would be silly for a macro that is executed only once
	ENDM
zr64_128r_fft_cmn MACRO srcreg,srcoff,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	vmovapd	zmm1, [srcreg+srcoff+d1]	;; r2+r10
	vmovapd	zmm3, [srcreg+srcoff+d4+d2+d1]	;; r8+r16
	vaddpd	zmm0, zmm1, zmm3		;; r2++ = (r2+r10)+(r8+r16)					; 1-4		n 6
	vsubpd	zmm1, zmm1, zmm3		;; r2+- = (r2+r10)-(r8+r16)					; 1-4		n 7

	vmovapd	zmm5, [srcreg+srcoff+d2+d1]	;; r4+r12
	vmovapd	zmm7, [srcreg+srcoff+d4+d1]	;; r6+r14
	vaddpd	zmm3, zmm5, zmm7		;; r4++ = (r4+r12)+(r6+r14)					; 2-5		n 6
	vsubpd	zmm5, zmm5, zmm7		;; r4+- = (r4+r12)-(r6+r14)					; 2-5		n 7

	vmovapd	zmm13, [srcreg+srcoff]		;; r1+r9
	vmovapd	zmm15, [srcreg+srcoff+d4]	;; r5+r13
	vaddpd	zmm7, zmm13, zmm15		;; r1++ = (r1+r9)+(r5+r13)					; 3-6		n 8
	vsubpd	zmm13, zmm13, zmm15		;; r1+- = (r1+r9)-(r5+r13)					; 3-6		n 11

	vmovapd	zmm9, [srcreg+srcoff+d2]	;; r3+r11
	vmovapd	zmm11, [srcreg+srcoff+d4+d2]	;; r7+r15
	vaddpd	zmm15, zmm9, zmm11		;; r3++ = (r3+r11)+(r7+r15)					; 4-7		n 8
	vsubpd	zmm9, zmm9, zmm11		;; r3+- = (r3+r11)-(r7+r15)					; 4-7		n 12

	vmovapd	zmm10, [srcreg+srcoff+d2+64]	;; r3-r11
	vmovapd	zmm12, [srcreg+srcoff+d4+d2+64]	;; r7-r15
	vaddpd	zmm11, zmm10, zmm12		;; r3-+ = (r3-r11)+(r7-r15)					; 5-8		n 16
	vsubpd	zmm10, zmm10, zmm12		;; r3-- = (r3-r11)-(r7-r15)					; 5-8		n 14

	vmovapd	zmm2, [srcreg+srcoff+d1+64]	;; r2-r10
	vaddpd	zmm12, zmm0, zmm3		;; r2+++ = (r2++) + (r4++)					; 6-9		n 13
	vsubpd	zmm0, zmm0, zmm3		;; r2++- = (r2++) - (r4++)					; 6-9		n 11

	vmovapd	zmm4, [srcreg+srcoff+d4+d2+d1+64] ;; r8-r16
	vaddpd	zmm3, zmm1, zmm5		;; r2+-+ = (r2+-) + (r4+-)					; 7-10		n 12
	vsubpd	zmm1, zmm1, zmm5		;; r2+-- = (r2+-) - (r4+-)	(final I5)			; 7-10		n 18

	vmovapd	zmm6, [srcreg+srcoff+d2+d1+64]	;; r4-r12
	vaddpd	zmm5, zmm7, zmm15		;; r1+++ = (r1++) + (r3++)					; 8-11		n 13
	vsubpd	zmm7, zmm7, zmm15		;; r1++- = (r1++) - (r3++)	(final R5)			; 8-11		n 18

	vmovapd	zmm8, [srcreg+srcoff+d4+d1+64]	;; r6-r14
	vaddpd	zmm15, zmm2, zmm4		;; r2-+ = (r2-r10)+(r8-r16)					; 9-12		n 17
	vsubpd	zmm2, zmm2, zmm4		;; r2-- = (r2-r10)-(r8-r16)					; 9-12		n 15

	vbroadcastsd zmm31, ZMM_SQRTHALF
	vaddpd	zmm4, zmm6, zmm8		;; r4-+ = (r4-r12)+(r6-r14)					; 10-13		n 17
	vsubpd	zmm6, zmm6, zmm8		;; r4-- = (r4-r12)-(r6-r14)					; 10-13		n 15

	vmovapd	zmm14, [srcreg+srcoff+64]	;; r1-r9
	zfmaddpd zmm8, zmm0, zmm31, zmm13	;; R3 = (r1+-) + .707(r2++-)					; 11-14		n 19
	zfnmaddpd zmm0, zmm0, zmm31, zmm13	;; R7 = (r1+-) - .707(r2++-)					; 11-14		n 20

	vbroadcastsd zmm30, ZMM_P924_P383
	zfmaddpd zmm13, zmm3, zmm31, zmm9	;; I3 = .707*(r2+-+) + (r3+-)					; 12-15		n 19
	zfmsubpd zmm3, zmm3, zmm31, zmm9	;; I7 = .707*(r2+-+) - (r3+-)					; 12-15		n 20

	vmovapd	zmm16, [srcreg+srcoff+d4+64]	;; r5-r13
	vaddpd	zmm9, zmm5, zmm12		;; R1 = (r1+++) + (r2+++)					; 13-16		n 31
	vsubpd	zmm5, zmm5, zmm12		;; R9 = (r1+++) - (r2+++)					; 13-16		n 33

	vmovapd	zmm28, [screg2+1*128+64]	;; cosine/sine for R5/I5 (w^4 = complex w^2)
	zfmaddpd zmm12, zmm10, zmm31, zmm14	;; r2o = (r1-r9) + .707*(r3--)					; 14-17		n 21
	zfnmaddpd zmm10, zmm10, zmm31, zmm14	;; r4o = (r1-r9) - .707*(r3--)					; 14-17		n 22

	vmovapd	zmm27, [screg2+0*128+64]	;; cosine/sine for R3/I3 (w^2 = complex w^1)
	zfmaddpd zmm14, zmm2, zmm30, zmm6	;; r2e/.383 = .924/.383(r2--) + (r4--)				; 15-18		n 21
	zfnmaddpd zmm6, zmm6, zmm30, zmm2	;; r4e/.383 = (r2--) - .924/.383(r4--)				; 15-18		n 22

	vmovapd	zmm26, [screg2+2*128+64]	;; cosine/sine for R7/I7 (w^6 = complex w^3)
	zfmaddpd zmm2, zmm11, zmm31, zmm16	;; i2o = .707(r3-+) + (r5-r13)					; 16-19		n 23
	zfmsubpd zmm11, zmm11, zmm31, zmm16	;; i4o = .707(r3-+) - (r5-r13)					; 16-19		n 24

	vbroadcastsd zmm29, ZMM_P383
	zfmaddpd zmm16, zmm4, zmm30, zmm15	;; i2e/.383 = (r2-+) + .924/.383(r4-+)				; 17-20		n 23
	zfmsubpd zmm15, zmm15, zmm30, zmm4	;; i4e/.383 = .924/.383(r2-+) - (r4-+)				; 17-20		n 24

	vmovapd	zmm25, [screg2+1*128]		;; sine for R5/I5
	zfmsubpd zmm4, zmm7, zmm28, zmm1	;; A5 = R5 * cosine/sine - I5					; 18-21		n 25
	zfmaddpd zmm1, zmm1, zmm28, zmm7	;; B5 = I5 * cosine/sine + R5					; 18-21		n 25

	vmovapd	zmm24, [screg2+0*128]		;; sine for R3/I3
	zfmsubpd zmm7, zmm8, zmm27, zmm13	;; A3 = R3 * cosine/sine - I3					; 19-22		n 26
	zfmaddpd zmm13, zmm13, zmm27, zmm8	;; B3 = I3 * cosine/sine + R3					; 19-22		n 26

	vmovapd	zmm23, [screg2+2*128]		;; sine for R7/I7
	zfmsubpd zmm8, zmm0, zmm26, zmm3	;; A7 = R7 * cosine/sine - I7					; 20-23		n 27
	zfmaddpd zmm3, zmm3, zmm26, zmm0	;; B7 = I7 * cosine/sine + R7					; 20-23		n 27

	vmovapd	zmm22, [screg1+0*128+64]	;; cosine/sine for R2/I2 (w^1)
	zfmaddpd zmm0, zmm14, zmm29, zmm12	;; R2 = r2o + .383*r2e						; 21-24		n 28
	zfnmaddpd zmm14, zmm14, zmm29, zmm12	;; R8 = r2o - .383*r2e						; 21-24		n 32

	vmovapd	zmm19, [screg1+2*128+64]	;; cosine/sine for R6/I6 (w^5)
	zfmaddpd zmm12, zmm6, zmm29, zmm10	;; R4 = r4o + .383*r4e						; 22-25		n 30
	zfnmaddpd zmm6, zmm6, zmm29, zmm10	;; R6 = r4o - .383*r4e						; 22-25		n 29

	vmovapd	zmm20, [screg1+1*128+64]	;; cosine/sine for R4/I4 (w^3)
	zfmaddpd zmm10, zmm16, zmm29, zmm2	;; I2 = .383*i2e + i2o						; 23-26		n 28
	zfmsubpd zmm16, zmm16, zmm29, zmm2	;; I8 = .383*i2e - i2o						; 23-26		n 32

	vmovapd	zmm21, [screg1+3*128+64]	;; cosine/sine for R8/I8 (w^7)
	zfmaddpd zmm2, zmm15, zmm29, zmm11	;; I4 = .383*i4e + i4o						; 24-27		n 30
	zfmsubpd zmm15, zmm15, zmm29, zmm11	;; I6 = .383*i4e - i4o						; 24-27		n 29

	vmovapd	zmm18, [screg1+0*128]		;; sine for R2/I2
	vmulpd	zmm4, zmm4, zmm25		;; A5 = A5 * sine (final R5)					; 25-28		n 32
	vmulpd	zmm1, zmm1, zmm25		;; B5 = B5 * sine (final I5)					; 25-28		n 34

	vmovapd zmm17, [screg1+2*128]		;; sine for R6/I6
	vmulpd	zmm7, zmm7, zmm24		;; A3 = A3 * sine (final R3)					; 26-29		n 36
	vmulpd	zmm13, zmm13, zmm24		;; B3 = B3 * sine (final I3)					; 26-29		n 38

	vmovapd	zmm28, [screg1+1*128]		;; sine for R4/I4
	vmulpd	zmm8, zmm8, zmm23		;; A7 = A7 * sine (final R7)					; 27-30		n 36
	vmulpd	zmm3, zmm3, zmm23		;; B7 = B7 * sine (final I7)					; 27-30		n 38

	vmovapd	zmm27, [screg1+3*128]		;; sine for R8/I8
	zfmsubpd zmm11, zmm0, zmm22, zmm10	;; A2 = R2 * cosine/sine - I2					; 28-31		n 34
	zfmaddpd zmm10, zmm10, zmm22, zmm0	;; B2 = I2 * cosine/sine + R2					; 28-31		n 35

	vbroadcastsd zmm22, ZMM_ONE
	zfmsubpd zmm0, zmm6, zmm19, zmm15	;; A6 = R6 * cosine/sine - I6					; 29-32		n 36
	zfmaddpd zmm15, zmm15, zmm19, zmm6	;; B6 = I6 * cosine/sine + R6					; 29-32		n 37

	mov	eax, 11111110b			;; We're pretty sure callers of this macro call zloop_init next
	kmovw	k7, eax				;; Set k7 to 11111110b						; 30		n 31
	knotw	k6, k7				;; Set k6 to 00000001b						; 31		n 50
	L1prefetchw srcreg+L1pd, L1pt

	zfmsubpd zmm6, zmm12, zmm20, zmm2	;; A4 = R4 * cosine/sine - I4					; 30-33		n 38
	zfmaddpd zmm2, zmm2, zmm20, zmm12	;; B4 = I4 * cosine/sine + R4					; 31-34		n 39
	L1prefetchw srcreg+64+L1pd, L1pt

	;; Swap the four aparts
	vshuff64x2 zmm19, zmm9, zmm4, 01000100b	;; R5_3 R5_2 R5_1 R5_0 R1_3 R1_2 R1_1 R1_0 (R15L)		; 32-34		n 42
	vshuff64x2 zmm9, zmm9, zmm4, 11101110b	;; R5_7	R5_6 R5_5 R5_4 R1_7 R1_6 R1_5 R1_4 (R15H)		; 33-35		n 42
	L1prefetchw srcreg+d1+L1pd, L1pt

	zfmsubpd zmm12, zmm14, zmm21, zmm16	;; A8 = R8 * cosine/sine - I8					; 32-35		n 40
	zfmaddpd zmm16, zmm16, zmm21, zmm14	;; B8 = I8 * cosine/sine + R8					; 33-36		n 41
	L1prefetchw srcreg+d1+64+L1pd, L1pt

	vshuff64x2 zmm4, zmm5, zmm1, 01000100b	;; I5_3 I5_2 I5_1 I5_0 I1_3 I1_2 I1_1 I1_0 (I15L)		; 34-36		n 48
	vshuff64x2 zmm5, zmm5, zmm1, 11101110b	;; I5_7 I5_6 I5_5 I5_4 I1_7 I1_6 I1_5 I1_4 (I15H)		; 35-37		n 50
	L1prefetchw srcreg+d2+L1pd, L1pt

	vmulpd	zmm11, zmm11, zmm18		;; A2 = A2 * sine (final R2)					; 34-37		n 40
	vmulpd	zmm10, zmm10, zmm18		;; B2 = B2 * sine (final I2)					; 35-38		n 42
	L1prefetchw srcreg+d2+64+L1pd, L1pt

	vshuff64x2 zmm1, zmm7, zmm8, 01000100b	;; R7_3 R7_2 R7_1 R7_0 R3_3 R3_2 R3_1 R3_0 (R37L)		; 36-38		n 44
	vshuff64x2 zmm7, zmm7, zmm8, 11101110b	;; R7_7 R7_6 R7_5 R7_4 R3_7 R3_6 R3_5 R3_4 (R37H)		; 37-39		n 44
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vmulpd	zmm0, zmm0, zmm17		;; A6 = A6 * sine (final R6)					; 36-39		n 40
	vmulpd	zmm15, zmm15, zmm17		;; B6 = B6 * sine (final I6)					; 37-40		n 42
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt

	vshuff64x2 zmm8, zmm13, zmm3, 01000100b	;; I7_3 I7_2 I7_1 I7_0 I3_3 I3_2 I3_1 I3_0 (I37L)		; 38-40		n 48
	vshuff64x2 zmm13, zmm13, zmm3, 11101110b;; I7_7	I7_6 I7_5 I7_4 I3_7 I3_6 I3_5 I3_4 (I37H)		; 39-41		n 50
	L1prefetchw srcreg+d4+L1pd, L1pt

	vmulpd	zmm6, zmm6, zmm28		;; A4 = A4 * sine (final R4)					; 38-41		n 44
	vmulpd	zmm2, zmm2, zmm28		;; B4 = B4 * sine (final I4)					; 39-42		n 46
	L1prefetchw srcreg+d4+64+L1pd, L1pt

	vshuff64x2 zmm3, zmm11, zmm0, 01000100b	;; R6_3 R6_2 R6_1 R6_0 R2_3 R2_2 R2_1 R2_0 (R26L)		; 40-42		n 46
	vshuff64x2 zmm11, zmm11, zmm0, 11101110b;; R6_7	R6_6 R6_5 R6_4 R2_7 R2_6 R2_5 R2_4 (R26H)		; 41-43		n 46
	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	vmulpd	zmm12, zmm12, zmm27		;; A8 = A8 * sine (final R8)					; 40-43		n 44
	vmulpd	zmm16, zmm16, zmm27		;; B8 = B8 * sine (final I8)					; 41-44		n 46
	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt

	vshuff64x2 zmm0, zmm10, zmm15, 01000100b;; I6_3 I6_2 I6_1 I6_0 I2_3 I2_2 I2_1 I2_0 (I26L)		; 42-44		n 52
	vshuff64x2 zmm10, zmm10, zmm15, 11101110b;; I6_7 I6_6 I6_5 I6_4 I2_7 I2_6 I2_5 I2_4 (I26H)		; 43-45		n 54
	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	;; Do four of the first butterflies while we are shuffling!
	vaddpd	zmm14, zmm19, zmm9		;; add r1/r5 4-aparts (R15+)					; 42-45		n 56
	vsubpd	zmm19, zmm19, zmm9		;; sub r1/r5 4-aparts (R15-)					; 43-46		n 58
	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt

	vshuff64x2 zmm15, zmm6, zmm12, 01000100b;; R8_3 R8_2 R8_1 R8_0 R4_3 R4_2 R4_1 R4_0 (R48L)		; 44-46		n 48
	vshuff64x2 zmm6, zmm6, zmm12, 11101110b	;; R8_7 R8_6 R8_5 R8_4 R4_7 R4_6 R4_5 R4_4 (R48H)		; 45-47		n 48
	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	vaddpd	zmm9, zmm1, zmm7		;; add r3/r7 4-aparts (R37+)					; 44-47		n 56
	vsubpd	zmm1, zmm1, zmm7		;; sub r3/r7 4-aparts (R37-)					; 45-48		n 58
	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt

	vshuff64x2 zmm12, zmm2, zmm16, 01000100b;; I8_3 I8_2 I8_1 I8_0 I4_3 I4_2 I4_1 I4_0 (I48L)		; 46-48		n 52
	vshuff64x2 zmm2, zmm2, zmm16, 11101110b	;; I8_7 I8_6 I8_5 I8_4 I4_7 I4_6 I4_5 I4_4 (I48H)		; 47-49		n 54
	bump	screg1, scinc1

	vaddpd	zmm7, zmm3, zmm11		;; add r2/r6 4-aparts (R26+)					; 46-49		n 60
	vsubpd	zmm3, zmm3, zmm11		;; sub r2/r6 4-aparts (R26-)					; 47-50		n 62
	bump	screg2, scinc2

	;; Swap the two aparts
	vshuff64x2 zmm16, zmm4, zmm8, 10001000b	;; I7_1 I7_0 I3_1 I3_0 I5_1 I5_0 I1_1 I1_0 (i1537LL)		; 48-50		n 68
	vshuff64x2 zmm4, zmm4, zmm8, 11011101b	;; I7_3 I7_2 I3_3 I3_2 I5_3 I5_2 I1_3 I1_2 (i1537LH)		; 49-51		n 66

	vaddpd	zmm11, zmm15, zmm6		;; add r4/r8 4-aparts (R48+)					; 48-51		n 60
	vsubpd	zmm15, zmm15, zmm6		;; sub r4/r8 4-aparts (R48-)					; 49-52		n 62

	vshuff64x2 zmm8, zmm5, zmm13, 10001000b	;; I7_5 I7_4 I3_5 I3_4 I5_5 I5_4 I1_5 I1_4 (i1537HL)		; 50-52		n 70
	vshuff64x2 zmm5, zmm5, zmm13, 11011101b	;; I7_7 I7_6 I3_7 I3_6 I5_7 I5_6 I1_7 I1_6 (i1537HH)		; 51-53		n 64

	vblendmpd zmm26 {k6}, zmm22, zmm30	;; 1, 1, 1, 1, 1, 1, 1		.924/.383			; 50		n 68
	vblendmpd zmm27 {k6}, zmm22, zmm31	;; 1, 1, 1, 1, 1, 1, 1		SQRTHALF			; 51		n 87

	vshuff64x2 zmm13, zmm0, zmm12, 10001000b;; I8_1 I8_0 I4_1 I4_0 I6_1 I6_0 I2_1 I2_0 (i2648LL)		; 52-54		n 68
	vshuff64x2 zmm0, zmm0, zmm12, 11011101b	;; I8_3 I8_2 I4_3 I4_2 I6_3 I6_2 I2_3 I2_2 (i2648LH)		; 53-55		n 66

	vmovapd zmm25 {k7}{z}, zmm22		;; 1, 1, 1, 1, 1, 1, 1		0				; 52		n 53
	vsubpd	zmm25 {k6}, zmm25, zmm31	;; 1, 1, 1, 1, 1, 1, 1		-SQRTHALF			; 53		n 86

	vshuff64x2 zmm12, zmm10, zmm2, 10001000b;; I8_5 I8_4 I4_5 I4_4 I6_5 I6_4 I2_5 I2_4 (i2648HL)		; 54-56		n 70
	vshuff64x2 zmm10, zmm10, zmm2, 11011101b;; I8_7	I8_6 I4_7 I4_6 I6_7 I6_6 I2_7 I2_6 (i2648HH)		; 55-57		n 64

	vblendmpd zmm28 {k6}, zmm31, zmm29	;; SQRTHALF, SQRTHALF, ...,	.383				; 54		n 96

	vshuff64x2 zmm2, zmm14, zmm9, 10001000b	;; R7_1 R7_0 R3_1 R3_0 R5_1 R5_0 R1_1 R1_0 (r1537LL+)		; 56-58		n 76
	vshuff64x2 zmm14, zmm14, zmm9, 11011101b;; R7_3 R7_2 R3_3 R3_2 R5_3 R5_2 R1_3 R1_2 (r1537LH+)		; 57-59		n 78

	vshuff64x2 zmm9, zmm19, zmm1, 10001000b	;; R7_5 R7_4 R3_5 R3_4 R5_5 R5_4 R1_5 R1_4 (r1537HL-)		; 58-60		n 72
	vshuff64x2 zmm19, zmm19, zmm1, 11011101b;; R7_7 R7_6 R3_7 R3_6 R5_7 R5_6 R1_7 R1_6 (r1537HH-)		; 59-61		n 74

	vshuff64x2 zmm1, zmm7, zmm11, 10001000b	;; R8_1 R8_0 R4_1 R4_0 R6_1 R6_0 R2_1 R2_0 (r2648LL+)		; 60-62		n 76
	vshuff64x2 zmm7, zmm7, zmm11, 11011101b	;; R8_3 R8_2 R4_3 R4_2 R6_3 R6_2 R2_3 R2_2 (r2648LH+)		; 61-63		n 78

	vshuff64x2 zmm11, zmm3, zmm15, 10001000b;; R8_5 R8_4 R4_5 R4_4 R6_5 R6_4 R2_5 R2_4 (r2648HL-)		; 62-64		n 72
	vshuff64x2 zmm3, zmm3, zmm15, 11011101b	;; R8_7 R8_6 R4_7 R4_6 R6_7 R6_6 R2_7 R2_6 (r2648HH-)		; 63-65		n 74

	;; Swap the one aparts
	vshufpd	zmm15, zmm5, zmm10, 11111111b	;; I8_7	I7_7 I4_7 I3_7 I6_7 I5_7 I2_7 I1_7 (first I8 & R16)	; 64		n 68
	vshufpd	zmm5, zmm5, zmm10, 00000000b	;; I8_6	I7_6 I4_6 I3_6 I6_6 I5_6 I2_6 I1_6 (first I7 & R15)	; 65		n 70

	vshufpd	zmm10, zmm4, zmm0, 11111111b	;; I8_3 I7_3 I4_3 I3_3 I6_3 I5_3 I2_3 I1_3 (first I4 & R12)	; 66		n 68
	vshufpd	zmm4, zmm4, zmm0, 00000000b	;; I8_2 I7_2 I4_2 I3_2 I6_2 I5_2 I2_2 I1_2 (first I3 & R11)	; 67		n 70

	vshufpd	zmm0, zmm16, zmm13, 11111111b	;; I8_1 I7_1 I4_1 I3_1 I6_1 I5_1 I2_1 I1_1 (first I2 & R10)	; 68		n 72
	vshufpd	zmm16, zmm16, zmm13, 00000000b	;; I8_0 I7_0 I4_0 I3_0 I6_0 I5_0 I2_0 I1_0 (first I1 & R9)	; 69		n 74

						;; Simultaneously do an 8-complex and 16-reals
						;; 8-complex			16-reals
						;;				R12/R16 becomes newer R12/I12
						;;				mul R12/I12 by w^3 = .383 + .924i
	zfnmaddpd zmm6, zmm15, zmm26, zmm10	;; I4 - I8*1 (new I8)		R12/.383 = R12 - I12*.924/.383	; 68-71		n 76
	zfmaddpd zmm10, zmm10, zmm26, zmm15	;; I4*1 + I8 (new I4)		I12/.383 = R12*.924/.383 + I12	; 69-72		n 81

	vshufpd	zmm13, zmm8, zmm12, 11111111b	;; I8_5 I7_5 I4_5 I3_5 I6_5 I5_5 I2_5 I1_5 (first I6 & R14)	; 70		n 72
	vshufpd	zmm8, zmm8, zmm12, 00000000b	;; I8_4 I7_4 I4_4 I3_4 I6_4 I5_4 I2_4 I1_4 (first I5 & R13)	; 71		n 74

						;;				R11/R15 becomes newer R11/I11
						;;				mul R11/I11 by SQRTHALF + i*SQRTHALF
	vaddpd	zmm15, zmm4, zmm5		;; I3 + I7 (new I3)		I11/SQRTHALF = R11 + I11	; 70-73		n 84
	vsubpd	zmm4, zmm4, zmm5		;; I3 - I7 (new I7)		R11/SQRTHALF = R11 - I11	; 71-74		n 86

	vshufpd	zmm12, zmm9, zmm11, 11111111b	;; R8_5 R7_5 R4_5 R3_5 R6_5 R5_5 R2_5 R1_5 (R2-R6 = new R6)	; 72		n 78
	vshufpd	zmm9, zmm9, zmm11, 00000000b	;; R8_4 R7_4 R4_4 R3_4 R6_4 R5_4 R2_4 R1_4 (R1-R5 = new R5)	; 73		n 84

						;;				R10/R14 becomes newer R10/I10
						;;				mul R10/I10 by w^1 = .924 + .383i
	zfmsubpd zmm5, zmm0, zmm26, zmm13	;; I2*1 - I6 (new I6)		R10/.383 = R10*.924/.383 - I10	; 72-75		n 80
	zfmaddpd zmm0, zmm13, zmm26, zmm0	;; I2 + I6*1 (new I2)		I10/.383 = I10*.924/.383 + R10	; 73-76		n 81

	vshufpd	zmm11, zmm19, zmm3, 11111111b	;; R8_7	R7_7 R4_7 R3_7 R6_7 R5_7 R2_7 R1_7 (R4-R8 = new R8)	; 74		n 76
	vshufpd	zmm19, zmm19, zmm3, 00000000b	;; R8_6	R7_6 R4_6 R3_6 R6_6 R5_6 R2_6 R1_6 (R3-R7 = new R7)	; 75		n 84

						;;				R9/R13 becomes newer R9/I9
	vaddpd	zmm13 {k7}{z}, zmm16, zmm8	;; I1 + I5 (new I1)		0				; 74-77		n 85
	vsubpd	zmm8 {k7}, zmm16, zmm8		;; I1 - I5 (new I5)		I9				; 75-78		n 87

	vshufpd	zmm3, zmm2, zmm1, 11111111b	;; R8_1 R7_1 R4_1 R3_1 R6_1 R5_1 R2_1 R1_1 (R2+R6=  new R2)	; 76		n 83
	vshufpd	zmm2, zmm2, zmm1, 00000000b	;; R8_0 R7_0 R4_0 R3_0 R6_0 R5_0 R2_0 R1_0 (R1+R5 = new R1)	; 77		n 82

						;; next FFT level
						;;				R6/R8 becomes newest R6/I6
						;;				mul R6/I6 by w^2 = .707 + .707i
	vblendmpd zmm18 {k6}, zmm6, zmm11	;; I8				I6				; 76		n 78
	vblendmpd zmm11 {k6}, zmm11, zmm6	;; R8				R12				; 77		n 80

	vshufpd	zmm1, zmm14, zmm7, 11111111b	;; R8_3 R7_3 R4_3 R3_3 R6_3 R5_3 R2_3 R1_3 (R4+R8 = new R4)	; 78		n 83
	vshufpd	zmm14, zmm14, zmm7, 00000000b	;; R8_2 R7_2 R4_2 R3_2 R6_2 R5_2 R2_2 R1_2 (R3+R7 = new R3)	; 79		n 82

	vsubpd	zmm17, zmm12, zmm18		;; R6 - I8 (new2 R6)		R6 - I6 (newest R6/SQRTHALF)	; 78-12		n 88
	vaddpd	zmm12, zmm12, zmm18		;; R6 + I8 (new2 R8)		R6 + I6 (newest I6/SQRTHALF)	; 79-12		n 90

	vaddpd	zmm6, zmm5, zmm11		;; I6 + R8 (new2 I6)		R10 + R12 (newest R10/.383)	; 80-13		n 88
	vsubpd	zmm5, zmm5, zmm11		;; I6 - R8 (new2 I8)		R10 - R12 (newest R12/.383)	; 80-13		n 90

	vaddpd	zmm11, zmm0, zmm10		;; I2 + I4 (newer I2)		I10 + I12 (newest I10/.383)	; 81-14		n 88
	vsubpd	zmm0, zmm0, zmm10		;; I2 - I4 (newer I4)		I10 - I12 (newest I12/.383)	; 81-14		n 89

	vaddpd	zmm10, zmm2, zmm14		;; R1 + R3 (newer R1)		R1 + R3 (newest R1)		; 82-15		n 89
	vsubpd	zmm2, zmm2, zmm14		;; R1 - R3 (newer R3)		R1 - R3 (newest R3)		; 82-15		n 91

	vaddpd	zmm14, zmm3, zmm1		;; R2 + R4 (newer R2)		R2 + R4 (newest R2)		; 83-86		n 89
	vsubpd	zmm3, zmm3, zmm1		;; R2 - R4 (newer R4)		R2 - R4 (newest R4)		; 83-16		n 91

	vblendmpd zmm18 {k6}, zmm19, zmm15	;; R7				I11/SQRTHALF			; 84		n 87
	vblendmpd zmm16 {k6}, zmm9, zmm16	;; R5				R9				; 84		n 86

						;;				R5/R7 becomes newest R5/I5
	vsubpd	zmm19 {k7}, zmm13, zmm15	;; I1 - I3 (newer I3)		I5				; 85-17		n 95
	vaddpd	zmm13 {k7}{z}, zmm13, zmm15	;; I1 + I3 (newer I1)		0				; 85-17		n 93

	zfnmaddpd zmm7, zmm4, zmm25, zmm16	;; R5 - I7*1 (newer R5)		R9 - R11*-SQRTHALF (newest R9)	; 86-18		n 96
	zfmaddpd zmm4, zmm4, zmm25, zmm16	;; R5 + I7*1 (newer R7)		R9 + R11*-SQRTHALF (newest R11)	; 86-18		n 98

	zfmaddpd zmm16, zmm18, zmm27, zmm8	;; I5 + R7*1 (newer I5)		I9 + I11*SQRTHALF (newest I9)	; 87-19		n 97
	zfnmaddpd zmm18, zmm18, zmm27, zmm8	;; I5 - R7*1 (newer I7)		I9 - I11*SQRTHALF (newest I11)	; 87-19		n 99

	vmovapd	zmm8, zmm11			;;				I10/.383
	vaddpd	zmm8 {k7}, zmm17, zmm6		;; I6 = R6 + I6 (newer I6/SQRTHALF)	I10/.383		; 88-91		n 97
	vsubpd	zmm6 {k7}, zmm17, zmm6		;; R6 = R6 - I6 (newer R6/SQRTHALF)	R10/.383		; 88-91		n 96

	vblendmpd zmm17 {k6}, zmm0, zmm17	;; I4				R6/SQRTHALF			; 89		n 94
	vsubpd	zmm11 {k6}, zmm10, zmm14	;; I2				R1 - R2 (final R1b)		; 89-92		n 93

	vaddpd	zmm0 {k7}, zmm12, zmm5		;; I8 = R8 + I8 (newer I8/SQRTHALF)	I12/.383		; 90-92		n 98
	vsubpd	zmm5 {k7}, zmm12, zmm5		;; R8 = R8 - I8 (newer R8/SQRTHALF)	R12/.383		; 90-92		n 99

	;; last FFT level

	vblendmpd zmm9 {k6}, zmm2, zmm9		;; R3				R5				; 91		n 94
	vblendmpd zmm12 {k6}, zmm3, zmm12	;; R4				I6/SQRTHALF			; 91		n 95

						;;				R3/R4 becomes final R3/I3
	vaddpd	zmm1, zmm10, zmm14		;; R1 + R2 (final R1)		R1 + R2 (final R1a => R1)	; 92-95		n 
	vsubpd	zmm2 {k7}, zmm10, zmm14		;; R1 - R2 (final R2)		R3 => R2			; 92-95		n 

	vsubpd	zmm3 {k7}, zmm13, zmm11		;; I1 - I2 (final I2)		I3 => I2			; 93-96		n 
	vaddpd	zmm11 {k7}, zmm13, zmm11	;; I1 + I2 (final I1)		R1b => I1			; 93-96		n 

	zfnmaddpd zmm13, zmm17, zmm25, zmm9	;; R3 - I4*1 (final R3)		R5 - R6*-SQRTHALF (final R5 => R3) ; 94-97	n 
	zfmaddpd zmm9, zmm17, zmm25, zmm9	;; R3 + I4*1 (final R4)		R5 + R6*-SQRTHALF (final R6 => R4) ; 94-97	n 

	zfmaddpd zmm15, zmm12, zmm27, zmm19	;; I3 + R4*1 (final I3)		I5 + I6*SQRTHALF (final I5 => I3) ; 95-98	n 
	zfnmaddpd zmm12, zmm12, zmm27, zmm19	;; I3 - R4*1 (final I4)		I5 - I6*SQRTHALF (final I6 => I4) ; 95-98	n 

	zfmaddpd zmm10, zmm6, zmm28, zmm7	;; R5 + R6*SQRTHALF (final R5)	R9 + R10*.383 (final R9 => R5)	; 96-99		n 
	zfnmaddpd zmm6, zmm6, zmm28, zmm7	;; R5 - R6*SQRTHALF (final R6)	R9 - R10*.383 (final R10 => R6)	; 96-99		n 

	zfmaddpd zmm7, zmm8, zmm28, zmm16	;; I5 + I6*SQRTHALF (final I5)	I9 + I10*.383 (final I9 => I5)	; 97-100	n 
	zfnmaddpd zmm8, zmm8, zmm28, zmm16	;; I5 - I6*SQRTHALF (final I6)	I9 - I10*.383 (final I10 => I6)	; 97-100	n 

	zfnmaddpd zmm14, zmm0, zmm28, zmm4	;; R7 - I8*SQRTHALF (final R7)	R11 - I12*.383 (final R11 => R7); 98-101	n 
	zfmaddpd zmm0, zmm0, zmm28, zmm4	;; R7 + I8*SQRTHALF (final R8)	R11 + I12*.383 (final R12 => R8); 98-101	n 

	zfmaddpd zmm4, zmm5, zmm28, zmm18	;; I7 + R8*SQRTHALF (final I7)	I11 + R12*.383 (final I11 => I7); 99-102	n 
	zfnmaddpd zmm5, zmm5, zmm28, zmm18	;; I7 - R8*SQRTHALF (final I8)	I11 - R12*.383 (final I12 => I8); 99-102	n 

	zstore	[srcreg], zmm1			;; Save R1
	zstore	[srcreg+d1], zmm2		;; Save R2
	zstore	[srcreg+d1+64], zmm3		;; Save I2
	zstore	[srcreg+64], zmm11		;; Save I1
	zstore	[srcreg+d2], zmm13		;; Save R3
	zstore	[srcreg+d2+d1], zmm9		;; Save R4
	zstore	[srcreg+d2+64], zmm15		;; Save I3
	zstore	[srcreg+d2+d1+64], zmm12	;; Save I4
	zstore	[srcreg+d4], zmm10		;; Save R5
	zstore	[srcreg+d4+d1], zmm6		;; Save R6
	zstore	[srcreg+d4+64], zmm7		;; Save I5
	zstore	[srcreg+d4+d1+64], zmm8		;; Save I6
	zstore	[srcreg+d4+d2], zmm14		;; Save R7
	zstore	[srcreg+d4+d2+d1], zmm0		;; Save R8
	zstore	[srcreg+d4+d2+64], zmm4		;; Save I7
	zstore	[srcreg+d4+d2+d1+64], zmm5	;; Save I8
	bump	srcreg, srcinc
	ENDM



zr64_hundredtwentyeight_real_with_square_preload MACRO
	zr64_128r_square_cmn_preload
	ENDM
zr64_hundredtwentyeight_real_with_square MACRO srcreg,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	zr64_128r_square_cmn srcreg,0,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM
zr64f_hundredtwentyeight_real_with_square_preload MACRO
	zr64_128r_square_cmn_preload
	ENDM
zr64f_hundredtwentyeight_real_with_square MACRO srcreg,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	zr64_128r_square_cmn srcreg,rbx,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

zr64_128r_square_cmn_preload MACRO
	;; preload would be silly for a macro that is executed only once
	ENDM
zr64_128r_square_cmn MACRO srcreg,srcoff,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	LOCAL	nosave, orig, back_to_orig, muladd, muladdhard, mulsub, mulsubhard, fma_done

	;; For zero pad
	zsquare7 srcreg

	vmovapd	zmm1, [srcreg+srcoff+d1]	;; r2+r10
	vmovapd	zmm3, [srcreg+srcoff+d4+d2+d1]	;; r8+r16
	vaddpd	zmm0, zmm1, zmm3		;; r2++ = (r2+r10)+(r8+r16)					; 1-4		n 6
	vsubpd	zmm1, zmm1, zmm3		;; r2+- = (r2+r10)-(r8+r16)					; 1-4		n 7

	vmovapd	zmm5, [srcreg+srcoff+d2+d1]	;; r4+r12
	vmovapd	zmm7, [srcreg+srcoff+d4+d1]	;; r6+r14
	vaddpd	zmm3, zmm5, zmm7		;; r4++ = (r4+r12)+(r6+r14)					; 2-5		n 6
	vsubpd	zmm5, zmm5, zmm7		;; r4+- = (r4+r12)-(r6+r14)					; 2-5		n 7

	vmovapd	zmm13, [srcreg+srcoff]		;; r1+r9
	vmovapd	zmm15, [srcreg+srcoff+d4]	;; r5+r13
	vaddpd	zmm7, zmm13, zmm15		;; r1++ = (r1+r9)+(r5+r13)					; 3-6		n 8
	vsubpd	zmm13, zmm13, zmm15		;; r1+- = (r1+r9)-(r5+r13)					; 3-6		n 11

	vmovapd	zmm9, [srcreg+srcoff+d2]	;; r3+r11
	vmovapd	zmm11, [srcreg+srcoff+d4+d2]	;; r7+r15
	vaddpd	zmm15, zmm9, zmm11		;; r3++ = (r3+r11)+(r7+r15)					; 4-7		n 8
	vsubpd	zmm9, zmm9, zmm11		;; r3+- = (r3+r11)-(r7+r15)					; 4-7		n 12

	vmovapd	zmm10, [srcreg+srcoff+d2+64]	;; r3-r11
	vmovapd	zmm12, [srcreg+srcoff+d4+d2+64]	;; r7-r15
	vaddpd	zmm11, zmm10, zmm12		;; r3-+ = (r3-r11)+(r7-r15)					; 5-8		n 16
	vsubpd	zmm10, zmm10, zmm12		;; r3-- = (r3-r11)-(r7-r15)					; 5-8		n 14

	vmovapd	zmm2, [srcreg+srcoff+d1+64]	;; r2-r10
	vaddpd	zmm12, zmm0, zmm3		;; r2+++ = (r2++) + (r4++)					; 6-9		n 13
	vsubpd	zmm0, zmm0, zmm3		;; r2++- = (r2++) - (r4++)					; 6-9		n 11

	vmovapd	zmm4, [srcreg+srcoff+d4+d2+d1+64] ;; r8-r16
	vaddpd	zmm3, zmm1, zmm5		;; r2+-+ = (r2+-) + (r4+-)					; 7-10		n 12
	vsubpd	zmm1, zmm1, zmm5		;; r2+-- = (r2+-) - (r4+-)	(final I5)			; 7-10		n 18

	vmovapd	zmm6, [srcreg+srcoff+d2+d1+64]	;; r4-r12
	vaddpd	zmm5, zmm7, zmm15		;; r1+++ = (r1++) + (r3++)					; 8-11		n 13
	vsubpd	zmm7, zmm7, zmm15		;; r1++- = (r1++) - (r3++)	(final R5)			; 8-11		n 18

	vmovapd	zmm8, [srcreg+srcoff+d4+d1+64]	;; r6-r14
	vaddpd	zmm15, zmm2, zmm4		;; r2-+ = (r2-r10)+(r8-r16)					; 9-12		n 17
	vsubpd	zmm2, zmm2, zmm4		;; r2-- = (r2-r10)-(r8-r16)					; 9-12		n 15

	vbroadcastsd zmm31, ZMM_SQRTHALF
	vaddpd	zmm4, zmm6, zmm8		;; r4-+ = (r4-r12)+(r6-r14)					; 10-13		n 17
	vsubpd	zmm6, zmm6, zmm8		;; r4-- = (r4-r12)-(r6-r14)					; 10-13		n 15

	vmovapd	zmm14, [srcreg+srcoff+64]	;; r1-r9
	zfmaddpd zmm8, zmm0, zmm31, zmm13	;; R3 = (r1+-) + .707(r2++-)					; 11-14		n 19
	zfnmaddpd zmm0, zmm0, zmm31, zmm13	;; R7 = (r1+-) - .707(r2++-)					; 11-14		n 20

	vbroadcastsd zmm30, ZMM_P924_P383
	zfmaddpd zmm13, zmm3, zmm31, zmm9	;; I3 = .707*(r2+-+) + (r3+-)					; 12-15		n 19
	zfmsubpd zmm3, zmm3, zmm31, zmm9	;; I7 = .707*(r2+-+) - (r3+-)					; 12-15		n 20

	vmovapd	zmm16, [srcreg+srcoff+d4+64]	;; r5-r13
	vaddpd	zmm9, zmm5, zmm12		;; R1 = (r1+++) + (r2+++)					; 13-16		n 31
	vsubpd	zmm5, zmm5, zmm12		;; R9 = (r1+++) - (r2+++)					; 13-16		n 33

	vmovapd	zmm28, [screg2+1*128+64]	;; cosine/sine for R5/I5 (w^4 = complex w^2)
	zfmaddpd zmm12, zmm10, zmm31, zmm14	;; r2o = (r1-r9) + .707*(r3--)					; 14-17		n 21
	zfnmaddpd zmm10, zmm10, zmm31, zmm14	;; r4o = (r1-r9) - .707*(r3--)					; 14-17		n 22

	vmovapd	zmm27, [screg2+0*128+64]	;; cosine/sine for R3/I3 (w^2 = complex w^1)
	zfmaddpd zmm14, zmm2, zmm30, zmm6	;; r2e/.383 = .924/.383(r2--) + (r4--)				; 15-18		n 21
	zfnmaddpd zmm6, zmm6, zmm30, zmm2	;; r4e/.383 = (r2--) - .924/.383(r4--)				; 15-18		n 22

	vmovapd	zmm26, [screg2+2*128+64]	;; cosine/sine for R7/I7 (w^6 = complex w^3)
	zfmaddpd zmm2, zmm11, zmm31, zmm16	;; i2o = .707(r3-+) + (r5-r13)					; 16-19		n 23
	zfmsubpd zmm11, zmm11, zmm31, zmm16	;; i4o = .707(r3-+) - (r5-r13)					; 16-19		n 24

	vbroadcastsd zmm29, ZMM_P383
	zfmaddpd zmm16, zmm4, zmm30, zmm15	;; i2e/.383 = (r2-+) + .924/.383(r4-+)				; 17-20		n 23
	zfmsubpd zmm15, zmm15, zmm30, zmm4	;; i4e/.383 = .924/.383(r2-+) - (r4-+)				; 17-20		n 24

	vmovapd	zmm25, [screg2+1*128]		;; sine for R5/I5
	zfmsubpd zmm4, zmm7, zmm28, zmm1	;; A5 = R5 * cosine/sine - I5					; 18-21		n 25
	zfmaddpd zmm1, zmm1, zmm28, zmm7	;; B5 = I5 * cosine/sine + R5					; 18-21		n 25

	vmovapd	zmm24, [screg2+0*128]		;; sine for R3/I3
	zfmsubpd zmm7, zmm8, zmm27, zmm13	;; A3 = R3 * cosine/sine - I3					; 19-22		n 26
	zfmaddpd zmm13, zmm13, zmm27, zmm8	;; B3 = I3 * cosine/sine + R3					; 19-22		n 26

	vmovapd	zmm23, [screg2+2*128]		;; sine for R7/I7
	zfmsubpd zmm8, zmm0, zmm26, zmm3	;; A7 = R7 * cosine/sine - I7					; 20-23		n 27
	zfmaddpd zmm3, zmm3, zmm26, zmm0	;; B7 = I7 * cosine/sine + R7					; 20-23		n 27

	vmovapd	zmm22, [screg1+0*128+64]	;; cosine/sine for R2/I2 (w^1)
	zfmaddpd zmm0, zmm14, zmm29, zmm12	;; R2 = r2o + .383*r2e						; 21-24		n 28
	zfnmaddpd zmm14, zmm14, zmm29, zmm12	;; R8 = r2o - .383*r2e						; 21-24		n 32

	vmovapd	zmm19, [screg1+2*128+64]	;; cosine/sine for R6/I6 (w^5)
	zfmaddpd zmm12, zmm6, zmm29, zmm10	;; R4 = r4o + .383*r4e						; 22-25		n 30
	zfnmaddpd zmm6, zmm6, zmm29, zmm10	;; R6 = r4o - .383*r4e						; 22-25		n 29

	vmovapd	zmm20, [screg1+1*128+64]	;; cosine/sine for R4/I4 (w^3)
	zfmaddpd zmm10, zmm16, zmm29, zmm2	;; I2 = .383*i2e + i2o						; 23-26		n 28
	zfmsubpd zmm16, zmm16, zmm29, zmm2	;; I8 = .383*i2e - i2o						; 23-26		n 32

	vmovapd	zmm21, [screg1+3*128+64]	;; cosine/sine for R8/I8 (w^7)
	zfmaddpd zmm2, zmm15, zmm29, zmm11	;; I4 = .383*i4e + i4o						; 24-27		n 30
	zfmsubpd zmm15, zmm15, zmm29, zmm11	;; I6 = .383*i4e - i4o						; 24-27		n 29

	vmovapd	zmm18, [screg1+0*128]		;; sine for R2/I2
	vmulpd	zmm4, zmm4, zmm25		;; A5 = A5 * sine (final R5)					; 25-28		n 32
	vmulpd	zmm1, zmm1, zmm25		;; B5 = B5 * sine (final I5)					; 25-28		n 34

	vmovapd zmm17, [screg1+2*128]		;; sine for R6/I6
	vmulpd	zmm7, zmm7, zmm24		;; A3 = A3 * sine (final R3)					; 26-29		n 36
	vmulpd	zmm13, zmm13, zmm24		;; B3 = B3 * sine (final I3)					; 26-29		n 38

	vmovapd	zmm28, [screg1+1*128]		;; sine for R4/I4
	vmulpd	zmm8, zmm8, zmm23		;; A7 = A7 * sine (final R7)					; 27-30		n 36
	vmulpd	zmm3, zmm3, zmm23		;; B7 = B7 * sine (final I7)					; 27-30		n 38

	vmovapd	zmm27, [screg1+3*128]		;; sine for R8/I8
	zfmsubpd zmm11, zmm0, zmm22, zmm10	;; A2 = R2 * cosine/sine - I2					; 28-31		n 34
	zfmaddpd zmm10, zmm10, zmm22, zmm0	;; B2 = I2 * cosine/sine + R2					; 28-31		n 35

	vbroadcastsd zmm22, ZMM_ONE
	zfmsubpd zmm0, zmm6, zmm19, zmm15	;; A6 = R6 * cosine/sine - I6					; 29-32		n 36
	zfmaddpd zmm15, zmm15, zmm19, zmm6	;; B6 = I6 * cosine/sine + R6					; 29-32		n 37

	mov	eax, 11111110b			;; We're pretty sure callers of this macro call zloop_init next
	kmovw	k7, eax				;; Set k7 to 11111110b						; 30		n 31
	knotw	k6, k7				;; Set k6 to 00000001b						; 31		n 50
	L1prefetchw srcreg+L1pd, L1pt

	zfmsubpd zmm6, zmm12, zmm20, zmm2	;; A4 = R4 * cosine/sine - I4					; 30-33		n 38
	zfmaddpd zmm2, zmm2, zmm20, zmm12	;; B4 = I4 * cosine/sine + R4					; 31-34		n 39
	L1prefetchw srcreg+64+L1pd, L1pt

	;; Swap the four aparts
	vshuff64x2 zmm19, zmm9, zmm4, 01000100b	;; R5_3 R5_2 R5_1 R5_0 R1_3 R1_2 R1_1 R1_0 (R15L)		; 32-34		n 42
	vshuff64x2 zmm9, zmm9, zmm4, 11101110b	;; R5_7	R5_6 R5_5 R5_4 R1_7 R1_6 R1_5 R1_4 (R15H)		; 33-35		n 42
	L1prefetchw srcreg+d1+L1pd, L1pt

	zfmsubpd zmm12, zmm14, zmm21, zmm16	;; A8 = R8 * cosine/sine - I8					; 32-35		n 40
	zfmaddpd zmm16, zmm16, zmm21, zmm14	;; B8 = I8 * cosine/sine + R8					; 33-36		n 41
	L1prefetchw srcreg+d1+64+L1pd, L1pt

	vshuff64x2 zmm4, zmm5, zmm1, 01000100b	;; I5_3 I5_2 I5_1 I5_0 I1_3 I1_2 I1_1 I1_0 (I15L)		; 34-36		n 48
	vshuff64x2 zmm5, zmm5, zmm1, 11101110b	;; I5_7 I5_6 I5_5 I5_4 I1_7 I1_6 I1_5 I1_4 (I15H)		; 35-37		n 50
	L1prefetchw srcreg+d2+L1pd, L1pt

	vmulpd	zmm11, zmm11, zmm18		;; A2 = A2 * sine (final R2)					; 34-37		n 40
	vmulpd	zmm10, zmm10, zmm18		;; B2 = B2 * sine (final I2)					; 35-38		n 42
	L1prefetchw srcreg+d2+64+L1pd, L1pt

	vshuff64x2 zmm1, zmm7, zmm8, 01000100b	;; R7_3 R7_2 R7_1 R7_0 R3_3 R3_2 R3_1 R3_0 (R37L)		; 36-38		n 44
	vshuff64x2 zmm7, zmm7, zmm8, 11101110b	;; R7_7 R7_6 R7_5 R7_4 R3_7 R3_6 R3_5 R3_4 (R37H)		; 37-39		n 44
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vmulpd	zmm0, zmm0, zmm17		;; A6 = A6 * sine (final R6)					; 36-39		n 40
	vmulpd	zmm15, zmm15, zmm17		;; B6 = B6 * sine (final I6)					; 37-40		n 42
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt

	vshuff64x2 zmm8, zmm13, zmm3, 01000100b	;; I7_3 I7_2 I7_1 I7_0 I3_3 I3_2 I3_1 I3_0 (I37L)		; 38-40		n 48
	vshuff64x2 zmm13, zmm13, zmm3, 11101110b;; I7_7	I7_6 I7_5 I7_4 I3_7 I3_6 I3_5 I3_4 (I37H)		; 39-41		n 50
	L1prefetchw srcreg+d4+L1pd, L1pt

	vmulpd	zmm6, zmm6, zmm28		;; A4 = A4 * sine (final R4)					; 38-41		n 44
	vmulpd	zmm2, zmm2, zmm28		;; B4 = B4 * sine (final I4)					; 39-42		n 46
	L1prefetchw srcreg+d4+64+L1pd, L1pt

	vshuff64x2 zmm3, zmm11, zmm0, 01000100b	;; R6_3 R6_2 R6_1 R6_0 R2_3 R2_2 R2_1 R2_0 (R26L)		; 40-42		n 46
	vshuff64x2 zmm11, zmm11, zmm0, 11101110b;; R6_7	R6_6 R6_5 R6_4 R2_7 R2_6 R2_5 R2_4 (R26H)		; 41-43		n 46
	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	vmulpd	zmm12, zmm12, zmm27		;; A8 = A8 * sine (final R8)					; 40-43		n 44
	vmulpd	zmm16, zmm16, zmm27		;; B8 = B8 * sine (final I8)					; 41-44		n 46
	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt

	vshuff64x2 zmm0, zmm10, zmm15, 01000100b;; I6_3 I6_2 I6_1 I6_0 I2_3 I2_2 I2_1 I2_0 (I26L)		; 42-44		n 52
	vshuff64x2 zmm10, zmm10, zmm15, 11101110b;; I6_7 I6_6 I6_5 I6_4 I2_7 I2_6 I2_5 I2_4 (I26H)		; 43-45		n 54
	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	;; Do four of the first butterflies while we are shuffling!
	vaddpd	zmm14, zmm19, zmm9		;; add r1/r5 4-aparts (R15+)					; 42-45		n 56
	vsubpd	zmm19, zmm19, zmm9		;; sub r1/r5 4-aparts (R15-)					; 43-46		n 58
	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt

	vshuff64x2 zmm15, zmm6, zmm12, 01000100b;; R8_3 R8_2 R8_1 R8_0 R4_3 R4_2 R4_1 R4_0 (R48L)		; 44-46		n 48
	vshuff64x2 zmm6, zmm6, zmm12, 11101110b	;; R8_7 R8_6 R8_5 R8_4 R4_7 R4_6 R4_5 R4_4 (R48H)		; 45-47		n 48
	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	vaddpd	zmm9, zmm1, zmm7		;; add r3/r7 4-aparts (R37+)					; 44-47		n 56
	vsubpd	zmm1, zmm1, zmm7		;; sub r3/r7 4-aparts (R37-)					; 45-48		n 58
	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt

	vshuff64x2 zmm12, zmm2, zmm16, 01000100b;; I8_3 I8_2 I8_1 I8_0 I4_3 I4_2 I4_1 I4_0 (I48L)		; 46-48		n 52
	vshuff64x2 zmm2, zmm2, zmm16, 11101110b	;; I8_7 I8_6 I8_5 I8_4 I4_7 I4_6 I4_5 I4_4 (I48H)		; 47-49		n 54

	vaddpd	zmm7, zmm3, zmm11		;; add r2/r6 4-aparts (R26+)					; 46-49		n 60
	vsubpd	zmm3, zmm3, zmm11		;; sub r2/r6 4-aparts (R26-)					; 47-50		n 62

	;; Swap the two aparts
	vshuff64x2 zmm16, zmm4, zmm8, 10001000b	;; I7_1 I7_0 I3_1 I3_0 I5_1 I5_0 I1_1 I1_0 (i1537LL)		; 48-50		n 68
	vshuff64x2 zmm4, zmm4, zmm8, 11011101b	;; I7_3 I7_2 I3_3 I3_2 I5_3 I5_2 I1_3 I1_2 (i1537LH)		; 49-51		n 66

	vaddpd	zmm11, zmm15, zmm6		;; add r4/r8 4-aparts (R48+)					; 48-51		n 60
	vsubpd	zmm15, zmm15, zmm6		;; sub r4/r8 4-aparts (R48-)					; 49-52		n 62

	vshuff64x2 zmm8, zmm5, zmm13, 10001000b	;; I7_5 I7_4 I3_5 I3_4 I5_5 I5_4 I1_5 I1_4 (i1537HL)		; 50-52		n 70
	vshuff64x2 zmm5, zmm5, zmm13, 11011101b	;; I7_7 I7_6 I3_7 I3_6 I5_7 I5_6 I1_7 I1_6 (i1537HH)		; 51-53		n 64

	vblendmpd zmm26 {k6}, zmm22, zmm30	;; 1, 1, 1, 1, 1, 1, 1		.924/.383			; 50		n 68
	vblendmpd zmm27 {k6}, zmm22, zmm31	;; 1, 1, 1, 1, 1, 1, 1		SQRTHALF			; 51		n 87

	vshuff64x2 zmm13, zmm0, zmm12, 10001000b;; I8_1 I8_0 I4_1 I4_0 I6_1 I6_0 I2_1 I2_0 (i2648LL)		; 52-54		n 68
	vshuff64x2 zmm0, zmm0, zmm12, 11011101b	;; I8_3 I8_2 I4_3 I4_2 I6_3 I6_2 I2_3 I2_2 (i2648LH)		; 53-55		n 66

	vmovapd zmm25 {k7}{z}, zmm22		;; 1, 1, 1, 1, 1, 1, 1		0				; 52		n 53
	vsubpd	zmm25 {k6}, zmm25, zmm31	;; 1, 1, 1, 1, 1, 1, 1		-SQRTHALF			; 53		n 86

	vshuff64x2 zmm12, zmm10, zmm2, 10001000b;; I8_5 I8_4 I4_5 I4_4 I6_5 I6_4 I2_5 I2_4 (i2648HL)		; 54-56		n 70
	vshuff64x2 zmm10, zmm10, zmm2, 11011101b;; I8_7	I8_6 I4_7 I4_6 I6_7 I6_6 I2_7 I2_6 (i2648HH)		; 55-57		n 64

	vblendmpd zmm28 {k6}, zmm31, zmm29	;; SQRTHALF, SQRTHALF, ...,	.383				; 54		n 96

	vshuff64x2 zmm2, zmm14, zmm9, 10001000b	;; R7_1 R7_0 R3_1 R3_0 R5_1 R5_0 R1_1 R1_0 (r1537LL+)		; 56-58		n 76
	vshuff64x2 zmm14, zmm14, zmm9, 11011101b;; R7_3 R7_2 R3_3 R3_2 R5_3 R5_2 R1_3 R1_2 (r1537LH+)		; 57-59		n 78

	vshuff64x2 zmm9, zmm19, zmm1, 10001000b	;; R7_5 R7_4 R3_5 R3_4 R5_5 R5_4 R1_5 R1_4 (r1537HL-)		; 58-60		n 72
	vshuff64x2 zmm19, zmm19, zmm1, 11011101b;; R7_7 R7_6 R3_7 R3_6 R5_7 R5_6 R1_7 R1_6 (r1537HH-)		; 59-61		n 74

	vshuff64x2 zmm1, zmm7, zmm11, 10001000b	;; R8_1 R8_0 R4_1 R4_0 R6_1 R6_0 R2_1 R2_0 (r2648LL+)		; 60-62		n 76
	vshuff64x2 zmm7, zmm7, zmm11, 11011101b	;; R8_3 R8_2 R4_3 R4_2 R6_3 R6_2 R2_3 R2_2 (r2648LH+)		; 61-63		n 78

	vshuff64x2 zmm11, zmm3, zmm15, 10001000b;; R8_5 R8_4 R4_5 R4_4 R6_5 R6_4 R2_5 R2_4 (r2648HL-)		; 62-64		n 72
	vshuff64x2 zmm3, zmm3, zmm15, 11011101b	;; R8_7 R8_6 R4_7 R4_6 R6_7 R6_6 R2_7 R2_6 (r2648HH-)		; 63-65		n 74

	;; Swap the one aparts
	vshufpd	zmm15, zmm5, zmm10, 11111111b	;; I8_7	I7_7 I4_7 I3_7 I6_7 I5_7 I2_7 I1_7 (first I8 & R16)	; 64		n 68
	vshufpd	zmm5, zmm5, zmm10, 00000000b	;; I8_6	I7_6 I4_6 I3_6 I6_6 I5_6 I2_6 I1_6 (first I7 & R15)	; 65		n 70

	vshufpd	zmm10, zmm4, zmm0, 11111111b	;; I8_3 I7_3 I4_3 I3_3 I6_3 I5_3 I2_3 I1_3 (first I4 & R12)	; 66		n 68
	vshufpd	zmm4, zmm4, zmm0, 00000000b	;; I8_2 I7_2 I4_2 I3_2 I6_2 I5_2 I2_2 I1_2 (first I3 & R11)	; 67		n 70

	vshufpd	zmm0, zmm16, zmm13, 11111111b	;; I8_1 I7_1 I4_1 I3_1 I6_1 I5_1 I2_1 I1_1 (first I2 & R10)	; 68		n 72
	vshufpd	zmm16, zmm16, zmm13, 00000000b	;; I8_0 I7_0 I4_0 I3_0 I6_0 I5_0 I2_0 I1_0 (first I1 & R9)	; 69		n 74

						;; Simultaneously do an 8-complex and 16-reals
						;; 8-complex			16-reals
						;;				R12/R16 becomes newer R12/I12
						;;				mul R12/I12 by w^3 = .383 + .924i
	zfnmaddpd zmm6, zmm15, zmm26, zmm10	;; I4 - I8*1 (new I8)		R12/.383 = R12 - I12*.924/.383	; 68-71		n 76
	zfmaddpd zmm10, zmm10, zmm26, zmm15	;; I4*1 + I8 (new I4)		I12/.383 = R12*.924/.383 + I12	; 69-72		n 81

	vshufpd	zmm13, zmm8, zmm12, 11111111b	;; I8_5 I7_5 I4_5 I3_5 I6_5 I5_5 I2_5 I1_5 (first I6 & R14)	; 70		n 72
	vshufpd	zmm8, zmm8, zmm12, 00000000b	;; I8_4 I7_4 I4_4 I3_4 I6_4 I5_4 I2_4 I1_4 (first I5 & R13)	; 71		n 74

						;;				R11/R15 becomes newer R11/I11
						;;				mul R11/I11 by SQRTHALF + i*SQRTHALF
	vaddpd	zmm15, zmm4, zmm5		;; I3 + I7 (new I3)		I11/SQRTHALF = R11 + I11	; 70-73		n 84
	vsubpd	zmm4, zmm4, zmm5		;; I3 - I7 (new I7)		R11/SQRTHALF = R11 - I11	; 71-74		n 86

	vshufpd	zmm12, zmm9, zmm11, 11111111b	;; R8_5 R7_5 R4_5 R3_5 R6_5 R5_5 R2_5 R1_5 (R2-R6 = new R6)	; 72		n 78
	vshufpd	zmm9, zmm9, zmm11, 00000000b	;; R8_4 R7_4 R4_4 R3_4 R6_4 R5_4 R2_4 R1_4 (R1-R5 = new R5)	; 73		n 84

						;;				R10/R14 becomes newer R10/I10
						;;				mul R10/I10 by w^1 = .924 + .383i
	zfmsubpd zmm5, zmm0, zmm26, zmm13	;; I2*1 - I6 (new I6)		R10/.383 = R10*.924/.383 - I10	; 72-75		n 80
	zfmaddpd zmm0, zmm13, zmm26, zmm0	;; I2 + I6*1 (new I2)		I10/.383 = I10*.924/.383 + R10	; 73-76		n 81

	vshufpd	zmm11, zmm19, zmm3, 11111111b	;; R8_7	R7_7 R4_7 R3_7 R6_7 R5_7 R2_7 R1_7 (R4-R8 = new R8)	; 74		n 76
	vshufpd	zmm19, zmm19, zmm3, 00000000b	;; R8_6	R7_6 R4_6 R3_6 R6_6 R5_6 R2_6 R1_6 (R3-R7 = new R7)	; 75		n 84

						;;				R9/R13 becomes newer R9/I9
	vaddpd	zmm13 {k7}{z}, zmm16, zmm8	;; I1 + I5 (new I1)		0				; 74-77		n 85
	vsubpd	zmm8 {k7}, zmm16, zmm8		;; I1 - I5 (new I5)		I9				; 75-78		n 87

	vshufpd	zmm3, zmm2, zmm1, 11111111b	;; R8_1 R7_1 R4_1 R3_1 R6_1 R5_1 R2_1 R1_1 (R2+R6=  new R2)	; 76		n 83
	vshufpd	zmm2, zmm2, zmm1, 00000000b	;; R8_0 R7_0 R4_0 R3_0 R6_0 R5_0 R2_0 R1_0 (R1+R5 = new R1)	; 77		n 82

						;; next FFT level
						;;				R6/R8 becomes newest R6/I6
						;;				mul R6/I6 by w^2 = .707 + .707i
	vblendmpd zmm18 {k6}, zmm6, zmm11	;; I8				I6				; 76		n 78
	vblendmpd zmm11 {k6}, zmm11, zmm6	;; R8				R12				; 77		n 80

	vshufpd	zmm1, zmm14, zmm7, 11111111b	;; R8_3 R7_3 R4_3 R3_3 R6_3 R5_3 R2_3 R1_3 (R4+R8 = new R4)	; 78		n 83
	vshufpd	zmm14, zmm14, zmm7, 00000000b	;; R8_2 R7_2 R4_2 R3_2 R6_2 R5_2 R2_2 R1_2 (R3+R7 = new R3)	; 79		n 82

	vsubpd	zmm17, zmm12, zmm18		;; R6 - I8 (new2 R6)		R6 - I6 (newest R6/SQRTHALF)	; 78-12		n 88
	vaddpd	zmm12, zmm12, zmm18		;; R6 + I8 (new2 R8)		R6 + I6 (newest I6/SQRTHALF)	; 79-12		n 90

	vaddpd	zmm6, zmm5, zmm11		;; I6 + R8 (new2 I6)		R10 + R12 (newest R10/.383)	; 80-13		n 88
	vsubpd	zmm5, zmm5, zmm11		;; I6 - R8 (new2 I8)		R10 - R12 (newest R12/.383)	; 80-13		n 90

	vaddpd	zmm11, zmm0, zmm10		;; I2 + I4 (newer I2)		I10 + I12 (newest I10/.383)	; 81-14		n 88
	vsubpd	zmm0, zmm0, zmm10		;; I2 - I4 (newer I4)		I10 - I12 (newest I12/.383)	; 81-14		n 89

	vaddpd	zmm10, zmm2, zmm14		;; R1 + R3 (newer R1)		R1 + R3 (newest R1)		; 82-15		n 89
	vsubpd	zmm2, zmm2, zmm14		;; R1 - R3 (newer R3)		R1 - R3 (newest R3)		; 82-15		n 91

	vaddpd	zmm14, zmm3, zmm1		;; R2 + R4 (newer R2)		R2 + R4 (newest R2)		; 83-86		n 89
	vsubpd	zmm3, zmm3, zmm1		;; R2 - R4 (newer R4)		R2 - R4 (newest R4)		; 83-16		n 91

	vblendmpd zmm18 {k6}, zmm19, zmm15	;; R7				I11/SQRTHALF			; 84		n 87
	vblendmpd zmm16 {k6}, zmm9, zmm16	;; R5				R9				; 84		n 86

						;;				R5/R7 becomes newest R5/I5
	vsubpd	zmm19 {k7}, zmm13, zmm15	;; I1 - I3 (newer I3)		I5				; 85-17		n 95
	vaddpd	zmm13 {k7}{z}, zmm13, zmm15	;; I1 + I3 (newer I1)		0				; 85-17		n 93

	zfnmaddpd zmm7, zmm4, zmm25, zmm16	;; R5 - I7*1 (newer R5)		R9 - R11*-SQRTHALF (newest R9)	; 86-18		n 96
	zfmaddpd zmm4, zmm4, zmm25, zmm16	;; R5 + I7*1 (newer R7)		R9 + R11*-SQRTHALF (newest R11)	; 86-18		n 98

	zfmaddpd zmm16, zmm18, zmm27, zmm8	;; I5 + R7*1 (newer I5)		I9 + I11*SQRTHALF (newest I9)	; 87-19		n 97
	zfnmaddpd zmm18, zmm18, zmm27, zmm8	;; I5 - R7*1 (newer I7)		I9 - I11*SQRTHALF (newest I11)	; 87-19		n 99

	vmovapd	zmm8, zmm11			;;				I10/.383
	vaddpd	zmm8 {k7}, zmm17, zmm6		;; I6 = R6 + I6 (newer I6/SQRTHALF)	I10/.383		; 88-91		n 97
	vsubpd	zmm6 {k7}, zmm17, zmm6		;; R6 = R6 - I6 (newer R6/SQRTHALF)	R10/.383		; 88-91		n 96

	vblendmpd zmm17 {k6}, zmm0, zmm17	;; I4				R6/SQRTHALF			; 89		n 94
	vsubpd	zmm11 {k6}, zmm10, zmm14	;; I2				R1 - R2 (final R1b)		; 89-92		n 93

	vaddpd	zmm0 {k7}, zmm12, zmm5		;; I8 = R8 + I8 (newer I8/SQRTHALF)	I12/.383		; 90-92		n 98
	vsubpd	zmm5 {k7}, zmm12, zmm5		;; R8 = R8 - I8 (newer R8/SQRTHALF)	R12/.383		; 90-92		n 99

	;; last FFT level

	vblendmpd zmm9 {k6}, zmm2, zmm9		;; R3				R5				; 91		n 94
	vblendmpd zmm12 {k6}, zmm3, zmm12	;; R4				I6/SQRTHALF			; 91		n 95

						;;				R3/R4 becomes final R3/I3
	vaddpd	zmm1, zmm10, zmm14		;; R1 + R2 (final R1)		R1 + R2 (final R1a => R1)	; 92-95		n 
	vsubpd	zmm2 {k7}, zmm10, zmm14		;; R1 - R2 (final R2)		R3 => R2			; 92-95		n 

	vsubpd	zmm3 {k7}, zmm13, zmm11		;; I1 - I2 (final I2)		I3 => I2			; 93-96		n 
	vaddpd	zmm11 {k7}, zmm13, zmm11	;; I1 + I2 (final I1)		R1b => I1			; 93-96		n 

	zfnmaddpd zmm13, zmm17, zmm25, zmm9	;; R3 - I4*1 (final R3)		R5 - R6*-SQRTHALF (final R5 => R3) ; 94-97	n 
	zfmaddpd zmm9, zmm17, zmm25, zmm9	;; R3 + I4*1 (final R4)		R5 + R6*-SQRTHALF (final R6 => R4) ; 94-97	n 

	zfmaddpd zmm15, zmm12, zmm27, zmm19	;; I3 + R4*1 (final I3)		I5 + I6*SQRTHALF (final I5 => I3) ; 95-98	n 
	zfnmaddpd zmm12, zmm12, zmm27, zmm19	;; I3 - R4*1 (final I4)		I5 - I6*SQRTHALF (final I6 => I4) ; 95-98	n 
	vbroadcastsd zmm27, ZMM_TWO

	zfmaddpd zmm10, zmm6, zmm28, zmm7	;; R5 + R6*SQRTHALF (final R5)	R9 + R10*.383 (final R9 => R5)	; 96-99		n 
	zfnmaddpd zmm6, zmm6, zmm28, zmm7	;; R5 - R6*SQRTHALF (final R6)	R9 - R10*.383 (final R10 => R6)	; 96-99		n 

	zfmaddpd zmm7, zmm8, zmm28, zmm16	;; I5 + I6*SQRTHALF (final I5)	I9 + I10*.383 (final I9 => I5)	; 97-100	n 
	zfnmaddpd zmm8, zmm8, zmm28, zmm16	;; I5 - I6*SQRTHALF (final I6)	I9 - I10*.383 (final I10 => I6)	; 97-100	n 

	zfnmaddpd zmm14, zmm0, zmm28, zmm4	;; R7 - I8*SQRTHALF (final R7)	R11 - I12*.383 (final R11 => R7); 98-101	n 
	zfmaddpd zmm0, zmm0, zmm28, zmm4	;; R7 + I8*SQRTHALF (final R8)	R11 + I12*.383 (final R12 => R8); 98-101	n 

	zfmaddpd zmm4, zmm5, zmm28, zmm18	;; I7 + R8*SQRTHALF (final I7)	I11 + R12*.383 (final I11 => I7); 99-102	n 
	zfnmaddpd zmm5, zmm5, zmm28, zmm18	;; I7 - R8*SQRTHALF (final I8)	I11 - R12*.383 (final I12 => I8); 99-102	n 

	mov	al, mul4_opcode			;; Load the mul4_opcode
	cmp	al, 0				;; See if we need to do more than the original type-2 FFT multiply
	je	orig				;; Jump if nothing special
	mov	r9, SRC2ARG			;; Distance to s3 arg in gwmuladd4 or gwmulsub4
	mov	r10, SRC3ARG			;; Load distance to FFT(1)
	jg	short nosave			;; Jump if not saving result of the forward FFT?

	;; Store FFT results
	zstore	[srcreg], zmm1			;; R1			R1a
	zstore	[srcreg+64], zmm11		;; I1			R1b
	zstore	[srcreg+d1], zmm2		;;		R2
	zstore	[srcreg+d1+64], zmm3		;;		I2
	zstore	[srcreg+d2], zmm13		;;		R3
	zstore	[srcreg+d2+64], zmm15		;;		I3
	zstore	[srcreg+d2+d1], zmm9		;;		R4
	zstore	[srcreg+d2+d1+64], zmm12	;;		I4
	zstore	[srcreg+d4], zmm10		;;		R5
	zstore	[srcreg+d4+64], zmm7		;;		I5
	zstore	[srcreg+d4+d1], zmm6		;;		R6
	zstore	[srcreg+d4+d1+64], zmm8		;;		I6
	zstore	[srcreg+d4+d2], zmm14		;;		R7
	zstore	[srcreg+d4+d2+64], zmm4		;;		I7
	zstore	[srcreg+d4+d2+d1], zmm0		;;		R8
	zstore	[srcreg+d4+d2+d1+64], zmm5	;;		I8
	and	al, 7Fh				;; Strip "save bit" from mul4_opcode
	jz	orig				;; 0=no opcode, goto original squaring code

nosave:
	;; Square the results
	vmulpd	zmm23, zmm1, zmm1		;; A1 = R1 * R1			R1a*R1a (R1a)			; 1-4		n 5
	vmulpd	zmm17, zmm2, zmm2		;;		A2 = R2 * R2					; 1-4		n 5

	vmulpd	zmm19, zmm13, zmm13		;;		A3 = R3 * R3					; 2-5		n 8
	vmulpd	zmm21, zmm9, zmm9		;;		A4 = R4 * R4					; 2-5		n 8

	vmulpd	zmm22, zmm10, zmm10		;;		A5 = R5 * R5					; 3-6		n 9
	vmulpd	zmm18, zmm6, zmm6		;;		A6 = R6 * R6					; 3-6		n 10

	vmulpd	zmm16, zmm14, zmm14		;;		A7 = R7 * R7					; 4-7		n 12
	vmulpd	zmm20, zmm0, zmm0		;;		A8 = R8 * R8					; 4-7		n 12

	zfnmaddpd zmm23 {k7}, zmm11, zmm11, zmm23;; A1 - I1 * I1 (R1)		R1a				; 5-8		n 13
	zfnmaddpd zmm17, zmm3, zmm3, zmm17	;;		A2 - I2 * I2 (R2)				; 5-8		n 9
	vmovsd	Q [srcreg+r8-16], xmm23		;;				Save square of FFT sums		; 5

	vmulpd	zmm9, zmm12, zmm9		;;		B4 = I4 * R4 (I4/2)				; 6-9		n 10
	vmulpd	zmm0, zmm5, zmm0		;;		B8 = I8 * R8 (I8/2)				; 6-9		n 13

	vmulpd	zmm2, zmm3, zmm2		;;		I2 * R2 (I2/2)					; 7-10		n 14
	vmulpd	zmm13, zmm15, zmm13		;;		I3 * R3 (I3/2 & I5/2)				; 7-10		n 16

	zfnmaddpd zmm19, zmm15, zmm15, zmm19	;;		A3 - I3 * I3 (R3 & R5)				; 8-11		n 
	zfnmaddpd zmm21, zmm12, zmm12, zmm21	;;		A4 - I4 * I4 (R4 & R6)				; 8-11		n 

	vmulpd	zmm11 {k7}, zmm11, zmm1		;; I1*R1 (I1/2)			 R1b

	zfnmaddpd zmm22, zmm7, zmm7, zmm22	;;		A5 - I5 * I5 (R5 & R9)				; 9-12		n 
	zfnmaddpd zmm18, zmm8, zmm8, zmm18	;;		A6 - I6 * I6 (R6 & R10)				; 10-13		n 

	vmulpd	zmm7, zmm7, zmm10		;;		I5 * R5 (I5/2 & I9/2)
	vmulpd	zmm6, zmm8, zmm6 		;;		I6 * R6 (I6/2 & I10/2)				; 11-12		n 
	vmulpd	zmm14, zmm4, zmm14		;;		I7 * R7 (I7/2 & I11/2)				; 11-15		n 

	zfnmaddpd zmm16, zmm4, zmm4, zmm16	;;		A7 - I7 * I7 (R7 & R11)				; 12-18		n 
	zfnmaddpd zmm20, zmm5, zmm5, zmm20	;;		A8 - I8 * I8 (R8 & R12)				; 12-20		n 

	vbroadcastsd zmm10, ZMM_HALF
	vbroadcastsd zmm1, ZMM_TWO

	cmp	al, 4				;; Case off opcode
	;jb	muladd				;; 3=muladd, fall through
	je	mulsub				;; 4=mulsub

muladd:	cmp	r10, 1				;; Check if we need to multiply add-in by FFT(1)
	jne	muladdhard			;; Yes, do it the hard way

	vaddpd	zmm17, zmm17, [srcreg+r9+d1]			;; R2 = R2 + MemR2
	zfmaddpd zmm2, zmm10, [srcreg+r9+d1+64], zmm2		;; I2/2 = I2/2 + MemI2/2

	vaddpd	zmm19, zmm19, [srcreg+r9+d2]			;; R3 = R3 + MemR3
	zfmaddpd zmm13, zmm10, [srcreg+r9+d2+64], zmm13		;; I3/2 = I3/2 + MemI3/2

	vaddpd	zmm21, zmm21, [srcreg+r9+d2+d1]			;; R4 = R4 + MemR4
	zfmaddpd zmm9, zmm9, zmm1, [srcreg+r9+d2+d1+64]		;; I4 = B4 * 2 + MemI4

	vmovapd	zmm12, zmm17					;;		Copy R2
	vmulpd	zmm12 {k6}, zmm11, zmm11			;; R2				R1b * R1b (R1b)

	vaddpd	zmm23, zmm23, [srcreg+r9]			;; R1 = R1 + MemR1
	zfmaddpd zmm11, zmm10, [srcreg+r9+64], zmm11		;; I1/2 = I1/2 + MemI1/2	undefined
	vaddpd zmm12 {k6}, zmm12, [srcreg+r9+64]		;; R2				R1b = R1b + MemI1

	vaddpd	zmm22, zmm22, [srcreg+r9+d4]			;; R5 = R5 + MemR5
	zfmaddpd zmm7, zmm10, [srcreg+r9+d4+64], zmm7		;; I5/2 = I5/2 + MemI5/2

	vaddpd	zmm18, zmm18, [srcreg+r9+d4+d1]			;; R6 = R6 + MemR6
	zfmaddpd zmm6, zmm10, [srcreg+r9+d4+d1+64], zmm6	;; I6/2 = I6/2 + MemI6/2

	vaddpd	zmm16, zmm16, [srcreg+r9+d4+d2]			;; R7 = R7 + MemR7
	zfmaddpd zmm14, zmm10, [srcreg+r9+d4+d2+64], zmm14	;; I7/2 = I7/2 + MemI7/2

	vaddpd	zmm20, zmm20, [srcreg+r9+d4+d2+d1]		;; R8 = R8 + MemR8
	zfmaddpd zmm0, zmm0, zmm1, [srcreg+r9+d4+d2+d1+64]	;; I8 = B8 * 2 + MemI8

	jmp	fma_done

muladdhard:
	vmovapd	zmm3, [srcreg+r9+d1]				;; MemR2
	vmovapd	zmm4, [srcreg+r10+d1]				;; MemR2#2
	zfmaddpd zmm17, zmm3, zmm4, zmm17			;; R2 = R2 + MemR2*MemR2#2
	vmovapd	zmm5, [srcreg+r10+d1+64]			;; MemI2#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR2*MemI2#2
	vmovapd	zmm8, [srcreg+r9+d1+64]				;; MemI2
	zfnmaddpd zmm17, zmm8, zmm5, zmm17			;; R2 = R2 - MemI2*MemI2#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI2*MemR2#2
	zfmaddpd zmm2, zmm10, zmm3, zmm2			;; I2/2 = I2/2 + TMP/2

	vmovapd	zmm3, [srcreg+r9+d2]				;; MemR3
	vmovapd	zmm4, [srcreg+r10+d2]				;; MemR3#2
	zfmaddpd zmm19, zmm3, zmm4, zmm19			;; R3 = R3 + MemR3*MemR3#2
	vmovapd	zmm5, [srcreg+r10+d2+64]			;; MemI3#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR3*MemI3#2
	vmovapd	zmm8, [srcreg+r9+d2+64]				;; MemI3
	zfnmaddpd zmm19, zmm8, zmm5, zmm19			;; R3 = R3 - MemI3*MemI3#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI3*MemR3#2
	zfmaddpd zmm13, zmm10, zmm3, zmm13			;; I3/2 = I3/2 + TMP/2

	vmovapd	zmm3, [srcreg+r9+d2+d1]				;; MemR4
	vmovapd	zmm4, [srcreg+r10+d2+d1]			;; MemR4#2
	zfmaddpd zmm21, zmm3, zmm4, zmm21			;; R4 = R4 + MemR4*MemR4#2
	vmovapd	zmm5, [srcreg+r10+d2+d1+64]			;; MemI4#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR4*MemI4#2
	vmovapd	zmm8, [srcreg+r9+d2+d1+64]			;; MemI4
	zfnmaddpd zmm21, zmm8, zmm5, zmm21			;; R4 = R4 - MemI4*MemI4#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI4*MemR4#2
	zfmaddpd zmm9, zmm9, zmm1, zmm3				;; I4 = B4 * 2 + TMP

	vmovapd	zmm12, zmm17					;;		Copy R2
	vmulpd	zmm12 {k6}, zmm11, zmm11			;; R2				R1b * R1b (R1b)

	vmovapd	zmm3, [srcreg+r9]				;; MemR1
	vmovapd	zmm4, [srcreg+r10]				;; MemR1#2
	zfmaddpd zmm23, zmm3, zmm4, zmm23			;; R1 = R1 + MemR1*MemR1#2
	vmovapd	zmm5, [srcreg+r10+64]				;; MemI1#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR1*MemI1#2
	vmovapd	zmm8, [srcreg+r9+64]				;; MemI1
	zfnmaddpd zmm23{k7}, zmm8, zmm5, zmm23			;; R1 = R1 - MemI1*MemI1#2	R1
	zfmaddpd zmm12{k6}, zmm8, zmm5, zmm12			;; R2				R1b = R1b + MemI1*MemI1#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI1*MemR1#2
	zfmaddpd zmm11, zmm10, zmm3, zmm11			;; I1/2 = I1/2 + TMP/2		undefined

	vmovapd	zmm3, [srcreg+r9+d4]				;; MemR5
	vmovapd	zmm4, [srcreg+r10+d4]				;; MemR5#2
	zfmaddpd zmm22, zmm3, zmm4, zmm22			;; R5 = R5 + MemR5*MemR5#2
	vmovapd	zmm5, [srcreg+r10+d4+64]			;; MemI5#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR5*MemI5#2
	vmovapd	zmm8, [srcreg+r9+d4+64]				;; MemI5
	zfnmaddpd zmm22, zmm8, zmm5, zmm22			;; R5 = R5 - MemI5*MemI5#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI5*MemR5#2
	zfmaddpd zmm7, zmm10, zmm3, zmm7			;; I5/2 = I5/2 + TMP/2

	vmovapd	zmm3, [srcreg+r9+d4+d1]				;; MemR6
	vmovapd	zmm4, [srcreg+r10+d4+d1]			;; MemR6#2
	zfmaddpd zmm18, zmm3, zmm4, zmm18			;; R6 = R6 + MemR6*MemR6#2
	vmovapd	zmm5, [srcreg+r10+d4+d1+64]			;; MemI6#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR6*MemI6#2
	vmovapd	zmm8, [srcreg+r9+d4+d1+64]			;; MemI6
	zfnmaddpd zmm18, zmm8, zmm5, zmm18			;; R6 = R6 - MemI6*MemI6#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI6*MemR6#2
	zfmaddpd zmm6, zmm10, zmm3, zmm6			;; I6/2 = I6/2 + TMP/2

	vmovapd	zmm3, [srcreg+r9+d4+d2]				;; MemR7
	vmovapd	zmm4, [srcreg+r10+d4+d2]			;; MemR7#2
	zfmaddpd zmm16, zmm3, zmm4, zmm16			;; R7 = R7 + MemR7*MemR7#2
	vmovapd	zmm5, [srcreg+r10+d4+d2+64]			;; MemI7#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR7*MemI7#2
	vmovapd	zmm8, [srcreg+r9+d4+d2+64]			;; MemI7
	zfnmaddpd zmm16, zmm8, zmm5, zmm16			;; R7 = R7 - MemI7*MemI7#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI7*MemR7#2
	zfmaddpd zmm14, zmm10, zmm3, zmm14			;; I7/2 = I7/2 + TMP/2

	vmovapd	zmm3, [srcreg+r9+d4+d2+d1]			;; MemR8
	vmovapd	zmm4, [srcreg+r10+d4+d2+d1]			;; MemR8#2
	zfmaddpd zmm20, zmm3, zmm4, zmm20			;; R8 = R8 + MemR8*MemR8#2
	vmovapd	zmm5, [srcreg+r10+d4+d2+d1+64]			;; MemI8#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR8*MemI8#2
	vmovapd	zmm8, [srcreg+r9+d4+d2+d1+64]			;; MemI8
	zfnmaddpd zmm20, zmm8, zmm5, zmm20			;; R8 = R8 - MemI8*MemI8#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI8*MemR8#2
	zfmaddpd zmm0, zmm0, zmm1, zmm3				;; I8 = B8 * 2 + TMP

	jmp	fma_done

mulsub:	cmp	r10, 1				;; Check if we need to multiply add-in by FFT(1)
	jne	mulsubhard			;; Yes, do it the hard way

	vsubpd	zmm17, zmm17, [srcreg+r9+d1]			;; R2 = R2 - MemR2
	zfnmaddpd zmm2, zmm10, [srcreg+r9+d1+64], zmm2		;; I2/2 = I2/2 - MemI2/2

	vsubpd	zmm19, zmm19, [srcreg+r9+d2]			;; R3 = R3 - MemR3
	zfnmaddpd zmm13, zmm10, [srcreg+r9+d2+64], zmm13	;; I3/2 = I3/2 - MemI3/2

	vsubpd	zmm21, zmm21, [srcreg+r9+d2+d1]			;; R4 = R4 - MemR4
	zfmsubpd zmm9, zmm9, zmm1, [srcreg+r9+d2+d1+64]		;; I4 = B4 * 2 - MemI4

	vmovapd	zmm12, zmm17					;;		Copy R2
	vmulpd	zmm12 {k6}, zmm11, zmm11			;; R2				R1b * R1b (R1b)

	vsubpd	zmm23, zmm23, [srcreg+r9]			;; R1 = R1 - MemR1
	zfnmaddpd zmm11, zmm10, [srcreg+r9+64], zmm11		;; I1/2 = I1/2 - MemI1/2	undefined
	vsubpd zmm12 {k6}, zmm12, [srcreg+r9+64]		;; R2				R1b = R1b - MemI1

	vsubpd	zmm22, zmm22, [srcreg+r9+d4]			;; R5 = R5 - MemR5
	zfnmaddpd zmm7, zmm10, [srcreg+r9+d4+64], zmm7		;; I5/2 = I5/2 - MemI5/2

	vsubpd	zmm18, zmm18, [srcreg+r9+d4+d1]			;; R6 = R6 - MemR6
	zfnmaddpd zmm6, zmm10, [srcreg+r9+d4+d1+64], zmm6	;; I6/2 = I6/2 - MemI6/2

	vsubpd	zmm16, zmm16, [srcreg+r9+d4+d2]			;; R7 = R7 - MemR7
	zfnmaddpd zmm14, zmm10, [srcreg+r9+d4+d2+64], zmm14	;; I7/2 = I7/2 - MemI7/2

	vsubpd	zmm20, zmm20, [srcreg+r9+d4+d2+d1]		;; R8 = R8 - MemR8
	zfmsubpd zmm0, zmm0, zmm1, [srcreg+r9+d4+d2+d1+64]	;; I8 = B8 * 2 - MemI8

	jmp	fma_done

mulsubhard:
	vmovapd	zmm3, [srcreg+r9+d1]				;; MemR2
	vmovapd	zmm4, [srcreg+r10+d1]				;; MemR2#2
	zfnmaddpd zmm17, zmm3, zmm4, zmm17			;; R2 = R2 - MemR2*MemR2#2
	vmovapd	zmm5, [srcreg+r10+d1+64]			;; MemI2#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR2*MemI2#2
	vmovapd	zmm8, [srcreg+r9+d1+64]				;; MemI2
	zfmaddpd zmm17, zmm8, zmm5, zmm17			;; R2 = R2 + MemI2*MemI2#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI2*MemR2#2
	zfnmaddpd zmm2, zmm10, zmm3, zmm2			;; I2/2 = I2/2 - TMP/2

	vmovapd	zmm3, [srcreg+r9+d2]				;; MemR3
	vmovapd	zmm4, [srcreg+r10+d2]				;; MemR3#2
	zfnmaddpd zmm19, zmm3, zmm4, zmm19			;; R3 = R3 - MemR3*MemR3#2
	vmovapd	zmm5, [srcreg+r10+d2+64]			;; MemI3#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR3*MemI3#2
	vmovapd	zmm8, [srcreg+r9+d2+64]				;; MemI3
	zfmaddpd zmm19, zmm8, zmm5, zmm19			;; R3 = R3 + MemI3*MemI3#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI3*MemR3#2
	zfnmaddpd zmm13, zmm10, zmm3, zmm13			;; I3/2 = I3/2 - TMP/2

	vmovapd	zmm3, [srcreg+r9+d2+d1]				;; MemR4
	vmovapd	zmm4, [srcreg+r10+d2+d1]			;; MemR4#2
	zfnmaddpd zmm21, zmm3, zmm4, zmm21			;; R4 = R4 - MemR4*MemR4#2
	vmovapd	zmm5, [srcreg+r10+d2+d1+64]			;; MemI4#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR4*MemI4#2
	vmovapd	zmm8, [srcreg+r9+d2+d1+64]			;; MemI4
	zfmaddpd zmm21, zmm8, zmm5, zmm21			;; R4 = R4 + MemI4*MemI4#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI4*MemR4#2
	zfmsubpd zmm9, zmm9, zmm1, zmm3				;; I4 = B4 * 2 - TMP

	vmovapd	zmm12, zmm17					;;		Copy R2
	vmulpd	zmm12 {k6}, zmm11, zmm11			;; R2				R1b * R1b (R1b)

	vmovapd	zmm3, [srcreg+r9]				;; MemR1
	vmovapd	zmm4, [srcreg+r10]				;; MemR1#2
	zfnmaddpd zmm23, zmm3, zmm4, zmm23			;; R1 = R1 - MemR1*MemR1#2
	vmovapd	zmm5, [srcreg+r10+64]				;; MemI1#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR1*MemI1#2
	vmovapd	zmm8, [srcreg+r9+64]				;; MemI1
	zfmaddpd zmm23{k7}, zmm8, zmm5, zmm23			;; R1 = R1 + MemI1*MemI1#2	R1
	zfnmaddpd zmm12{k6}, zmm8, zmm5, zmm12			;; R2				R1b = R1b - MemI1*MemI1#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI1*MemR1#2
	zfnmaddpd zmm11, zmm10, zmm3, zmm11			;; I1/2 = I1/2 - TMP/2		undefined

	vmovapd	zmm3, [srcreg+r9+d4]				;; MemR5
	vmovapd	zmm4, [srcreg+r10+d4]				;; MemR5#2
	zfnmaddpd zmm22, zmm3, zmm4, zmm22			;; R5 = R5 - MemR5*MemR5#2
	vmovapd	zmm5, [srcreg+r10+d4+64]			;; MemI5#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR5*MemI5#2
	vmovapd	zmm8, [srcreg+r9+d4+64]				;; MemI5
	zfmaddpd zmm22, zmm8, zmm5, zmm22			;; R5 = R5 + MemI5*MemI5#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI5*MemR5#2
	zfnmaddpd zmm7, zmm10, zmm3, zmm7			;; I5/2 = I5/2 - TMP/2

	vmovapd	zmm3, [srcreg+r9+d4+d1]				;; MemR6
	vmovapd	zmm4, [srcreg+r10+d4+d1]			;; MemR6#2
	zfnmaddpd zmm18, zmm3, zmm4, zmm18			;; R6 = R6 - MemR6*MemR6#2
	vmovapd	zmm5, [srcreg+r10+d4+d1+64]			;; MemI6#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR6*MemI6#2
	vmovapd	zmm8, [srcreg+r9+d4+d1+64]			;; MemI6
	zfmaddpd zmm18, zmm8, zmm5, zmm18			;; R6 = R6 + MemI6*MemI6#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI6*MemR6#2
	zfnmaddpd zmm6, zmm10, zmm3, zmm6			;; I6/2 = I6/2 - TMP/2

	vmovapd	zmm3, [srcreg+r9+d4+d2]				;; MemR7
	vmovapd	zmm4, [srcreg+r10+d4+d2]			;; MemR7#2
	zfnmaddpd zmm16, zmm3, zmm4, zmm16			;; R7 = R7 - MemR7*MemR7#2
	vmovapd	zmm5, [srcreg+r10+d4+d2+64]			;; MemI7#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR7*MemI7#2
	vmovapd	zmm8, [srcreg+r9+d4+d2+64]			;; MemI7
	zfmaddpd zmm16, zmm8, zmm5, zmm16			;; R7 = R7 + MemI7*MemI7#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI7*MemR7#2
	zfnmaddpd zmm14, zmm10, zmm3, zmm14			;; I7/2 = I7/2 - TMP/2

	vmovapd	zmm3, [srcreg+r9+d4+d2+d1]			;; MemR8
	vmovapd	zmm4, [srcreg+r10+d4+d2+d1]			;; MemR8#2
	zfnmaddpd zmm20, zmm3, zmm4, zmm20			;; R8 = R8 - MemR8*MemR8#2
	vmovapd	zmm5, [srcreg+r10+d4+d2+d1+64]			;; MemI8#2
	vmulpd	zmm3, zmm3, zmm5				;; TMP = MemR8*MemI8#2
	vmovapd	zmm8, [srcreg+r9+d4+d2+d1+64]			;; MemI8
	zfmaddpd zmm20, zmm8, zmm5, zmm20			;; R8 = R8 + MemI8*MemI8#2
	zfmaddpd zmm3, zmm8, zmm4, zmm3				;; TMP += MemI8*MemR8#2
	zfmsubpd zmm0, zmm0, zmm1, zmm3				;; I8 = B8 * 2 - TMP

fma_done:					;;				R1a/R1b becomes R1/R2
	vaddpd	zmm3, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 13-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 14-20		n 28

						;;				R2/(I2/2) becomes R3/(R4/2)
	vaddpd	zmm15, zmm11, zmm2		;; I1/2 + I2/2 (new I1/2)	undefined			; 14-47		n 25
	vsubpd	zmm2 {k7}, zmm11, zmm2		;; I1/2 - I2/2 (new I2/2)	R4/2				; 15-47		n 26

	vaddpd	zmm8, zmm21, zmm19		;; R4 + R3 (new R3)		R6 + R5 (new R5)		; 15-52		n 25
	vsubpd	zmm21, zmm21, zmm19		;; R4 - R3 (new I4)		R6 - R5 (new negR6)		; 16-52		n 33

	zfmaddpd zmm11, zmm13, zmm27, zmm9	;; I3*2 + I4 (new I3)		I5*2 + I6 (new I5)		; 16-53		n 27
	zfmsubpd zmm13, zmm13, zmm27, zmm9	;; I3*2 - I4 (new R4)		I5*2 - I6 (new I6)		; 17-53		n 26

	vaddpd	zmm4, zmm22, zmm18		;; R5 + R6 (new R5)		R9 + R10 (new R9)		; 17-48		n 34
	vsubpd	zmm22, zmm22, zmm18		;; R5 - R6 (new R6)		R9 - R10 (new R10)		; 18-48		n 29

	vaddpd	zmm1, zmm7, zmm6		;; I5/2 + I6/2 (new I5/2)	I9/2 + I10/2 (new I9/2)		; 18-49		n 35
	vsubpd	zmm6, zmm7, zmm6		;; I5/2 - I6/2 (new I6/2)	I9/2 - I10/2 (new I10/2)	; 19-49		n 

	jmp	back_to_orig
orig:
	;; Square the results
	vmulpd	zmm23, zmm1, zmm1		;; A1 = R1 * R1			R1a*R1a (R1a)			; 1-4		n 5
	vmulpd	zmm17, zmm2, zmm2		;;		A2 = R2 * R2					; 1-4		n 5

	vmulpd	zmm19, zmm13, zmm13		;;		A3 = R3 * R3					; 2-5		n 8
	vmulpd	zmm21, zmm9, zmm9		;;		A4 = R4 * R4					; 2-5		n 8

	vmulpd	zmm22, zmm10, zmm10		;;		A5 = R5 * R5					; 3-6		n 9
	vmulpd	zmm18, zmm6, zmm6		;;		A6 = R6 * R6					; 3-6		n 10

	vmulpd	zmm16, zmm14, zmm14		;;		A7 = R7 * R7					; 4-7		n 12
	vmulpd	zmm20, zmm0, zmm0		;;		A8 = R8 * R8					; 4-7		n 12

	zfnmaddpd zmm23 {k7}, zmm11, zmm11, zmm23;; A1 - I1 * I1 (R1)		R1a				; 5-8		n 13
	zfnmaddpd zmm17, zmm3, zmm3, zmm17	;;		A2 - I2 * I2 (R2)				; 5-8		n 9
	vmovsd	Q [srcreg+r8-16], xmm23		;;				Save square of FFT sums		; 5

	vmulpd	zmm9, zmm12, zmm9		;;		B4 = I4 * R4					; 6-9		n 10
	vmulpd	zmm0, zmm5, zmm0		;;		B8 = I8 * R8					; 6-9		n 13

	vmulpd	zmm2, zmm3, zmm2		;;		I2 * R2 (I2/2)					; 7-10		n 14
	vmulpd	zmm13, zmm15, zmm13		;;		I3 * R3 (I3/2 & I5/2)				; 7-10		n 16

	zfnmaddpd zmm19, zmm15, zmm15, zmm19	;;		A3 - I3 * I3 (R3 & R5)				; 8-11		n 
	zfnmaddpd zmm21, zmm12, zmm12, zmm21	;;		A4 - I4 * I4 (R4 & R6)				; 8-11		n 

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmulpd	zmm12 {k6}, zmm11, zmm11	;; R2				R1b * R1b (R1b)			; 9-12		n 13
	zfnmaddpd zmm22, zmm7, zmm7, zmm22	;;		A5 - I5 * I5 (R5 & R9)				; 9-12		n 

	vaddpd	zmm9, zmm9, zmm9		;;		B4 * 2 (I4 & I6)				; 10-13		n 
	zfnmaddpd zmm18, zmm8, zmm8, zmm18	;;		A6 - I6 * I6 (R6 & R10)				; 10-13		n 

	vmulpd	zmm6, zmm8, zmm6 		;;		I6 * R6 (I6/2 & I10/2)				; 11-12		n 
	vmulpd	zmm14, zmm4, zmm14		;;		I7 * R7 (I7/2 & I11/2)				; 11-15		n 

	zfnmaddpd zmm16, zmm4, zmm4, zmm16	;;		A7 - I7 * I7 (R7 & R11)				; 12-18		n 
	zfnmaddpd zmm20, zmm5, zmm5, zmm20	;;		A8 - I8 * I8 (R8 & R12)				; 12-20		n 

	vaddpd	zmm0, zmm0, zmm0		;;		B8 * 2 (I8 & I12)				; 13-17		n 
						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm3, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 13-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 14-20		n 28

						;;				R2/(I2/2) becomes R3/(R4/2)
	zfmaddpd zmm15, zmm11, zmm1, zmm2	;; origI1*origR1 + I2 (new I1/2) undefined			; 14-47		n 25
	zfmsubpd zmm2 {k7}, zmm11, zmm1, zmm2	;; origI1*origR1 - I2 (new I2/2) R4/2				; 15-47		n 26

	vaddpd	zmm8, zmm21, zmm19		;; R4 + R3 (new R3)		R6 + R5 (new R5)		; 15-52		n 25
	vsubpd	zmm21, zmm21, zmm19		;; R4 - R3 (new I4)		R6 - R5 (new negR6)		; 16-52		n 33

	zfmaddpd zmm11, zmm13, zmm27, zmm9	;; I3*2 + I4 (new I3)		I5*2 + I6 (new I5)		; 16-53		n 27
	zfmsubpd zmm13, zmm13, zmm27, zmm9	;; I3*2 - I4 (new R4)		I5*2 - I6 (new I6)		; 17-53		n 26

	vaddpd	zmm4, zmm22, zmm18		;; R5 + R6 (new R5)		R9 + R10 (new R9)		; 17-48		n 34
	vsubpd	zmm22, zmm22, zmm18		;; R5 - R6 (new R6)		R9 - R10 (new R10)		; 18-48		n 29

	zfmaddpd zmm1, zmm7, zmm10, zmm6	;; origI5*origR5 + I6 (new I5/2) origI5*origR5 + I10 (new I9/2)	; 18-49		n 35
	zfmsubpd zmm6, zmm7, zmm10, zmm6	;; origI5*origR5 - I6 (new I6/2) origI5*origR5 - I10 (new I10/2); 19-49		n 

back_to_orig:
	vaddpd	zmm12, zmm20, zmm16		;; R8 + R7 (new R7)		R12 + R11 (new R11)		; 19-50		n 34
	vsubpd	zmm20, zmm20, zmm16		;; R8 - R7 (new I8)		R12 - R11 (new I12)		; 20-50		n 29

	zfmaddpd zmm5, zmm14, zmm27, zmm0	;; I7*2 + I8 (new I7)		I11*2 + I12 (new I11)		; 20-51		n 35
	zfmsubpd zmm14, zmm14, zmm27, zmm0	;; I7*2 - I8 (new R8)		I11*2 - I12 (new R12)		; 21-51		n 32

	vblendmpd zmm28 {k6}, zmm26, ZMM_HALF{1to8} ;; 1, 1, 1, 1, 1, 1, 1	HALF				; 25***		n 
	vblendmpd zmm0 {k6}, zmm8, zmm17	;; R3				R3				; 25		n 26

	vmovapd	zmm16, zmm13			;; R4				I6
	vaddpd zmm16 {k6}, zmm2, zmm2		;; R4				(R4/2)*2			; 18-21		n 
	vaddpd zmm13 {k7}, zmm2, zmm2		;; (I2/2)*2			I6				; 31-34		n ??***

	zfmaddpd zmm19, zmm3, zmm28, zmm0	;; R1*1 + R3 (newer R1)		R1*HALF + R3 (newer R1)		; 26-57		n 
	zfmsubpd zmm3, zmm3, zmm28, zmm0	;; R1*1 - R3 (newer R3)		R1*HALF - R3 (newer R3)		; 27-57		n 

	zfmaddpd zmm9, zmm15, zmm27, zmm11	;; I1*2 + I3 (newer I1)		undefined			; 27-59		n 38
	zfmsubpd zmm15 {k7}{z}, zmm15, zmm27, zmm11;; I1*2 - I3 (newer I3)	0				; 28-59		n 39

	zfmaddpd zmm7, zmm23, zmm28, zmm16	;; R2*1 + R4 (newer R2)		R2*HALF + R4 (newer R2)		; 28-61		n 45
	zfmsubpd zmm23, zmm23, zmm28, zmm16	;; R2*1 - R4 (newer R4)		R2*HALF - R4 (newer R4)		; 29-61		n 46

	vblendmpd zmm0 {k6}, zmm22, zmm20	;; R6				I12				; 29		n 
	vblendmpd zmm20 {k6}, zmm20, zmm22	;; I8				R10				; 30		n 

						;; multiply R6/I6 by SQRTHALF - i*SQRTHALF
	zfmaddpd zmm16, zmm6, zmm27, zmm0	;; I6*2 + R6 (new2 R6/SQRTHALF)	I10*2 + I12 (newer I10)		; 30-54		n 36
	zfmsubpd zmm6, zmm6, zmm27, zmm0	;; I6*2 - R6 (new2 I6/SQRTHALF)	I10*2 - I12 (newer I12)		; 31-54		n 37

						;; multiply R8/I8 by SQRTHALF - i*SQRTHALF
	vaddpd	zmm2, zmm20, zmm14		;; I8 + R8 (new2 R8/SQRTHALF)	R10 + R12 (newer R10)		; 32-55		n 36
	vsubpd	zmm20, zmm20, zmm14		;; I8 - R8 (new2 I8/SQRTHALF)	R10 - R12 (newer R12)		; 32-55		n 37

						;;				mul R6/I6 by w^2 = .707 - .707i
	vaddpd	zmm0, zmm13, zmm21		;; I2 + I4 (newer I2)		I6 + negR6 (newer I6/SQRTHALF) 	; 33-60		n 44
	vsubpd	zmm13, zmm13, zmm21		;; I2 - I4 (newer I4)		I6 - negR6 (newer R6/SQRTHALF) 	; 33-60		n 44

	vsubpd	zmm18, zmm12, zmm4		;; R7 - R5 (newer I7)		R11 - R9 (newer negR11)		; 34-58		n 38
	vaddpd	zmm12, zmm12, zmm4		;; R7 + R5 (newer R5)		R11 + R9 (newer R9)		; 34-37		n 38

	zfmaddpd zmm4, zmm1, zmm27, zmm5	;; I5*2 + I7 (newer I5)		I9*2 + I11 (newer I9)		; 35-56		n 42
	zfmsubpd zmm1, zmm1, zmm27, zmm5	;; I5*2 - I7 (newer R7)		I9*2 - I11 (newer I11)		; 35-56		n 39

						;;				mul R10/I10 by w^1 = .924 - .383i
	zfmaddpd zmm5, zmm2, zmm26, zmm16	;; R6 + R8*1 (newer R6/SQRTHALF)    I10 + R10*.924/.383 (R10/.383) ; 36-71	n 44
	zfmsubpd zmm16, zmm16, zmm26, zmm2	;; R6*1 - R8 (newer negI8/SQRTHALF) I10*.924/.383 - R10 (I10/.383) ; 36-71	n 

						;;				mul R12/I12 by w^3 = .383 - .924i
	zfmaddpd zmm2, zmm6, zmm26, zmm20	;; I6*1 + I8 (newer I6/SQRTHALF)  I12*.924/.383 + R12 (R12/.383); 37-63		n 47
	zfnmaddpd zmm20, zmm20, zmm26, zmm6	;; I6 - I8*1 (newer R8/SQRTHALF)  I12 - R12*.924/.383 (I12/.383); 37-63		n 44

						;;				R5/I5 becomes new R5/R7
	vblendmpd zmm8 {k6}, zmm12, zmm8	;; R5				R5				; 38		n 40
	vsubpd	zmm18 {k6}, zmm15, zmm18	;; I7				R11 = 0 - negR11		; 38-55		n 43

	vblendmpd zmm11 {k6}, zmm1, zmm11	;; R7				R7				; 39		n 41
	vblendmpd zmm1 {k6}, zmm15, zmm1	;; I3				I11				; 39		n 43

;; four aparts

	vaddpd	zmm22, zmm19, zmm8		;; R1 + R5 (final R1)		R1 + R5 (final R1)		; 40-66		n 
	vsubpd	zmm19, zmm19, zmm8		;; R1 - R5 (final R5)		R1 - R5 (final R5)		; 40-66		n 

	vaddpd	zmm8, zmm3, zmm11		;; R3 + R7 (final R3)		R3 + R7 (final R3)		; 41-64		n 
	vsubpd	zmm3, zmm3, zmm11		;; R3 - R7 (final R7)		R3 - R7 (final R7)		; 41-64		n 

						;;				R9/I9 becomes newer R9/R13
	vaddpd	zmm12 {k7}, zmm9, zmm4		;; I1 + I5 (final I1)		R9				; 42-67		n 
	vsubpd	zmm4 {k7}, zmm9, zmm4		;; I1 - I5 (final I5)		R13				; 42-67		n 

						;;				R11/I11 becomes newer R11/R15
						;;				mul R11/I11 by SQRTHALF - i*SQRTHALF
	vaddpd	zmm6, zmm1, zmm18		;; I3 + I7 (final I3)		I11 + R11 (final R11/SQRTHALF)	; 43-65		n 
	vsubpd	zmm1, zmm1, zmm18		;; I3 - I7 (final I7)		I11 - R11 (final I11/SQRTHALF)	; 43-65		n 

						;;				R6/I6 becomes new R6/R8
	vblendmpd zmm9 {k6}, zmm5, zmm13	;; R6/SQRTHALF			R6/SQRTHALF			; 44		n 
	vblendmpd zmm21 {k6}, zmm20, zmm0	;; R8/SQRTHALF			R8/SQRTHALF			; 44		n 

	zfmaddpd zmm17, zmm9, zmm31, zmm7	;; R2 + R6*SQRTHALF (final R2)	R2 + R6 * SQRTHALF (final R2)	; 45-68		n 
	zfnmaddpd zmm9, zmm9, zmm31, zmm7	;; R2 - R6*SQRTHALF (final R6)	R2 - R6 * SQRTHALF (final R6)	; 45-68		n 

	zfmaddpd zmm7, zmm21, zmm31, zmm23	;; R4 + R8*SQRTHALF (final R4)	R4 + R8 * SQRTHALF (final R4)	; 46-70		n 
	zfnmaddpd zmm21, zmm21, zmm31, zmm23	;; R4 - R8*SQRTHALF (final R8)	R4 - R8 * SQRTHALF (final R8)	; 46-70		n 

						;;				R10/I10 becomes newer R10/R14
	vmovapd	zmm18, zmm16			;; negI8			R14/.383
;; bug - hidden vmovapd uops
	zfmaddpd zmm5 {k7}, zmm2, zmm31, zmm0	;; I2 + I6*SQRTHALF (final I2)	R10/.383			; 47-71		n 
	zfnmaddpd zmm16 {k7}, zmm2, zmm31, zmm0	;; I2 - I6*SQRTHALF (final I6)	R14/.383			; 47-71		n 

						;;				R12/I12 becomes newer R12/R16
;; bug - hidden vmovapd uops
	zfmaddpd zmm20 {k7}, zmm18, zmm31, zmm13;; I4 + negI8*SQRTHALF (final I8)  R16/.383			; 48-69		n 
	zfnmaddpd zmm2 {k7}, zmm18, zmm31, zmm13;; I4 - negI8*SQRTHALF (final I4)  R12/.383			; 48-69		n 


	;; shuffle inputs are:
	;; R1 = R8_0 R7_0 R4_0 R3_0 R6_0 R5_0 R2_0 R1_0	goal new R1 is  R1_7 R1_6 R1_5 R1_4 R1_3 R1_2 R1_1 R1_0
	;; R2 = R8_1 R7_1 R4_1 R3_1 R6_1 R5_1 R2_1 R1_1	goal new R2 is  R2_7 R2_6 R2_5 R2_4 R2_3 R2_2 R2_1 R2_0, etc.
	;; I1 = I8_0 I7_0 I4_0 I3_0 I6_0 I5_0 I2_0 I1_0	goal new I1 is  I1_7 I1_6 I1_5 I1_4 I1_3 I1_2 I1_1 I1_0, etc.

	vshuff64x2 zmm0, zmm22, zmm8, 01000100b	;; R6_2 R5_2 R2_2 R1_2 R6_0 R5_0 R2_0 R1_0 (R13L)		; 126-128	n 139
	vshuff64x2 zmm22, zmm22, zmm8, 11101110b;; R8_2 R7_2 R4_2 R3_2 R8_0 R7_0 R4_0 R3_0 (R13H)		; 125-127	n 131

	vshuff64x2 zmm8, zmm19, zmm3, 01000100b;; R6_6 R5_6 R2_6 R1_6 R6_4 R5_4 R2_4 R1_4 (R57L)		; 124-126	n 139
	vshuff64x2 zmm19, zmm19, zmm3, 11101110b;; R8_6 R7_6 R4_6 R3_6 R8_4 R7_4 R4_4 R3_4 (R57H)		; 123-125	n 131

	vshuff64x2 zmm3, zmm12, zmm6, 01000100b;; I6_2 I5_2 I2_2 I1_2 I6_0 I5_0 I2_0 I1_0 (I13L)		; 120-122	n 137
	vshuff64x2 zmm12, zmm12, zmm6, 11101110b;; I8_2 I7_2 I4_2 I3_2 I8_0 I7_0 I4_0 I3_0 (I13H)		; 119-121	n 129

	vshuff64x2 zmm6, zmm4, zmm1, 01000100b	;; I6_6 I5_6 I2_6 I1_6 I6_4 I5_4 I2_4 I1_4 (I57L)		; 122-124	n 137
	vshuff64x2 zmm4, zmm4, zmm1, 11101110b	;; I8_6 I7_6 I4_6 I3_6 I8_4 I7_4 I4_4 I3_4 (I57H)		; 121-123	n 129

	vshuff64x2 zmm1, zmm17, zmm7, 01000100b;; R6_3 R5_3 R2_3 R1_3 R6_1 R5_1 R2_1 R1_1 (R24L)		; 134-136	n 143
	vshuff64x2 zmm17, zmm17, zmm7, 11101110b;; R8_3 R7_3 R4_3 R3_3 R8_1 R7_1 R4_1 R3_1 (R24H)		; 132-134	n 135	r132

	vshuff64x2 zmm7, zmm9, zmm21, 01000100b	;; R6_7 R5_7 R2_7 R1_7 R6_5 R5_5 R2_5 R1_5 (R68L)		; 133-135	n 143
	vshuff64x2 zmm9, zmm9, zmm21, 11101110b	;; R8_7 R7_7 R4_7 R3_7 R8_5 R7_5 R4_5 R3_5 (R68H)		; 131-133	n 135

	vshuff64x2 zmm21, zmm5, zmm2, 01000100b	;; I6_3 I5_3 I2_3 I1_3 I6_1 I5_1 I2_1 I1_1 (I24L)		; 128-130	n 141
	vshuff64x2 zmm5, zmm5, zmm2, 11101110b	;; I8_3 I7_3 I4_3 I3_3 I8_1 I7_1 I4_1 I3_1 (I24H)		; 127-129	n 133

	vshuff64x2 zmm2, zmm16, zmm20, 01000100b;; I6_7 I5_7 I2_7 I1_7 I6_5 I5_5 I2_5 I1_5 (I68L)		; 130-132	n 141
	vshuff64x2 zmm16, zmm16, zmm20, 11101110b;; I8_7 I7_7 I4_7 I3_7 I8_5 I7_5 I4_5 I3_5 (I68H)		; 129-131	n 133

	;; shuffle the four aparts
	vshuff64x2 zmm20, zmm22, zmm19, 10001000b;; R4_6 R3_6 R4_4 R3_4 R4_2 R3_2 R4_0 R3_0 (R1357LH)		; 141-143	n 148	r136
	vshuff64x2 zmm22, zmm22, zmm19, 11011101b;; R8_6 R7_6 R8_4 R7_4 R8_2 R7_2 R8_0 R7_0 (R1357HH)		; 137-139	n 144	r136

	vshuff64x2 zmm19, zmm12, zmm4, 10001000b;; I4_6 I3_6 I4_4 I3_4 I4_2 I3_2 I4_0 I3_0 (I1357LH)		; 136-138	n 147	r134
	vshuff64x2 zmm12, zmm12, zmm4, 11011101b;; I8_6 I7_6 I8_4 I7_4 I8_2 I7_2 I8_0 I7_0 (I1357HH)		; 135-137	n 143	r134

	vshuff64x2 zmm4, zmm17, zmm9, 10001000b	;; R4_7 R3_7 R4_5 R3_5 R4_3 R3_3 R4_1 R3_1 (R2468LH)		; 142-144	n 148	r140
	vshuff64x2 zmm17, zmm17, zmm9, 11011101b;; R8_7 R7_7 R8_5 R7_5 R8_3 R7_3 R8_1 R7_1 (R2468HH)		; 140-142	n 144	r140

	vshuff64x2 zmm9, zmm5, zmm16, 10001000b	;; I4_7 I3_7 I4_5 I3_5 I4_3 I3_3 I4_1 I3_1 (I2468LH)		; 139-141	n 147	r138
	vshuff64x2 zmm5, zmm5, zmm16, 11011101b ;; I8_7 I7_7 I8_5 I7_5 I8_3 I7_3 I8_1 I7_1 (I2468HH)		; 138-140	n 143	r138

	vshuff64x2 zmm16, zmm0, zmm8, 10001000b	;; R2_6 R1_6 R2_4 R1_4 R2_2 R1_2 R2_0 R1_0 (R1357LL)		; 162-164	n 166
	vshuff64x2 zmm0, zmm0, zmm8, 11011101b	;; R6_6 R5_6 R6_4 R5_4 R6_2 R5_2 R6_0 R5_0 (R1357HL)		; 154-156	n 158

	vshuff64x2 zmm8, zmm1, zmm7, 10001000b	;; R2_7 R1_7 R2_5 R1_5 R2_3 R1_3 R2_1 R1_1 (R2468LL)		; 163-165	n 166
	vshuff64x2 zmm1, zmm1, zmm7, 11011101b;; R6_7 R5_7 R6_5 R5_5 R6_3 R5_3 R6_1 R5_1 (R2468HL)		; 153-155	n 158

	vshuff64x2 zmm7, zmm3, zmm6, 10001000b	;; I2_6 I1_6 I2_4 I1_4 I2_2 I1_2 I2_0 I1_0 (I1357LL)		; 155-157	n 161
	vshuff64x2 zmm3, zmm3, zmm6, 11011101b;; I6_6 I5_6 I6_4 I5_4 I6_2 I5_2 I6_0 I5_0 (I1357HL)		; 151-153	n 157

	vshuff64x2 zmm6, zmm21, zmm2, 10001000b	;; I2_7 I1_7 I2_5 I1_5 I2_3 I1_3 I2_1 I1_1 (I2468LL)		; 156-158	n 161
	vshuff64x2 zmm21, zmm21, zmm2, 11011101b;; I6_7 I5_7 I6_5 I5_5 I6_3 I5_3 I6_1 I5_1 (I2468HL)		; 152-154	n 157

	;; shufpd the one aparts
	vshufpd	zmm2, zmm20, zmm4, 00000000b	;; R3_7 R3_6 R3_5 R3_4 R3_3 R3_2 R3_1 R3_0 (next R3)	; 148		n 149
	vshufpd	zmm15, zmm19, zmm9, 00000000b	;; I3_7 I3_6 I3_5 I3_4 I3_3 I3_2 I3_1 I3_0 (next I3)	; 147		n 149

	vshufpd	zmm18, zmm16, zmm8, 11111111b	;; R2_7 R2_6 R2_5 R2_4 R2_3 R2_2 R2_1 R2_0 (next R2)	; 166		n 168
	vshufpd	zmm23, zmm7, zmm6, 11111111b	;; I2_7 I2_6 I2_5 I2_4 I2_3 I2_2 I2_1 I2_0 (next I2)	; 164		n 168

 	vshufpd	zmm20, zmm20, zmm4, 11111111b	;; R4_7 R4_6 R4_5 R4_4 R4_3 R4_2 R4_1 R4_0 (next R4)	; 150		n 151
	vshufpd	zmm19, zmm19, zmm9, 11111111b	;; I4_7 I4_6 I4_5 I4_4 I4_3 I4_2 I4_1 I4_0 (next I4)	; 149		n 151

	vshufpd	zmm4, zmm22, zmm17, 00000000b	;; R7_7 R7_6 R7_5 R7_4 R7_3 R7_2 R7_1 R7_0 (next R7)	; 146		n 147
	vshufpd	zmm9, zmm12, zmm5, 00000000b	;; I7_7 I7_6 I7_5 I7_4 I7_3 I7_2 I7_1 I7_0 (next I7)	; 145		n 147

	vshufpd	zmm22, zmm22, zmm17, 11111111b	;; R8_7 R8_6 R8_5 R8_4 R8_3 R8_2 R8_1 R8_0 (next R8)	; 144		n 145
	vshufpd	zmm12, zmm12, zmm5, 11111111b	;; I8_7 I8_6 I8_5 I8_4 I8_3 I8_2 I8_1 I8_0 (next I8)	; 143		n 145

	vshufpd	zmm17, zmm0, zmm1, 11111111b	;; R6_7 R6_6 R6_5 R6_4 R6_3 R6_2 R6_1 R6_0 (next R6)	; 160		n 161
	vshufpd	zmm5, zmm3, zmm21, 11111111b	;; I6_7 I6_6 I6_5 I6_4 I6_3 I6_2 I6_1 I6_0 (next I6)	; 159		n 161

	vshufpd	zmm0, zmm0, zmm1, 00000000b	;; R5_7 R5_6 R5_5 R5_4 R5_3 R5_2 R5_1 R5_0 (next R5)	; 158		n 159
	vshufpd	zmm3, zmm3, zmm21, 00000000b	;; I5_7 I5_6 I5_5 I5_4 I5_3 I5_2 I5_1 I5_0 (next I5)	; 157		n 159

	vshufpd	zmm16, zmm16, zmm8, 00000000b	;; R1_7 R1_6 R1_5 R1_4 R1_3 R1_2 R1_1 R1_0 (next R1)	; 167		n 169
	vshufpd	zmm7, zmm7, zmm6, 00000000b	;; I1_7 I1_6 I1_5 I1_4 I1_3 I1_2 I1_1 I1_0 (next I1)	; 161		n 163

	vmovapd	zmm28, [screg2+0*128+64]	;; cosine/sine for R3/I3 (w^2 = complex w^1)
	zfmaddpd zmm21, zmm2, zmm28, zmm15	;; A3 = R3 * cosine/sine + I3				; 1-4		n 5
	zfmsubpd zmm15, zmm15, zmm28, zmm2	;; B3 = I3 * cosine/sine - R3				; 1-4		n 5

	vmovapd	zmm28, [screg1+0*128+64]	;; cosine/sine for R2/I2 (w^1)
	zfmaddpd zmm6, zmm18, zmm28, zmm23	;; A2 = R2 * cosine/sine + I2				; 2-5		n 7
	zfmsubpd zmm23, zmm23, zmm28, zmm18	;; B2 = I2 * cosine/sine - R2				; 2-5		n 7

	vmovapd	zmm28, [screg1+1*128+64]	;; cosine/sine for R4/I4 (w^3)
	zfmaddpd zmm8, zmm20, zmm28, zmm19	;; A4 = R4 * cosine/sine + I4				; 3-6		n 9
	zfmsubpd zmm19, zmm19, zmm28, zmm20	;; B4 = I4 * cosine/sine - R4				; 3-6		n 9

	vmovapd	zmm28, [screg2+2*128+64]	;; cosine/sine for R7/I7 (w^6 = complex w^3)
	zfmaddpd zmm1, zmm4, zmm28, zmm9	;; A7 = R7 * cosine/sine + I7				; 4-7		n 11
	zfmsubpd zmm9, zmm9, zmm28, zmm4	;; B7 = I7 * cosine/sine - R7				; 4-7		n 12

	vmovapd	zmm28, [screg2+0*128]		;; sine for R3/I3
	vmulpd	zmm21, zmm21, zmm28		;; R3 = A3 * sine					; 5-8		n 11
	vmulpd	zmm15, zmm15, zmm28		;; I3 = B3 * sine					; 5-8		n 12

	vmovapd	zmm28, [screg1+3*128+64]	;; cosine/sine for R8/I8 (w^7)
	zfmaddpd zmm2, zmm22, zmm28, zmm12	;; A8 = R8 * cosine/sine + I8				; 6-9		n 13
	zfmsubpd zmm12, zmm12, zmm28, zmm22	;; B8 = I8 * cosine/sine - R8				; 6-9		n 14

	vmovapd	zmm28, [screg1+0*128]		;; sine for R2/I2
	vmulpd	zmm6, zmm6, zmm28		;; R2 = A2 * sine					; 7-10		n 13
	vmulpd	zmm23, zmm23, zmm28		;; I2 = B2 * sine					; 7-10		n 14

	vmovapd	zmm28, [screg1+2*128+64]	;; cosine/sine for R6/I6 (w^5)
	zfmaddpd zmm18, zmm17, zmm28, zmm5	;; A6 = R6 * cosine/sine + I6				; 8-11		n 15
	zfmsubpd zmm5, zmm5, zmm28, zmm17	;; B6 = I6 * cosine/sine - R6				; 8-11		n 16

	vmovapd	zmm28, [screg1+1*128]		;; sine for R4/I4
	vmulpd	zmm8, zmm8, zmm28		;; R4 = A4 * sine					; 9-12		n 15
	vmulpd	zmm19, zmm19, zmm28		;; I4 = B4 * sine					; 9-12		n 16

	vmovapd	zmm28, [screg2+1*128+64]	;; cosine/sine for R5/I5 (w^4 = complex w^2)
	zfmaddpd zmm20, zmm0, zmm28, zmm3	;; A5 = R5 * cosine/sine + I5				; 10-13		n 17
	zfmsubpd zmm3, zmm3, zmm28, zmm0	;; B5 = I5 * cosine/sine - R5				; 10-13		n 18

	vbroadcastf64x4 zmm27, ZMM_383_707_383_1 ;; .383 SQRTHALF .383 1 .383 SQRTHALF .383 1
	zfmaddpd zmm4, zmm7, zmm27, zmm16	;; R1+(.383,SQRTHALF,.383,1,.383,SQRTHALF,.383,1)*R9		; 88-89		n 
	zfnmaddpd zmm7, zmm7, zmm27, zmm16	;; R1-(.383,SQRTHALF,.383,1,.383,SQRTHALF,.383,1)*R9		; 88-89		n 

	vmovapd	zmm28, [screg2+2*128]		;; sine for R7/I7
	zfnmaddpd zmm24, zmm1, zmm28, zmm21	;; R3-(R7 = A7 * sine)					; 11-14		n 19
	zfmaddpd zmm1, zmm1, zmm28, zmm21	;; R3+(R7 = A7 * sine)					; 11-14		n 22

	zfmaddpd zmm22, zmm9, zmm28, zmm15	;; I3+(I7 = B7 * sine)					; 12-15		n 19
	zfnmaddpd zmm9, zmm9, zmm28, zmm15	;; I3-(I7 = B7 * sine)					; 12-15		n 23

	vmovapd	zmm28, [screg1+3*128]		;; sine for R8/I8
	zfmaddpd zmm17, zmm2, zmm28, zmm6	;; R2+(R8 = A8 * sine)					; 13-16		n 20
	zfnmaddpd zmm2, zmm2, zmm28, zmm6	;; R2-(R8 = A8 * sine)					; 13-16		n 24

	zfnmaddpd zmm0, zmm12, zmm28, zmm23	;; I2-(I8 = B8 * sine)					; 14-17		n 21
	zfmaddpd zmm12, zmm12, zmm28, zmm23	;; I2+(I8 = B8 * sine)					; 14-17		n 25

	vmovapd	zmm28, [screg1+2*128]		;; sine for R6/I6
	zfmaddpd zmm16, zmm18, zmm28, zmm8	;; R4+(R6 = A6 * sine)					; 15-18		n 20
	zfnmaddpd zmm18, zmm18, zmm28, zmm8	;; R4-(R6 = A6 * sine)					; 15-18		n 25

	zfnmaddpd zmm21, zmm5, zmm28, zmm19	;; I4-(I6 = B6 * sine)					; 16-19		n 21
	zfmaddpd zmm5, zmm5, zmm28, zmm19	;; I4+(I6 = B6 * sine)					; 16-19		n 24

	vmovapd	zmm28, [screg2+1*128]		;; sine for R5/I5
	zfmaddpd zmm15, zmm20, zmm28, zmm4	;; r1++ = (r1+r9) + r5*sine				; 17-20		n 22
	zfnmaddpd zmm20, zmm20, zmm28, zmm4	;; r1+- = (r1+r9) - r5*sine				; 17-20		n 23

	zfmaddpd zmm6, zmm3, zmm28, zmm7	;; r1-+ = (r1-r9) + i5*sine				; 18-21		n 29
	zfnmaddpd zmm3, zmm3, zmm28, zmm7	;; r1-- = (r1-r9) - i5*sine				; 18-21		n 31
	bump	screg1, scinc1

	vaddpd	zmm8, zmm24, zmm22		;; r3-+ = (r3-r7) + (i3+i7)				; 19-22		n 29
	vsubpd	zmm24, zmm24, zmm22		;; r3-- = (r3-r7) - (i3+i7)				; 19-22		n 31
	bump	screg2, scinc2

	vaddpd	zmm19, zmm17, zmm16		;; r2++ = (r2+r8) + (r4+r6)				; 20-23		n 26
	vsubpd	zmm17, zmm17, zmm16		;; r2+- = (r2+r8) - (r4+r6)				; 20-23		n 28

	vsubpd	zmm7, zmm0, zmm21		;; i2-- = (i2-i8) - (i4-i6)				; 21-24		n 27
	vaddpd	zmm0, zmm0, zmm21		;; i2-+ = (i2-i8) + (i4-i6)				; 21-24		n 28

	vaddpd	zmm22, zmm15, zmm1		;; r1+++ = (r1++) + (r3+r7)				; 22-25		n 26
	vsubpd	zmm15, zmm15, zmm1		;; r1++- = (r1++) - (r3+r7)				; 22-25		n 27

	vaddpd	zmm16, zmm20, zmm9		;; r1+-+ = (r1+-) + (i3-i7)				; 23-26		n 33
	vsubpd	zmm20, zmm20, zmm9		;; r1+-- = (r1+-) - (i3-i7)				; 23-26		n 34

	vaddpd	zmm21, zmm2, zmm5		;; r2-+ = (r2-r8) + (i4+i6)				; 24-27		n 30
	vsubpd	zmm2, zmm2, zmm5		;; r2-- = (r2-r8) - (i4+i6)				; 24-27		n 32

	vaddpd	zmm1, zmm12, zmm18		;; i2++ = (i2+i8) + (r4-r6)				; 25-28		n 30
	vsubpd	zmm12, zmm12, zmm18		;; i2+- = (i2+i8) - (r4-r6)				; 25-28		n 32

	vaddpd	zmm9, zmm22, zmm19		;; R1 = (r1+++) + (r2++)				; 26-29
	vsubpd	zmm22, zmm22, zmm19		;; R9 = (r1+++) - (r2++)				; 26-29

	vaddpd	zmm5, zmm15, zmm7		;; R5  = (r1++-) + (i2--)				; 27-30
	vsubpd	zmm15, zmm15, zmm7		;; R13 = (r1++-) - (i2--)				; 27-30

	vaddpd	zmm19, zmm17, zmm0		;; r2+-+ = (r2+-) + (i2-+)				; 28-31		n 33
	vsubpd	zmm17, zmm17, zmm0		;; r2+-- = (r2+-) - (i2-+)				; 28-31		n 34

	zfmaddpd zmm7, zmm8, zmm31, zmm6	;; r2_10o = (r1-+) + .707(r3-+)				; 29-32		n 35
	zfnmaddpd zmm8, zmm8, zmm31, zmm6	;; r6_14o = (r1-+) - .707(r3-+)				; 29-32		n 36

	zfmaddpd zmm0, zmm21, zmm30, zmm1	;; r2_10e/.383 = .924/.383(r2-+) + (i2++)		; 30-33		n 35
	zfmsubpd zmm1, zmm1, zmm30, zmm21	;; r6_14e/.383 = .924/.383(i2++) - (r2-+)		; 30-33		n 36
	zstore	[srcreg+r8], zmm9		;; Save R1						; 30

	zfnmaddpd zmm6, zmm24, zmm31, zmm3	;; r4_12o = (r1--) - .707(r3--)				; 31-34		n 37
	zfmaddpd zmm24, zmm24, zmm31, zmm3	;; r8_16o = (r1--) + .707(r3--)				; 31-34		n 38
	zstore	[srcreg+r8+64], zmm22		;; Save R9						; 30+1

	zfmaddpd zmm21, zmm12, zmm30, zmm2	;; r4_12e/.383 = .924/.383(i2+-) + (r2--)		; 32-35		n 37
	zfnmaddpd zmm2, zmm2, zmm30, zmm12	;; r8_16e/.383 = (i2+-) - .924/.383(r2--)		; 32-35		n 38
	zstore	[srcreg+r8+d4], zmm5		;; Save R5						; 31+1

	zfmaddpd zmm3, zmm19, zmm31, zmm16	;; R3  = (r1+-+) + .707(r2+-+)				; 33-36
	zfnmaddpd zmm19, zmm19, zmm31, zmm16	;; R11 = (r1+-+) - .707(r2+-+)				; 33-36
	zstore	[srcreg+r8+d4+64], zmm15	;; Save R13						; 31+2

	zfnmaddpd zmm12, zmm17, zmm31, zmm20	;; R7  = (r1+--) - .707(r2+--)				; 34-37
	zfmaddpd zmm17, zmm17, zmm31, zmm20	;; R15 = (r1+--) + .707(r2+--)				; 34-37

	zfmaddpd zmm16, zmm0, zmm29, zmm7	;; R2  = r2_10o + .383*r2_10e				; 35-38
	zfnmaddpd zmm0, zmm0, zmm29, zmm7	;; R10 = r2_10o - .383*r2_10e				; 35-38

	zfmaddpd zmm20, zmm1, zmm29, zmm8	;; R6  = r6_14o + .383*r6_14e				; 36-39
	zfnmaddpd zmm1, zmm1, zmm29, zmm8	;; R14 = r6_14o - .383*r6_14e				; 36-39

	zfmaddpd zmm7, zmm21, zmm29, zmm6	;; R4  = r4_12o + .383*r4_12e				; 37-40
	zfnmaddpd zmm21, zmm21, zmm29, zmm6	;; R12 = r4_12o - .383*r4_12e				; 37-40
	zstore	[srcreg+r8+d2], zmm3		;; Save R3						; 37

	zfmaddpd zmm8, zmm2, zmm29, zmm24	;; R8  = r8_16o + .383*r8_16e				; 38-41
	zfnmaddpd zmm2, zmm2, zmm29, zmm24	;; R16 = r8_16o - .383*r8_16e				; 38-41
	zstore	[srcreg+r8+d2+64], zmm19	;; Save R11						; 37+1

	zstore	[srcreg+r8+d4+d2], zmm12	;; Save R7						; 38+1
	zstore	[srcreg+r8+d4+d2+64], zmm17	;; Save R15						; 38+2
	zstore	[srcreg+r8+d1], zmm16		;; Save R2						; 39+2
	zstore	[srcreg+r8+d1+64], zmm0		;; Save R10						; 39+3
	zstore	[srcreg+r8+d4+d1], zmm20	;; Save R6						; 40+3
	zstore	[srcreg+r8+d4+d1+64], zmm1	;; Save R14						; 40+4
	zstore	[srcreg+r8+d2+d1], zmm7		;; Save R4						; 41+4
	zstore	[srcreg+r8+d2+d1+64], zmm21	;; Save R12						; 41+5
	zstore	[srcreg+r8+d4+d2+d1], zmm8	;; Save R8						; 42+5
	zstore	[srcreg+r8+d4+d2+d1+64], zmm2	;; Save R16						; 42+6
	bump	srcreg, srcinc
	ENDM


zr64_hundredtwentyeight_real_with_mult_preload MACRO
	zr64_128r_mult_cmn_preload
	ENDM
zr64_hundredtwentyeight_real_with_mult MACRO srcreg,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	zr64_128r_mult_cmn srcreg,0,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM
zr64f_hundredtwentyeight_real_with_mult_preload MACRO
	zr64_128r_mult_cmn_preload
	ENDM
zr64f_hundredtwentyeight_real_with_mult MACRO srcreg,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	;; Presently, gwnum does not support a with_mult (gwfftmul) interface with two sources and one destination.
	;; Thus, rbx is always zero.
	;; zr64_128r_mult_cmn srcreg,rbx,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	zr64_128r_mult_cmn srcreg,0,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

zr64_128r_mult_cmn_preload MACRO
	;; preload would be silly for a macro that is executed only once
	ENDM
zr64_128r_mult_cmn MACRO srcreg,srcoff,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	LOCAL	orig, back_to_orig, no_save_fft, submul, fma, muladd, muladdhard, mulsub, mulsubhard

	;; For zero pad
	zmult7	srcreg+srcoff, srcreg+rbp

	vmovapd	zmm1, [srcreg+srcoff+d1]	;; r2+r10
	vmovapd	zmm3, [srcreg+srcoff+d4+d2+d1]	;; r8+r16
	vaddpd	zmm0, zmm1, zmm3		;; r2++ = (r2+r10)+(r8+r16)					; 1-4		n 6
	vsubpd	zmm1, zmm1, zmm3		;; r2+- = (r2+r10)-(r8+r16)					; 1-4		n 7

	vmovapd	zmm5, [srcreg+srcoff+d2+d1]	;; r4+r12
	vmovapd	zmm7, [srcreg+srcoff+d4+d1]	;; r6+r14
	vaddpd	zmm3, zmm5, zmm7		;; r4++ = (r4+r12)+(r6+r14)					; 2-5		n 6
	vsubpd	zmm5, zmm5, zmm7		;; r4+- = (r4+r12)-(r6+r14)					; 2-5		n 7

	vmovapd	zmm13, [srcreg+srcoff]		;; r1+r9
	vmovapd	zmm15, [srcreg+srcoff+d4]	;; r5+r13
	vaddpd	zmm7, zmm13, zmm15		;; r1++ = (r1+r9)+(r5+r13)					; 3-6		n 8
	vsubpd	zmm13, zmm13, zmm15		;; r1+- = (r1+r9)-(r5+r13)					; 3-6		n 11

	vmovapd	zmm9, [srcreg+srcoff+d2]	;; r3+r11
	vmovapd	zmm11, [srcreg+srcoff+d4+d2]	;; r7+r15
	vaddpd	zmm15, zmm9, zmm11		;; r3++ = (r3+r11)+(r7+r15)					; 4-7		n 8
	vsubpd	zmm9, zmm9, zmm11		;; r3+- = (r3+r11)-(r7+r15)					; 4-7		n 12

	vmovapd	zmm10, [srcreg+srcoff+d2+64]	;; r3-r11
	vmovapd	zmm12, [srcreg+srcoff+d4+d2+64]	;; r7-r15
	vaddpd	zmm11, zmm10, zmm12		;; r3-+ = (r3-r11)+(r7-r15)					; 5-8		n 16
	vsubpd	zmm10, zmm10, zmm12		;; r3-- = (r3-r11)-(r7-r15)					; 5-8		n 14

	vmovapd	zmm2, [srcreg+srcoff+d1+64]	;; r2-r10
	vaddpd	zmm12, zmm0, zmm3		;; r2+++ = (r2++) + (r4++)					; 6-9		n 13
	vsubpd	zmm0, zmm0, zmm3		;; r2++- = (r2++) - (r4++)					; 6-9		n 11

	vmovapd	zmm4, [srcreg+srcoff+d4+d2+d1+64] ;; r8-r16
	vaddpd	zmm3, zmm1, zmm5		;; r2+-+ = (r2+-) + (r4+-)					; 7-10		n 12
	vsubpd	zmm1, zmm1, zmm5		;; r2+-- = (r2+-) - (r4+-)	(final I5)			; 7-10		n 18

	vmovapd	zmm6, [srcreg+srcoff+d2+d1+64]	;; r4-r12
	vaddpd	zmm5, zmm7, zmm15		;; r1+++ = (r1++) + (r3++)					; 8-11		n 13
	vsubpd	zmm7, zmm7, zmm15		;; r1++- = (r1++) - (r3++)	(final R5)			; 8-11		n 18

	vmovapd	zmm8, [srcreg+srcoff+d4+d1+64]	;; r6-r14
	vaddpd	zmm15, zmm2, zmm4		;; r2-+ = (r2-r10)+(r8-r16)					; 9-12		n 17
	vsubpd	zmm2, zmm2, zmm4		;; r2-- = (r2-r10)-(r8-r16)					; 9-12		n 15

	vbroadcastsd zmm31, ZMM_SQRTHALF
	vaddpd	zmm4, zmm6, zmm8		;; r4-+ = (r4-r12)+(r6-r14)					; 10-13		n 17
	vsubpd	zmm6, zmm6, zmm8		;; r4-- = (r4-r12)-(r6-r14)					; 10-13		n 15

	vmovapd	zmm14, [srcreg+srcoff+64]	;; r1-r9
	zfmaddpd zmm8, zmm0, zmm31, zmm13	;; R3 = (r1+-) + .707(r2++-)					; 11-14		n 19
	zfnmaddpd zmm0, zmm0, zmm31, zmm13	;; R7 = (r1+-) - .707(r2++-)					; 11-14		n 20

	vbroadcastsd zmm30, ZMM_P924_P383
	zfmaddpd zmm13, zmm3, zmm31, zmm9	;; I3 = .707*(r2+-+) + (r3+-)					; 12-15		n 19
	zfmsubpd zmm3, zmm3, zmm31, zmm9	;; I7 = .707*(r2+-+) - (r3+-)					; 12-15		n 20

	vmovapd	zmm16, [srcreg+srcoff+d4+64]	;; r5-r13
	vaddpd	zmm9, zmm5, zmm12		;; R1 = (r1+++) + (r2+++)					; 13-16		n 31
	vsubpd	zmm5, zmm5, zmm12		;; R9 = (r1+++) - (r2+++)					; 13-16		n 33

	vmovapd	zmm28, [screg2+1*128+64]	;; cosine/sine for R5/I5 (w^4 = complex w^2)
	zfmaddpd zmm12, zmm10, zmm31, zmm14	;; r2o = (r1-r9) + .707*(r3--)					; 14-17		n 21
	zfnmaddpd zmm10, zmm10, zmm31, zmm14	;; r4o = (r1-r9) - .707*(r3--)					; 14-17		n 22

	vmovapd	zmm27, [screg2+0*128+64]	;; cosine/sine for R3/I3 (w^2 = complex w^1)
	zfmaddpd zmm14, zmm2, zmm30, zmm6	;; r2e/.383 = .924/.383(r2--) + (r4--)				; 15-18		n 21
	zfnmaddpd zmm6, zmm6, zmm30, zmm2	;; r4e/.383 = (r2--) - .924/.383(r4--)				; 15-18		n 22

	vmovapd	zmm26, [screg2+2*128+64]	;; cosine/sine for R7/I7 (w^6 = complex w^3)
	zfmaddpd zmm2, zmm11, zmm31, zmm16	;; i2o = .707(r3-+) + (r5-r13)					; 16-19		n 23
	zfmsubpd zmm11, zmm11, zmm31, zmm16	;; i4o = .707(r3-+) - (r5-r13)					; 16-19		n 24

	vbroadcastsd zmm29, ZMM_P383
	zfmaddpd zmm16, zmm4, zmm30, zmm15	;; i2e/.383 = (r2-+) + .924/.383(r4-+)				; 17-20		n 23
	zfmsubpd zmm15, zmm15, zmm30, zmm4	;; i4e/.383 = .924/.383(r2-+) - (r4-+)				; 17-20		n 24

	vmovapd	zmm25, [screg2+1*128]		;; sine for R5/I5
	zfmsubpd zmm4, zmm7, zmm28, zmm1	;; A5 = R5 * cosine/sine - I5					; 18-21		n 25
	zfmaddpd zmm1, zmm1, zmm28, zmm7	;; B5 = I5 * cosine/sine + R5					; 18-21		n 25

	vmovapd	zmm24, [screg2+0*128]		;; sine for R3/I3
	zfmsubpd zmm7, zmm8, zmm27, zmm13	;; A3 = R3 * cosine/sine - I3					; 19-22		n 26
	zfmaddpd zmm13, zmm13, zmm27, zmm8	;; B3 = I3 * cosine/sine + R3					; 19-22		n 26

	vmovapd	zmm23, [screg2+2*128]		;; sine for R7/I7
	zfmsubpd zmm8, zmm0, zmm26, zmm3	;; A7 = R7 * cosine/sine - I7					; 20-23		n 27
	zfmaddpd zmm3, zmm3, zmm26, zmm0	;; B7 = I7 * cosine/sine + R7					; 20-23		n 27

	vmovapd	zmm22, [screg1+0*128+64]	;; cosine/sine for R2/I2 (w^1)
	zfmaddpd zmm0, zmm14, zmm29, zmm12	;; R2 = r2o + .383*r2e						; 21-24		n 28
	zfnmaddpd zmm14, zmm14, zmm29, zmm12	;; R8 = r2o - .383*r2e						; 21-24		n 32

	vmovapd	zmm19, [screg1+2*128+64]	;; cosine/sine for R6/I6 (w^5)
	zfmaddpd zmm12, zmm6, zmm29, zmm10	;; R4 = r4o + .383*r4e						; 22-25		n 30
	zfnmaddpd zmm6, zmm6, zmm29, zmm10	;; R6 = r4o - .383*r4e						; 22-25		n 29

	vmovapd	zmm20, [screg1+1*128+64]	;; cosine/sine for R4/I4 (w^3)
	zfmaddpd zmm10, zmm16, zmm29, zmm2	;; I2 = .383*i2e + i2o						; 23-26		n 28
	zfmsubpd zmm16, zmm16, zmm29, zmm2	;; I8 = .383*i2e - i2o						; 23-26		n 32

	vmovapd	zmm21, [screg1+3*128+64]	;; cosine/sine for R8/I8 (w^7)
	zfmaddpd zmm2, zmm15, zmm29, zmm11	;; I4 = .383*i4e + i4o						; 24-27		n 30
	zfmsubpd zmm15, zmm15, zmm29, zmm11	;; I6 = .383*i4e - i4o						; 24-27		n 29

	vmovapd	zmm18, [screg1+0*128]		;; sine for R2/I2
	vmulpd	zmm4, zmm4, zmm25		;; A5 = A5 * sine (final R5)					; 25-28		n 32
	vmulpd	zmm1, zmm1, zmm25		;; B5 = B5 * sine (final I5)					; 25-28		n 34

	vmovapd zmm17, [screg1+2*128]		;; sine for R6/I6
	vmulpd	zmm7, zmm7, zmm24		;; A3 = A3 * sine (final R3)					; 26-29		n 36
	vmulpd	zmm13, zmm13, zmm24		;; B3 = B3 * sine (final I3)					; 26-29		n 38

	vmovapd	zmm28, [screg1+1*128]		;; sine for R4/I4
	vmulpd	zmm8, zmm8, zmm23		;; A7 = A7 * sine (final R7)					; 27-30		n 36
	vmulpd	zmm3, zmm3, zmm23		;; B7 = B7 * sine (final I7)					; 27-30		n 38

	vmovapd	zmm27, [screg1+3*128]		;; sine for R8/I8
	zfmsubpd zmm11, zmm0, zmm22, zmm10	;; A2 = R2 * cosine/sine - I2					; 28-31		n 34
	zfmaddpd zmm10, zmm10, zmm22, zmm0	;; B2 = I2 * cosine/sine + R2					; 28-31		n 35

	vbroadcastsd zmm22, ZMM_ONE
	zfmsubpd zmm0, zmm6, zmm19, zmm15	;; A6 = R6 * cosine/sine - I6					; 29-32		n 36
	zfmaddpd zmm15, zmm15, zmm19, zmm6	;; B6 = I6 * cosine/sine + R6					; 29-32		n 37

	mov	eax, 11111110b			;; We're pretty sure callers of this macro call zloop_init next
	kmovw	k7, eax				;; Set k7 to 11111110b						; 30		n 31
	knotw	k6, k7				;; Set k6 to 00000001b						; 31		n 50
	L1prefetchw srcreg+L1pd, L1pt

	zfmsubpd zmm6, zmm12, zmm20, zmm2	;; A4 = R4 * cosine/sine - I4					; 30-33		n 38
	zfmaddpd zmm2, zmm2, zmm20, zmm12	;; B4 = I4 * cosine/sine + R4					; 31-34		n 39
	L1prefetchw srcreg+64+L1pd, L1pt

	;; Swap the four aparts
	vshuff64x2 zmm19, zmm9, zmm4, 01000100b	;; R5_3 R5_2 R5_1 R5_0 R1_3 R1_2 R1_1 R1_0 (R15L)		; 32-34		n 42
	vshuff64x2 zmm9, zmm9, zmm4, 11101110b	;; R5_7	R5_6 R5_5 R5_4 R1_7 R1_6 R1_5 R1_4 (R15H)		; 33-35		n 42
	L1prefetchw srcreg+d1+L1pd, L1pt

	zfmsubpd zmm12, zmm14, zmm21, zmm16	;; A8 = R8 * cosine/sine - I8					; 32-35		n 40
	zfmaddpd zmm16, zmm16, zmm21, zmm14	;; B8 = I8 * cosine/sine + R8					; 33-36		n 41
	L1prefetchw srcreg+d1+64+L1pd, L1pt

	vshuff64x2 zmm4, zmm5, zmm1, 01000100b	;; I5_3 I5_2 I5_1 I5_0 I1_3 I1_2 I1_1 I1_0 (I15L)		; 34-36		n 48
	vshuff64x2 zmm5, zmm5, zmm1, 11101110b	;; I5_7 I5_6 I5_5 I5_4 I1_7 I1_6 I1_5 I1_4 (I15H)		; 35-37		n 50
	L1prefetchw srcreg+d2+L1pd, L1pt

	vmulpd	zmm11, zmm11, zmm18		;; A2 = A2 * sine (final R2)					; 34-37		n 40
	vmulpd	zmm10, zmm10, zmm18		;; B2 = B2 * sine (final I2)					; 35-38		n 42
	L1prefetchw srcreg+d2+64+L1pd, L1pt

	vshuff64x2 zmm1, zmm7, zmm8, 01000100b	;; R7_3 R7_2 R7_1 R7_0 R3_3 R3_2 R3_1 R3_0 (R37L)		; 36-38		n 44
	vshuff64x2 zmm7, zmm7, zmm8, 11101110b	;; R7_7 R7_6 R7_5 R7_4 R3_7 R3_6 R3_5 R3_4 (R37H)		; 37-39		n 44
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vmulpd	zmm0, zmm0, zmm17		;; A6 = A6 * sine (final R6)					; 36-39		n 40
	vmulpd	zmm15, zmm15, zmm17		;; B6 = B6 * sine (final I6)					; 37-40		n 42
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt

	vshuff64x2 zmm8, zmm13, zmm3, 01000100b	;; I7_3 I7_2 I7_1 I7_0 I3_3 I3_2 I3_1 I3_0 (I37L)		; 38-40		n 48
	vshuff64x2 zmm13, zmm13, zmm3, 11101110b;; I7_7	I7_6 I7_5 I7_4 I3_7 I3_6 I3_5 I3_4 (I37H)		; 39-41		n 50
	L1prefetchw srcreg+d4+L1pd, L1pt

	vmulpd	zmm6, zmm6, zmm28		;; A4 = A4 * sine (final R4)					; 38-41		n 44
	vmulpd	zmm2, zmm2, zmm28		;; B4 = B4 * sine (final I4)					; 39-42		n 46
	L1prefetchw srcreg+d4+64+L1pd, L1pt

	vshuff64x2 zmm3, zmm11, zmm0, 01000100b	;; R6_3 R6_2 R6_1 R6_0 R2_3 R2_2 R2_1 R2_0 (R26L)		; 40-42		n 46
	vshuff64x2 zmm11, zmm11, zmm0, 11101110b;; R6_7	R6_6 R6_5 R6_4 R2_7 R2_6 R2_5 R2_4 (R26H)		; 41-43		n 46
	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	vmulpd	zmm12, zmm12, zmm27		;; A8 = A8 * sine (final R8)					; 40-43		n 44
	vmulpd	zmm16, zmm16, zmm27		;; B8 = B8 * sine (final I8)					; 41-44		n 46
	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt

	vshuff64x2 zmm0, zmm10, zmm15, 01000100b;; I6_3 I6_2 I6_1 I6_0 I2_3 I2_2 I2_1 I2_0 (I26L)		; 42-44		n 52
	vshuff64x2 zmm10, zmm10, zmm15, 11101110b;; I6_7 I6_6 I6_5 I6_4 I2_7 I2_6 I2_5 I2_4 (I26H)		; 43-45		n 54
	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	;; Do four of the first butterflies while we are shuffling!
	vaddpd	zmm14, zmm19, zmm9		;; add r1/r5 4-aparts (R15+)					; 42-45		n 56
	vsubpd	zmm19, zmm19, zmm9		;; sub r1/r5 4-aparts (R15-)					; 43-46		n 58
	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt

	vshuff64x2 zmm15, zmm6, zmm12, 01000100b;; R8_3 R8_2 R8_1 R8_0 R4_3 R4_2 R4_1 R4_0 (R48L)		; 44-46		n 48
	vshuff64x2 zmm6, zmm6, zmm12, 11101110b	;; R8_7 R8_6 R8_5 R8_4 R4_7 R4_6 R4_5 R4_4 (R48H)		; 45-47		n 48
	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	vaddpd	zmm9, zmm1, zmm7		;; add r3/r7 4-aparts (R37+)					; 44-47		n 56
	vsubpd	zmm1, zmm1, zmm7		;; sub r3/r7 4-aparts (R37-)					; 45-48		n 58
	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt

	vshuff64x2 zmm12, zmm2, zmm16, 01000100b;; I8_3 I8_2 I8_1 I8_0 I4_3 I4_2 I4_1 I4_0 (I48L)		; 46-48		n 52
	vshuff64x2 zmm2, zmm2, zmm16, 11101110b	;; I8_7 I8_6 I8_5 I8_4 I4_7 I4_6 I4_5 I4_4 (I48H)		; 47-49		n 54

	vaddpd	zmm7, zmm3, zmm11		;; add r2/r6 4-aparts (R26+)					; 46-49		n 60
	vsubpd	zmm3, zmm3, zmm11		;; sub r2/r6 4-aparts (R26-)					; 47-50		n 62

	;; Swap the two aparts
	vshuff64x2 zmm16, zmm4, zmm8, 10001000b	;; I7_1 I7_0 I3_1 I3_0 I5_1 I5_0 I1_1 I1_0 (i1537LL)		; 48-50		n 68
	vshuff64x2 zmm4, zmm4, zmm8, 11011101b	;; I7_3 I7_2 I3_3 I3_2 I5_3 I5_2 I1_3 I1_2 (i1537LH)		; 49-51		n 66

	vaddpd	zmm11, zmm15, zmm6		;; add r4/r8 4-aparts (R48+)					; 48-51		n 60
	vsubpd	zmm15, zmm15, zmm6		;; sub r4/r8 4-aparts (R48-)					; 49-52		n 62

	vshuff64x2 zmm8, zmm5, zmm13, 10001000b	;; I7_5 I7_4 I3_5 I3_4 I5_5 I5_4 I1_5 I1_4 (i1537HL)		; 50-52		n 70
	vshuff64x2 zmm5, zmm5, zmm13, 11011101b	;; I7_7 I7_6 I3_7 I3_6 I5_7 I5_6 I1_7 I1_6 (i1537HH)		; 51-53		n 64

	vblendmpd zmm26 {k6}, zmm22, zmm30	;; 1, 1, 1, 1, 1, 1, 1		.924/.383			; 50		n 68
	vblendmpd zmm27 {k6}, zmm22, zmm31	;; 1, 1, 1, 1, 1, 1, 1		SQRTHALF			; 51		n 87

	vshuff64x2 zmm13, zmm0, zmm12, 10001000b;; I8_1 I8_0 I4_1 I4_0 I6_1 I6_0 I2_1 I2_0 (i2648LL)		; 52-54		n 68
	vshuff64x2 zmm0, zmm0, zmm12, 11011101b	;; I8_3 I8_2 I4_3 I4_2 I6_3 I6_2 I2_3 I2_2 (i2648LH)		; 53-55		n 66

	vmovapd zmm25 {k7}{z}, zmm22		;; 1, 1, 1, 1, 1, 1, 1		0				; 52		n 53
	vsubpd	zmm25 {k6}, zmm25, zmm31	;; 1, 1, 1, 1, 1, 1, 1		-SQRTHALF			; 53		n 86

	vshuff64x2 zmm12, zmm10, zmm2, 10001000b;; I8_5 I8_4 I4_5 I4_4 I6_5 I6_4 I2_5 I2_4 (i2648HL)		; 54-56		n 70
	vshuff64x2 zmm10, zmm10, zmm2, 11011101b;; I8_7	I8_6 I4_7 I4_6 I6_7 I6_6 I2_7 I2_6 (i2648HH)		; 55-57		n 64

	vblendmpd zmm28 {k6}, zmm31, zmm29	;; SQRTHALF, SQRTHALF, ...,	.383				; 54		n 96

	vshuff64x2 zmm2, zmm14, zmm9, 10001000b	;; R7_1 R7_0 R3_1 R3_0 R5_1 R5_0 R1_1 R1_0 (r1537LL+)		; 56-58		n 76
	vshuff64x2 zmm14, zmm14, zmm9, 11011101b;; R7_3 R7_2 R3_3 R3_2 R5_3 R5_2 R1_3 R1_2 (r1537LH+)		; 57-59		n 78

	vshuff64x2 zmm9, zmm19, zmm1, 10001000b	;; R7_5 R7_4 R3_5 R3_4 R5_5 R5_4 R1_5 R1_4 (r1537HL-)		; 58-60		n 72
	vshuff64x2 zmm19, zmm19, zmm1, 11011101b;; R7_7 R7_6 R3_7 R3_6 R5_7 R5_6 R1_7 R1_6 (r1537HH-)		; 59-61		n 74

	vshuff64x2 zmm1, zmm7, zmm11, 10001000b	;; R8_1 R8_0 R4_1 R4_0 R6_1 R6_0 R2_1 R2_0 (r2648LL+)		; 60-62		n 76
	vshuff64x2 zmm7, zmm7, zmm11, 11011101b	;; R8_3 R8_2 R4_3 R4_2 R6_3 R6_2 R2_3 R2_2 (r2648LH+)		; 61-63		n 78

	vshuff64x2 zmm11, zmm3, zmm15, 10001000b;; R8_5 R8_4 R4_5 R4_4 R6_5 R6_4 R2_5 R2_4 (r2648HL-)		; 62-64		n 72
	vshuff64x2 zmm3, zmm3, zmm15, 11011101b	;; R8_7 R8_6 R4_7 R4_6 R6_7 R6_6 R2_7 R2_6 (r2648HH-)		; 63-65		n 74

	;; Swap the one aparts
	vshufpd	zmm15, zmm5, zmm10, 11111111b	;; I8_7	I7_7 I4_7 I3_7 I6_7 I5_7 I2_7 I1_7 (first I8 & R16)	; 64		n 68
	vshufpd	zmm5, zmm5, zmm10, 00000000b	;; I8_6	I7_6 I4_6 I3_6 I6_6 I5_6 I2_6 I1_6 (first I7 & R15)	; 65		n 70

	vshufpd	zmm10, zmm4, zmm0, 11111111b	;; I8_3 I7_3 I4_3 I3_3 I6_3 I5_3 I2_3 I1_3 (first I4 & R12)	; 66		n 68
	vshufpd	zmm4, zmm4, zmm0, 00000000b	;; I8_2 I7_2 I4_2 I3_2 I6_2 I5_2 I2_2 I1_2 (first I3 & R11)	; 67		n 70

	vshufpd	zmm0, zmm16, zmm13, 11111111b	;; I8_1 I7_1 I4_1 I3_1 I6_1 I5_1 I2_1 I1_1 (first I2 & R10)	; 68		n 72
	vshufpd	zmm16, zmm16, zmm13, 00000000b	;; I8_0 I7_0 I4_0 I3_0 I6_0 I5_0 I2_0 I1_0 (first I1 & R9)	; 69		n 74

						;; Simultaneously do an 8-complex and 16-reals
						;; 8-complex			16-reals
						;;				R12/R16 becomes newer R12/I12
						;;				mul R12/I12 by w^3 = .383 + .924i
	zfnmaddpd zmm6, zmm15, zmm26, zmm10	;; I4 - I8*1 (new I8)		R12/.383 = R12 - I12*.924/.383	; 68-71		n 76
	zfmaddpd zmm10, zmm10, zmm26, zmm15	;; I4*1 + I8 (new I4)		I12/.383 = R12*.924/.383 + I12	; 69-72		n 81

	vshufpd	zmm13, zmm8, zmm12, 11111111b	;; I8_5 I7_5 I4_5 I3_5 I6_5 I5_5 I2_5 I1_5 (first I6 & R14)	; 70		n 72
	vshufpd	zmm8, zmm8, zmm12, 00000000b	;; I8_4 I7_4 I4_4 I3_4 I6_4 I5_4 I2_4 I1_4 (first I5 & R13)	; 71		n 74

						;;				R11/R15 becomes newer R11/I11
						;;				mul R11/I11 by SQRTHALF + i*SQRTHALF
	vaddpd	zmm15, zmm4, zmm5		;; I3 + I7 (new I3)		I11/SQRTHALF = R11 + I11	; 70-73		n 84
	vsubpd	zmm4, zmm4, zmm5		;; I3 - I7 (new I7)		R11/SQRTHALF = R11 - I11	; 71-74		n 86

	vshufpd	zmm12, zmm9, zmm11, 11111111b	;; R8_5 R7_5 R4_5 R3_5 R6_5 R5_5 R2_5 R1_5 (R2-R6 = new R6)	; 72		n 78
	vshufpd	zmm9, zmm9, zmm11, 00000000b	;; R8_4 R7_4 R4_4 R3_4 R6_4 R5_4 R2_4 R1_4 (R1-R5 = new R5)	; 73		n 84

						;;				R10/R14 becomes newer R10/I10
						;;				mul R10/I10 by w^1 = .924 + .383i
	zfmsubpd zmm5, zmm0, zmm26, zmm13	;; I2*1 - I6 (new I6)		R10/.383 = R10*.924/.383 - I10	; 72-75		n 80
	zfmaddpd zmm0, zmm13, zmm26, zmm0	;; I2 + I6*1 (new I2)		I10/.383 = I10*.924/.383 + R10	; 73-76		n 81

	vshufpd	zmm11, zmm19, zmm3, 11111111b	;; R8_7	R7_7 R4_7 R3_7 R6_7 R5_7 R2_7 R1_7 (R4-R8 = new R8)	; 74		n 76
	vshufpd	zmm19, zmm19, zmm3, 00000000b	;; R8_6	R7_6 R4_6 R3_6 R6_6 R5_6 R2_6 R1_6 (R3-R7 = new R7)	; 75		n 84

						;;				R9/R13 becomes newer R9/I9
	vaddpd	zmm13 {k7}{z}, zmm16, zmm8	;; I1 + I5 (new I1)		0				; 74-77		n 85
	vsubpd	zmm8 {k7}, zmm16, zmm8		;; I1 - I5 (new I5)		I9				; 75-78		n 87

	vshufpd	zmm3, zmm2, zmm1, 11111111b	;; R8_1 R7_1 R4_1 R3_1 R6_1 R5_1 R2_1 R1_1 (R2+R6=  new R2)	; 76		n 83
	vshufpd	zmm2, zmm2, zmm1, 00000000b	;; R8_0 R7_0 R4_0 R3_0 R6_0 R5_0 R2_0 R1_0 (R1+R5 = new R1)	; 77		n 82

						;; next FFT level
						;;				R6/R8 becomes newest R6/I6
						;;				mul R6/I6 by w^2 = .707 + .707i
	vblendmpd zmm18 {k6}, zmm6, zmm11	;; I8				I6				; 76		n 78
	vblendmpd zmm11 {k6}, zmm11, zmm6	;; R8				R12				; 77		n 80

	vshufpd	zmm1, zmm14, zmm7, 11111111b	;; R8_3 R7_3 R4_3 R3_3 R6_3 R5_3 R2_3 R1_3 (R4+R8 = new R4)	; 78		n 83
	vshufpd	zmm14, zmm14, zmm7, 00000000b	;; R8_2 R7_2 R4_2 R3_2 R6_2 R5_2 R2_2 R1_2 (R3+R7 = new R3)	; 79		n 82

	vsubpd	zmm17, zmm12, zmm18		;; R6 - I8 (new2 R6)		R6 - I6 (newest R6/SQRTHALF)	; 78-12		n 88
	vaddpd	zmm12, zmm12, zmm18		;; R6 + I8 (new2 R8)		R6 + I6 (newest I6/SQRTHALF)	; 79-12		n 90

	vaddpd	zmm6, zmm5, zmm11		;; I6 + R8 (new2 I6)		R10 + R12 (newest R10/.383)	; 80-13		n 88
	vsubpd	zmm5, zmm5, zmm11		;; I6 - R8 (new2 I8)		R10 - R12 (newest R12/.383)	; 80-13		n 90

	vaddpd	zmm11, zmm0, zmm10		;; I2 + I4 (newer I2)		I10 + I12 (newest I10/.383)	; 81-14		n 88
	vsubpd	zmm0, zmm0, zmm10		;; I2 - I4 (newer I4)		I10 - I12 (newest I12/.383)	; 81-14		n 89

	vaddpd	zmm10, zmm2, zmm14		;; R1 + R3 (newer R1)		R1 + R3 (newest R1)		; 82-15		n 89
	vsubpd	zmm2, zmm2, zmm14		;; R1 - R3 (newer R3)		R1 - R3 (newest R3)		; 82-15		n 91

	vaddpd	zmm14, zmm3, zmm1		;; R2 + R4 (newer R2)		R2 + R4 (newest R2)		; 83-86		n 89
	vsubpd	zmm3, zmm3, zmm1		;; R2 - R4 (newer R4)		R2 - R4 (newest R4)		; 83-16		n 91

	vblendmpd zmm18 {k6}, zmm19, zmm15	;; R7				I11/SQRTHALF			; 84		n 87
	vblendmpd zmm16 {k6}, zmm9, zmm16	;; R5				R9				; 84		n 86

						;;				R5/R7 becomes newest R5/I5
	vsubpd	zmm19 {k7}, zmm13, zmm15	;; I1 - I3 (newer I3)		I5				; 85-17		n 95
	vaddpd	zmm13 {k7}{z}, zmm13, zmm15	;; I1 + I3 (newer I1)		0				; 85-17		n 93

	zfnmaddpd zmm7, zmm4, zmm25, zmm16	;; R5 - I7*1 (newer R5)		R9 - R11*-SQRTHALF (newest R9)	; 86-18		n 96
	zfmaddpd zmm4, zmm4, zmm25, zmm16	;; R5 + I7*1 (newer R7)		R9 + R11*-SQRTHALF (newest R11)	; 86-18		n 98

	zfmaddpd zmm16, zmm18, zmm27, zmm8	;; I5 + R7*1 (newer I5)		I9 + I11*SQRTHALF (newest I9)	; 87-19		n 97
	zfnmaddpd zmm18, zmm18, zmm27, zmm8	;; I5 - R7*1 (newer I7)		I9 - I11*SQRTHALF (newest I11)	; 87-19		n 99

	vmovapd	zmm8, zmm11			;;				I10/.383
	vaddpd	zmm8 {k7}, zmm17, zmm6		;; I6 = R6 + I6 (newer I6/SQRTHALF)	I10/.383		; 88-91		n 97
	vsubpd	zmm6 {k7}, zmm17, zmm6		;; R6 = R6 - I6 (newer R6/SQRTHALF)	R10/.383		; 88-91		n 96

	vblendmpd zmm17 {k6}, zmm0, zmm17	;; I4				R6/SQRTHALF			; 89		n 94
	vsubpd	zmm11 {k6}, zmm10, zmm14	;; I2				R1 - R2 (final R1b)		; 89-92		n 93

	vaddpd	zmm0 {k7}, zmm12, zmm5		;; I8 = R8 + I8 (newer I8/SQRTHALF)	I12/.383		; 90-92		n 98
	vsubpd	zmm5 {k7}, zmm12, zmm5		;; R8 = R8 - I8 (newer R8/SQRTHALF)	R12/.383		; 90-92		n 99

	;; last FFT level

	vblendmpd zmm9 {k6}, zmm2, zmm9		;; R3				R5				; 91		n 94
	vblendmpd zmm12 {k6}, zmm3, zmm12	;; R4				I6/SQRTHALF			; 91		n 95

						;;				R3/R4 becomes final R3/I3
	vaddpd	zmm1, zmm10, zmm14		;; R1 + R2 (final R1)		R1 + R2 (final R1a => R1)	; 92-95		n 
	vsubpd	zmm2 {k7}, zmm10, zmm14		;; R1 - R2 (final R2)		R3 => R2			; 92-95		n 

	vsubpd	zmm3 {k7}, zmm13, zmm11		;; I1 - I2 (final I2)		I3 => I2			; 93-96		n 
	vaddpd	zmm11 {k7}, zmm13, zmm11	;; I1 + I2 (final I1)		R1b => I1			; 93-96		n 

	zfnmaddpd zmm13, zmm17, zmm25, zmm9	;; R3 - I4*1 (final R3)		R5 - R6*-SQRTHALF (final R5 => R3) ; 94-97	n 
	zfmaddpd zmm9, zmm17, zmm25, zmm9	;; R3 + I4*1 (final R4)		R5 + R6*-SQRTHALF (final R6 => R4) ; 94-97	n 

	zfmaddpd zmm15, zmm12, zmm27, zmm19	;; I3 + R4*1 (final I3)		I5 + I6*SQRTHALF (final I5 => I3) ; 95-98	n 
	zfnmaddpd zmm12, zmm12, zmm27, zmm19	;; I3 - R4*1 (final I4)		I5 - I6*SQRTHALF (final I6 => I4) ; 95-98	n 

	zfmaddpd zmm10, zmm6, zmm28, zmm7	;; R5 + R6*SQRTHALF (final R5)	R9 + R10*.383 (final R9 => R5)	; 96-99		n 
	zfnmaddpd zmm6, zmm6, zmm28, zmm7	;; R5 - R6*SQRTHALF (final R6)	R9 - R10*.383 (final R10 => R6)	; 96-99		n 

	zfmaddpd zmm7, zmm8, zmm28, zmm16	;; I5 + I6*SQRTHALF (final I5)	I9 + I10*.383 (final I9 => I5)	; 97-100	n 
	zfnmaddpd zmm8, zmm8, zmm28, zmm16	;; I5 - I6*SQRTHALF (final I6)	I9 - I10*.383 (final I10 => I6)	; 97-100	n 

	zfnmaddpd zmm14, zmm0, zmm28, zmm4	;; R7 - I8*SQRTHALF (final R7)	R11 - I12*.383 (final R11 => R7); 98-101	n 
	zfmaddpd zmm0, zmm0, zmm28, zmm4	;; R7 + I8*SQRTHALF (final R8)	R11 + I12*.383 (final R12 => R8); 98-101	n 

	zfmaddpd zmm4, zmm5, zmm28, zmm18	;; I7 + R8*SQRTHALF (final I7)	I11 + R12*.383 (final I11 => I7); 99-102	n 
	zfnmaddpd zmm5, zmm5, zmm28, zmm18	;; I7 - R8*SQRTHALF (final I8)	I11 - R12*.383 (final I12 => I8); 99-102	n 

	mov	al, mul4_opcode			;; Load the mul4_opcode
	cmp	al, 0				;; See if we need to do more than the original type-3 FFT multiply
	je	orig				;; Jump if nothing special
	mov	r9, SRC2ARG			;; Distance to s2 arg from gwaddmul4 or gwsubmul4 or s3 arg in gwmuladd4 or gwmulsub4
	jg	no_save_fft			;; Jump if not saving result of the forward FFT?

	;; Store FFT results
	zstore	[srcreg], zmm1			;; R1			R1a
	zstore	[srcreg+64], zmm11		;; I1			R1b
	zstore	[srcreg+d1], zmm2		;;		R2
	zstore	[srcreg+d1+64], zmm3		;;		I2
	zstore	[srcreg+d2], zmm13		;;		R3
	zstore	[srcreg+d2+64], zmm15		;;		I3
	zstore	[srcreg+d2+d1], zmm9		;;		R4
	zstore	[srcreg+d2+d1+64], zmm12	;;		I4
	zstore	[srcreg+d4], zmm10		;;		R5
	zstore	[srcreg+d4+64], zmm7		;;		I5
	zstore	[srcreg+d4+d1], zmm6		;;		R6
	zstore	[srcreg+d4+d1+64], zmm8		;;		I6
	zstore	[srcreg+d4+d2], zmm14		;;		R7
	zstore	[srcreg+d4+d2+64], zmm4		;;		I7
	zstore	[srcreg+d4+d2+d1], zmm0		;;		R8
	zstore	[srcreg+d4+d2+d1+64], zmm5	;;		I8
	and	al, 7Fh				;; Mask out the save-FFT-results bit
	jz	orig				;; opcode == 0

no_save_fft:
	cmp	al, 2				;; Case off opcode
	jg	fma				;; 3,4=muladd,mulsub
	;jl	addmul				;; 1=addmul, fall through
	je	submul				;; 2=submul

	;; Multiply the results
	vmovapd	zmm24, [srcreg+rbp+d1]		;;		MemR2
	vaddpd	zmm24, zmm24, [srcreg+r9+d1]	;;		MemR2
	vmulpd	zmm17, zmm2, zmm24		;;		A2 = R2 * MemR2					; 1-4		n 5
	vmulpd	zmm16, zmm3, zmm24		;;		B2 = I2 * MemR2					; 1-4		n 5

	vmovapd	zmm24, [srcreg+rbp+d2]		;;		MemR3
 	vaddpd	zmm24, zmm24, [srcreg+r9+d2]	;;		MemR3
	vmulpd	zmm19, zmm13, zmm24		;;		A3 = R3 * MemR3					; 2-5		n 6
	vmulpd	zmm18, zmm15, zmm24		;;		B3 = I3 * MemR3					; 2-5		n 6

	vmovapd	zmm24, [srcreg+rbp+d2+d1]	;;		MemR4
	vaddpd	zmm24, zmm24, [srcreg+r9+d2+d1];;		MemR4
	vmulpd	zmm21, zmm9, zmm24		;;		A4 = R4 * MemR4					; 3-6		n 8
	vmulpd	zmm20, zmm12, zmm24		;;		B4 = I4 * MemR4					; 3-6		n 8

	vmovapd	zmm24, [srcreg+rbp]		;; MemR1			MemR1a
	vaddpd	zmm24, zmm24, [srcreg+r9]	;; MemR1			MemR1a
	vmulpd	zmm23, zmm1, zmm24		;; A1 = R1 * MemR1		R1a*MemR1a (R1a)		; 4-7		n 10
	vmulpd	zmm22 {k7}{z}, zmm11, zmm24	;; B1 = I1 * MemR1		0				; 4-7		n 10

	vmovapd zmm24, [srcreg+rbp+d1+64]	;;		MemI2
	vaddpd	zmm24, zmm24, [srcreg+r9+d1+64];;		MemI2
	zfnmaddpd zmm17, zmm3, zmm24, zmm17	;;		A2 - I2 * MemI2 (R2)				; 5-8		n 10
	zfmaddpd zmm16, zmm2, zmm24, zmm16	;;		B2 + R2 * MemI2 (I2)				; 5-8		n 18

	vmovapd zmm24, [srcreg+rbp+d2+64]	;;		MemI3
	vaddpd	zmm24, zmm24, [srcreg+r9+d2+64];;		MemI3
	zfnmaddpd zmm19, zmm15, zmm24, zmm19	;;		A3 - I3 * MemI3 (R3 & R5)			; 6-9		n 19
	zfmaddpd zmm18, zmm13, zmm24, zmm18	;;		B3 + R3 * MemI3 (I3 & I5)			; 6-9		n 20

	vmovapd	zmm24, [srcreg+rbp+d4]		;;		MemR5
	vaddpd	zmm24, zmm24, [srcreg+r9+d4]	;;		MemR5
	vmulpd	zmm3, zmm10, zmm24		;;		A5 = R5 * MemR5					; 7-10		n 12
	vmulpd	zmm2, zmm7, zmm24		;;		B5 = I5 * MemR5					; 7-10		n 13

	vmovapd zmm24, [srcreg+rbp+d2+d1+64]	;;		MemI4
	vaddpd	zmm24, zmm24, [srcreg+r9+d2+d1+64];;		MemI4
	zfnmaddpd zmm21, zmm12, zmm24, zmm21	;;		A4 - I4 * MemI4 (R4 & R6)			; 8-11		n 19
	zfmaddpd zmm20, zmm9, zmm24, zmm20	;;		B4 + R4 * MemI4 (I4 & I6)			; 8-11		n 20
	vmovsd	Q [srcreg+r8-16], xmm23		;;				Save product of FFT sums	; 8

	vmovapd	zmm24, [srcreg+rbp+d4+d1]	;;		MemR6
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+d1];;		MemR6
	vmulpd	zmm15, zmm6, zmm24		;;		A6 = R6 * MemR6					; 9-12		n 14
	vmulpd	zmm13, zmm8, zmm24		;;		B6 = I6 * MemR6					; 9-12		n 15

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	vaddpd	zmm24, zmm24, [srcreg+r9+64]	;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+d2];;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+64];;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+d2+d1];;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+d1+64];;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+d2+64];;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+d2+d1+64];;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

	jmp	back_to_orig

submul:
	vmovapd	zmm24, [srcreg+rbp+d1]		;;		MemR2
	vsubpd	zmm24, zmm24, [srcreg+r9+d1]	;;		MemR2
	vmulpd	zmm17, zmm2, zmm24		;;		A2 = R2 * MemR2					; 1-4		n 5
	vmulpd	zmm16, zmm3, zmm24		;;		B2 = I2 * MemR2					; 1-4		n 5

	vmovapd	zmm24, [srcreg+rbp+d2]		;;		MemR3
	vsubpd	zmm24, zmm24, [srcreg+r9+d2]	;;		MemR3
	vmulpd	zmm19, zmm13, zmm24		;;		A3 = R3 * MemR3					; 2-5		n 6
	vmulpd	zmm18, zmm15, zmm24		;;		B3 = I3 * MemR3					; 2-5		n 6

	vmovapd	zmm24, [srcreg+rbp+d2+d1]	;;		MemR4
	vsubpd	zmm24, zmm24, [srcreg+r9+d2+d1];;		MemR4
	vmulpd	zmm21, zmm9, zmm24		;;		A4 = R4 * MemR4					; 3-6		n 8
	vmulpd	zmm20, zmm12, zmm24		;;		B4 = I4 * MemR4					; 3-6		n 8

	vmovapd	zmm24, [srcreg+rbp]		;; MemR1			MemR1a
	vsubpd	zmm24, zmm24, [srcreg+r9]	;; MemR1			MemR1a
	vmulpd	zmm23, zmm1, zmm24		;; A1 = R1 * MemR1		R1a*MemR1a (R1a)		; 4-7		n 10
	vmulpd	zmm22 {k7}{z}, zmm11, zmm24	;; B1 = I1 * MemR1		0				; 4-7		n 10

	vmovapd zmm24, [srcreg+rbp+d1+64]	;;		MemI2
	vsubpd	zmm24, zmm24, [srcreg+r9+d1+64];;		MemI2
	zfnmaddpd zmm17, zmm3, zmm24, zmm17	;;		A2 - I2 * MemI2 (R2)				; 5-8		n 10
	zfmaddpd zmm16, zmm2, zmm24, zmm16	;;		B2 + R2 * MemI2 (I2)				; 5-8		n 18

	vmovapd zmm24, [srcreg+rbp+d2+64]	;;		MemI3
	vsubpd	zmm24, zmm24, [srcreg+r9+d2+64];;		MemI3
	zfnmaddpd zmm19, zmm15, zmm24, zmm19	;;		A3 - I3 * MemI3 (R3 & R5)			; 6-9		n 19
	zfmaddpd zmm18, zmm13, zmm24, zmm18	;;		B3 + R3 * MemI3 (I3 & I5)			; 6-9		n 20

	vmovapd	zmm24, [srcreg+rbp+d4]		;;		MemR5
	vsubpd	zmm24, zmm24, [srcreg+r9+d4]	;;		MemR5
	vmulpd	zmm3, zmm10, zmm24		;;		A5 = R5 * MemR5					; 7-10		n 12
	vmulpd	zmm2, zmm7, zmm24		;;		B5 = I5 * MemR5					; 7-10		n 13

	vmovapd zmm24, [srcreg+rbp+d2+d1+64]	;;		MemI4
	vsubpd	zmm24, zmm24, [srcreg+r9+d2+d1+64];;		MemI4
	zfnmaddpd zmm21, zmm12, zmm24, zmm21	;;		A4 - I4 * MemI4 (R4 & R6)			; 8-11		n 19
	zfmaddpd zmm20, zmm9, zmm24, zmm20	;;		B4 + R4 * MemI4 (I4 & I6)			; 8-11		n 20
	vmovsd	Q [srcreg-16], xmm23		;;				Save product of FFT sums	; 8

	vmovapd	zmm24, [srcreg+rbp+d4+d1]	;;		MemR6
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+d1];;		MemR6
	vmulpd	zmm15, zmm6, zmm24		;;		A6 = R6 * MemR6					; 9-12		n 14
	vmulpd	zmm13, zmm8, zmm24		;;		B6 = I6 * MemR6					; 9-12		n 15

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	vsubpd	zmm24, zmm24, [srcreg+r9+64]	;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+d2];;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+64];;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+d2+d1];;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+d1+64];;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+d2+64];;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+d2+d1+64];;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

	jmp	back_to_orig

fma:
	;; Multiply the results
	vmovapd	zmm24, [srcreg+rbp+d1]		;;		MemR2
	vmulpd	zmm17, zmm2, zmm24		;;		A2 = R2 * MemR2					; 1-4		n 5
	vmulpd	zmm16, zmm3, zmm24		;;		B2 = I2 * MemR2					; 1-4		n 5

 	vmovapd	zmm24, [srcreg+rbp+d2]		;;		MemR3
	vmulpd	zmm19, zmm13, zmm24		;;		A3 = R3 * MemR3					; 2-5		n 6
	vmulpd	zmm18, zmm15, zmm24		;;		B3 = I3 * MemR3					; 2-5		n 6

	vmovapd	zmm24, [srcreg+rbp+d2+d1]	;;		MemR4
	vmulpd	zmm21, zmm9, zmm24		;;		A4 = R4 * MemR4					; 3-6		n 8
	vmulpd	zmm20, zmm12, zmm24		;;		B4 = I4 * MemR4					; 3-6		n 8

	vmovapd	zmm24, [srcreg+rbp]		;; MemR1			MemR1a
	vmulpd	zmm23, zmm1, zmm24		;; A1 = R1 * MemR1		R1a*MemR1a (R1a)		; 4-7		n 10
	vmulpd	zmm22 {k7}{z}, zmm11, zmm24	;; B1 = I1 * MemR1		0				; 4-7		n 10

	vmovapd zmm24, [srcreg+rbp+d1+64]	;;		MemI2
	zfnmaddpd zmm17, zmm3, zmm24, zmm17	;;		A2 - I2 * MemI2 (R2)				; 5-8		n 10
	zfmaddpd zmm16, zmm2, zmm24, zmm16	;;		B2 + R2 * MemI2 (I2)				; 5-8		n 18

	vmovapd zmm24, [srcreg+rbp+d2+64]	;;		MemI3
	zfnmaddpd zmm19, zmm15, zmm24, zmm19	;;		A3 - I3 * MemI3 (R3 & R5)			; 6-9		n 19
	zfmaddpd zmm18, zmm13, zmm24, zmm18	;;		B3 + R3 * MemI3 (I3 & I5)			; 6-9		n 20

	vmovapd	zmm24, [srcreg+rbp+d4]		;;		MemR5
	vmulpd	zmm3, zmm10, zmm24		;;		A5 = R5 * MemR5					; 7-10		n 12
	vmulpd	zmm2, zmm7, zmm24		;;		B5 = I5 * MemR5					; 7-10		n 13

	vmovapd zmm24, [srcreg+rbp+d2+d1+64]	;;		MemI4
	zfnmaddpd zmm21, zmm12, zmm24, zmm21	;;		A4 - I4 * MemI4 (R4 & R6)			; 8-11		n 19
	zfmaddpd zmm20, zmm9, zmm24, zmm20	;;		B4 + R4 * MemI4 (I4 & I6)			; 8-11		n 20
	vmovsd	Q [srcreg-16], xmm23		;;				Save product of FFT sums	; 8

	vmovapd	zmm24, [srcreg+rbp+d4+d1]	;;		MemR6
	vmulpd	zmm15, zmm6, zmm24		;;		A6 = R6 * MemR6					; 9-12		n 14
	vmulpd	zmm13, zmm8, zmm24		;;		B6 = I6 * MemR6					; 9-12		n 15

	mov	r10, SRC3ARG			;; Load distance to FFT(1)
	cmp	al, 4				;; Case off opcode
	;jb	muladd				;; 3=muladd, fall through
	je	mulsub				;; 4=mulsub

muladd:	cmp	r10, 1				;; Check if we need to multiply add-in by FFT(1)
	jne	muladdhard			;; Yes, do it the hard way

	vaddpd	zmm17, zmm17, [srcreg+r9+d1]		;; R2 = R2 + MemR2
	vaddpd	zmm16, zmm16, [srcreg+r9+d1+64]		;; I2 = I2 + MemI2
	vaddpd	zmm19, zmm19, [srcreg+r9+d2]		;; R3 = R3 + MemR3
	vaddpd	zmm18, zmm18, [srcreg+r9+d2+64]		;; I3 = I3 + MemI3
	vaddpd	zmm21, zmm21, [srcreg+r9+d2+d1]		;; R4 = R4 + MemR4
	vaddpd	zmm20, zmm20, [srcreg+r9+d2+d1+64]	;; I4 = I4 + MemI4

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

	vaddpd	zmm23, zmm23, [srcreg+r9]		;; R1 = R1 + MemR1
	vaddpd	zmm12{k6}, zmm12, [srcreg+r9+64]	;; R1b = R1b + MemI1
	vaddpd	zmm22{k7}{z}, zmm22, [srcreg+r9+64]	;; I1 = I1 + MemI1
	vaddpd	zmm3, zmm3, [srcreg+r9+d4]		;; R5 = R5 + MemR5
	vaddpd	zmm2, zmm2, [srcreg+r9+d4+64]		;; I5 = I5 + MemI5

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

	vaddpd	zmm15, zmm15, [srcreg+r9+d4+d1]		;; R6 = R6 + MemR6
	vaddpd	zmm13, zmm13, [srcreg+r9+d4+d1+64]	;; I6 = I6 + MemI6
	vaddpd	zmm9, zmm9, [srcreg+r9+d4+d2]		;; R7 = R7 + MemR7
	vaddpd	zmm1, zmm1, [srcreg+r9+d4+d2+64]	;; I7 = I7 + MemI7
	vaddpd	zmm11, zmm11, [srcreg+r9+d4+d2+d1]	;; R8 = R8 + MemR8
	vaddpd	zmm7, zmm7, [srcreg+r9+d4+d2+d1+64]	;; I8 = I8 + MemI8

	jmp	back_to_orig

muladdhard:
	vmovapd	zmm9, [srcreg+r9+d1]			;; MemR2
	vmovapd	zmm12, [srcreg+r10+d1]			;; MemR2#2
	vmovapd	zmm24, [srcreg+r10+d1+64]		;; MemI2#2
	zfmaddpd zmm17, zmm9, zmm12, zmm17		;; R2 = R2 + MemR2*MemR2#2
	zfmaddpd zmm16, zmm9, zmm24, zmm16		;; I2 = I2 + MemR2*MemI2#2
	vmovapd	zmm9, [srcreg+r9+d1+64]			;; MemI2
	zfnmaddpd zmm17, zmm9, zmm24, zmm17		;; R2 = R2 - MemI2*MemI2#2
	zfmaddpd zmm16, zmm9, zmm12, zmm16		;; I2 = I2 + MemI2*MemR2#2

	vmovapd	zmm9, [srcreg+r9+d2]			;; MemR3
	vmovapd	zmm12, [srcreg+r10+d2]			;; MemR3#2
	vmovapd	zmm24, [srcreg+r10+d2+64]		;; MemI3#2
	zfmaddpd zmm19, zmm9, zmm12, zmm19		;; R3 = R3 + MemR3*MemR3#2
	zfmaddpd zmm18, zmm9, zmm24, zmm18		;; I3 = I3 + MemR3*MemI3#2
	vmovapd	zmm9, [srcreg+r9+d2+64]			;; MemI3
	zfnmaddpd zmm19, zmm9, zmm24, zmm19		;; R3 = R3 - MemI3*MemI3#2
	zfmaddpd zmm18, zmm9, zmm12, zmm18		;; I3 = I3 + MemI3*MemR3#2

	vmovapd	zmm9, [srcreg+r9+d2+d1]			;; MemR4
	vmovapd	zmm12, [srcreg+r10+d2+d1]		;; MemR4#2
	vmovapd	zmm24, [srcreg+r10+d2+d1+64]		;; MemI4#2
	zfmaddpd zmm21, zmm9, zmm12, zmm21		;; R4 = R4 + MemR4*MemR4#2
	zfmaddpd zmm20, zmm9, zmm24, zmm20		;; I4 = I4 + MemR4*MemI4#2
	vmovapd	zmm9, [srcreg+r9+d2+d1+64]		;; MemI4
	zfnmaddpd zmm21, zmm9, zmm24, zmm21		;; R4 = R4 - MemI4*MemI4#2
	zfmaddpd zmm20, zmm9, zmm12, zmm20		;; I4 = I4 + MemI4*MemR4#2

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

	vmovapd	zmm10, [srcreg+r9]			;; MemR1
	vmovapd	zmm6, [srcreg+r10]			;; MemR1#2
	vmovapd	zmm8, [srcreg+r10+64]			;; MemI1#2
	zfmaddpd zmm23, zmm10, zmm6, zmm23		;; R1 = R1 + MemR1*MemR1#2
	zfmaddpd zmm22{k7}{z}, zmm10, zmm8, zmm22	;; I1 = I1 + MemR1*MemI1#2
	vmovapd	zmm10, [srcreg+r9+64]			;; MemI1
	zfnmaddpd zmm23{k7}, zmm10, zmm8, zmm23		;; R1 = R1 - MemI1*MemI1#2
	zfmaddpd zmm12{k6}, zmm10, zmm8, zmm12		;; R1b = R1b + MemI1*MemI1#2
	zfmaddpd zmm22{k7}{z}, zmm10, zmm6, zmm22	;; I1 = I1 + MemI1*MemR1#2

	vmovapd	zmm10, [srcreg+r9+d4]			;; MemR5
	vmovapd	zmm6, [srcreg+r10+d4]			;; MemR5#2
	vmovapd	zmm8, [srcreg+r10+d4+64]		;; MemI5#2
	zfmaddpd zmm3, zmm10, zmm6, zmm3		;; R5 = R5 + MemR5*MemR5#2
	zfmaddpd zmm2, zmm10, zmm8, zmm2		;; I5 = I5 + MemR5*MemI5#2
	vmovapd	zmm10, [srcreg+r9+d4+64]		;; MemI5
	zfnmaddpd zmm3, zmm10, zmm8, zmm3		;; R5 = R5 - MemI5*MemI5#2
	zfmaddpd zmm2, zmm10, zmm6, zmm2		;; I5 = I5 + MemI5*MemR5#2

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

	vmovapd	zmm4, [srcreg+r9+d4+d1]			;; MemR6
	vmovapd	zmm6, [srcreg+r10+d4+d1]		;; MemR6#2
	vmovapd	zmm8, [srcreg+r10+d4+d1+64]		;; MemI6#2
	zfmaddpd zmm15, zmm4, zmm6, zmm15		;; R6 = R6 + MemR6*MemR6#2
	zfmaddpd zmm13, zmm4, zmm8, zmm13		;; I6 = I6 + MemR6*MemI6#2
	vmovapd	zmm4, [srcreg+r9+d4+d1+64]		;; MemI6
	zfnmaddpd zmm15, zmm4, zmm8, zmm15		;; R6 = R6 - MemI6*MemI6#2
	zfmaddpd zmm13, zmm4, zmm6, zmm13		;; I6 = I6 + MemI6*MemR6#2

	vmovapd	zmm4, [srcreg+r9+d4+d2]			;; MemR7
	vmovapd	zmm6, [srcreg+r10+d4+d2]		;; MemR7#2
	vmovapd	zmm8, [srcreg+r10+d4+d2+64]		;; MemI7#2
	zfmaddpd zmm9, zmm4, zmm6, zmm9			;; R7 = R7 + MemR7*MemR7#2
	zfmaddpd zmm1, zmm4, zmm8, zmm1			;; I7 = I7 + MemR7*MemI7#2
	vmovapd	zmm4, [srcreg+r9+d4+d2+64]		;; MemI7
	zfnmaddpd zmm9, zmm4, zmm8, zmm9		;; R7 = R7 - MemI7*MemI7#2
	zfmaddpd zmm1, zmm4, zmm6, zmm1			;; I7 = I7 + MemI7*MemR7#2

	vmovapd	zmm4, [srcreg+r9+d4+d2+d1]		;; MemR8
	vmovapd	zmm6, [srcreg+r10+d4+d2+d1]		;; MemR8#2
	vmovapd	zmm8, [srcreg+r10+d4+d2+d1+64]		;; MemI8#2
	zfmaddpd zmm11, zmm4, zmm6, zmm11		;; R8 = R8 + MemR8*MemR8#2
	zfmaddpd zmm7, zmm4, zmm8, zmm7			;; I8 = I8 + MemR8*MemI8#2
	vmovapd	zmm4, [srcreg+r9+d4+d2+d1+64]		;; MemI8
	zfnmaddpd zmm11, zmm4, zmm8, zmm11		;; R8 = R8 - MemI8*MemI8#2
	zfmaddpd zmm7, zmm4, zmm6, zmm7			;; I8 = I8 + MemI8*MemR8#2

	jmp	back_to_orig

mulsub:	cmp	r10, 1				;; Check if we need to multiply add-in by FFT(1)
	jne	mulsubhard			;; Yes, do it the hard way

	vsubpd	zmm17, zmm17, [srcreg+r9+d1]		;; R2 = R2 - MemR2
	vsubpd	zmm16, zmm16, [srcreg+r9+d1+64]		;; I2 = I2 - MemI2
	vsubpd	zmm19, zmm19, [srcreg+r9+d2]		;; R3 = R3 - MemR3
	vsubpd	zmm18, zmm18, [srcreg+r9+d2+64]		;; I3 = I3 - MemI3
	vsubpd	zmm21, zmm21, [srcreg+r9+d2+d1]		;; R4 = R4 - MemR4
	vsubpd	zmm20, zmm20, [srcreg+r9+d2+d1+64]	;; I4 = I4 - MemI4

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

	vsubpd	zmm23, zmm23, [srcreg+r9]		;; R1 = R1 - MemR1
	vsubpd	zmm12{k6}, zmm12, [srcreg+r9+64]	;; R1b = R1b - MemI1
	vsubpd	zmm22{k7}, zmm22, [srcreg+r9+64]	;; I1 = I1 - MemI1
	vsubpd	zmm3, zmm3, [srcreg+r9+d4]		;; R5 = R5 - MemR5
	vsubpd	zmm2, zmm2, [srcreg+r9+d4+64]		;; I5 = I5 - MemI5

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

	vsubpd	zmm15, zmm15, [srcreg+r9+d4+d1]		;; R6 = R6 - MemR6
	vsubpd	zmm13, zmm13, [srcreg+r9+d4+d1+64]	;; I6 = I6 - MemI6
	vsubpd	zmm9, zmm9, [srcreg+r9+d4+d2]		;; R7 = R7 - MemR7
	vsubpd	zmm1, zmm1, [srcreg+r9+d4+d2+64]	;; I7 = I7 - MemI7
	vsubpd	zmm11, zmm11, [srcreg+r9+d4+d2+d1]	;; R8 = R8 - MemR8
	vsubpd	zmm7, zmm7, [srcreg+r9+d4+d2+d1+64]	;; I8 = I8 - MemI8

	jmp	back_to_orig

mulsubhard:
	vmovapd	zmm9, [srcreg+r9+d1]			;; MemR2
	vmovapd	zmm12, [srcreg+r10+d1]			;; MemR2#2
	vmovapd	zmm24, [srcreg+r10+d1+64]		;; MemI2#2
	zfnmaddpd zmm17, zmm9, zmm12, zmm17		;; R2 = R2 - MemR2*MemR2#2
	zfnmaddpd zmm16, zmm9, zmm24, zmm16		;; I2 = I2 - MemR2*MemI2#2
	vmovapd	zmm9, [srcreg+r9+d1+64]			;; MemI2
	zfmaddpd zmm17, zmm9, zmm24, zmm17		;; R2 = R2 + MemI2*MemI2#2
	zfnmaddpd zmm16, zmm9, zmm12, zmm16		;; I2 = I2 - MemI2*MemR2#2

	vmovapd	zmm9, [srcreg+r9+d2]			;; MemR3
	vmovapd	zmm12, [srcreg+r10+d2]			;; MemR3#2
	vmovapd	zmm24, [srcreg+r10+d2+64]		;; MemI3#2
	zfnmaddpd zmm19, zmm9, zmm12, zmm19		;; R3 = R3 - MemR3*MemR3#2
	zfnmaddpd zmm18, zmm9, zmm24, zmm18		;; I3 = I3 - MemR3*MemI3#2
	vmovapd	zmm9, [srcreg+r9+d2+64]			;; MemI3
	zfmaddpd zmm19, zmm9, zmm24, zmm19		;; R3 = R3 + MemI3*MemI3#2
	zfnmaddpd zmm18, zmm9, zmm12, zmm18		;; I3 = I3 - MemI3*MemR3#2

	vmovapd	zmm9, [srcreg+r9+d2+d1]			;; MemR4
	vmovapd	zmm12, [srcreg+r10+d2+d1]		;; MemR4#2
	vmovapd	zmm24, [srcreg+r10+d2+d1+64]		;; MemI4#2
	zfnmaddpd zmm21, zmm9, zmm12, zmm21		;; R4 = R4 - MemR4*MemR4#2
	zfnmaddpd zmm20, zmm9, zmm24, zmm20		;; I4 = I4 - MemR4*MemI4#2
	vmovapd	zmm9, [srcreg+r9+d2+d1+64]		;; MemI4
	zfmaddpd zmm21, zmm9, zmm24, zmm21		;; R4 = R4 + MemI4*MemI4#2
	zfnmaddpd zmm20, zmm9, zmm12, zmm20		;; I4 = I4 - MemI4*MemR4#2

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

	vmovapd	zmm10, [srcreg+r9]			;; MemR1
	vmovapd	zmm6, [srcreg+r10]			;; MemR1#2
	vmovapd	zmm8, [srcreg+r10+64]			;; MemI1#2
	zfnmaddpd zmm23, zmm10, zmm6, zmm23		;; R1 = R1 - MemR1*MemR1#2
	zfnmaddpd zmm22{k7}{z}, zmm10, zmm8, zmm22	;; I1 = I1 - MemR1*MemI1#2
	vmovapd	zmm10, [srcreg+r9+64]			;; MemI1
	zfmaddpd zmm23{k7}, zmm10, zmm8, zmm23		;; R1 = R1 + MemI1*MemI1#2
	zfnmaddpd zmm12{k6}, zmm10, zmm8, zmm12		;; R1b = R1b - MemI1*MemI1#2
	zfnmaddpd zmm22{k7}{z}, zmm10, zmm6, zmm22	;; I1 = I1 - MemI1*MemR1#2

	vmovapd	zmm10, [srcreg+r9+d4]			;; MemR5
	vmovapd	zmm6, [srcreg+r10+d4]			;; MemR5#2
	vmovapd	zmm8, [srcreg+r10+d4+64]		;; MemI5#2
	zfnmaddpd zmm3, zmm10, zmm6, zmm3		;; R5 = R5 - MemR5*MemR5#2
	zfnmaddpd zmm2, zmm10, zmm8, zmm2		;; I5 = I5 - MemR5*MemI5#2
	vmovapd	zmm10, [srcreg+r9+d4+64]		;; MemI5
	zfmaddpd zmm3, zmm10, zmm8, zmm3		;; R5 = R5 + MemI5*MemI5#2
	zfnmaddpd zmm2, zmm10, zmm6, zmm2		;; I5 = I5 - MemI5*MemR5#2

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

	vmovapd	zmm4, [srcreg+r9+d4+d1]			;; MemR6
	vmovapd	zmm6, [srcreg+r10+d4+d1]		;; MemR6#2
	vmovapd	zmm8, [srcreg+r10+d4+d1+64]		;; MemI6#2
	zfnmaddpd zmm15, zmm4, zmm6, zmm15		;; R6 = R6 - MemR6*MemR6#2
	zfnmaddpd zmm13, zmm4, zmm8, zmm13		;; I6 = I6 - MemR6*MemI6#2
	vmovapd	zmm4, [srcreg+r9+d4+d1+64]		;; MemI6
	zfmaddpd zmm15, zmm4, zmm8, zmm15		;; R6 = R6 + MemI6*MemI6#2
	zfnmaddpd zmm13, zmm4, zmm6, zmm13		;; I6 = I6 - MemI6*MemR6#2

	vmovapd	zmm4, [srcreg+r9+d4+d2]			;; MemR7
	vmovapd	zmm6, [srcreg+r10+d4+d2]		;; MemR7#2
	vmovapd	zmm8, [srcreg+r10+d4+d2+64]		;; MemI7#2
	zfnmaddpd zmm9, zmm4, zmm6, zmm9		;; R7 = R7 - MemR7*MemR7#2
	zfnmaddpd zmm1, zmm4, zmm8, zmm1		;; I7 = I7 - MemR7*MemI7#2
	vmovapd	zmm4, [srcreg+r9+d4+d2+64]		;; MemI7
	zfmaddpd zmm9, zmm4, zmm8, zmm9			;; R7 = R7 + MemI7*MemI7#2
	zfnmaddpd zmm1, zmm4, zmm6, zmm1		;; I7 = I7 - MemI7*MemR7#2

	vmovapd	zmm4, [srcreg+r9+d4+d2+d1]		;; MemR8
	vmovapd	zmm6, [srcreg+r10+d4+d2+d1]		;; MemR8#2
	vmovapd	zmm8, [srcreg+r10+d4+d2+d1+64]		;; MemI8#2
	zfnmaddpd zmm11, zmm4, zmm6, zmm11		;; R8 = R8 - MemR8*MemR8#2
	zfnmaddpd zmm7, zmm4, zmm8, zmm7		;; I8 = I8 - MemR8*MemI8#2
	vmovapd	zmm4, [srcreg+r9+d4+d2+d1+64]		;; MemI8
	zfmaddpd zmm11, zmm4, zmm8, zmm11		;; R8 = R8 + MemI8*MemI8#2
	zfnmaddpd zmm7, zmm4, zmm6, zmm7		;; I8 = I8 - MemI8*MemR8#2

	jmp	back_to_orig

orig:
	;; Multiply the results
	vmovapd	zmm24, [srcreg+rbp+d1]		;;		MemR2
	vmulpd	zmm17, zmm2, zmm24		;;		A2 = R2 * MemR2					; 1-4		n 5
	vmulpd	zmm16, zmm3, zmm24		;;		B2 = I2 * MemR2					; 1-4		n 5

 	vmovapd	zmm24, [srcreg+rbp+d2]		;;		MemR3
	vmulpd	zmm19, zmm13, zmm24		;;		A3 = R3 * MemR3					; 2-5		n 6
	vmulpd	zmm18, zmm15, zmm24		;;		B3 = I3 * MemR3					; 2-5		n 6

	vmovapd	zmm24, [srcreg+rbp+d2+d1]	;;		MemR4
	vmulpd	zmm21, zmm9, zmm24		;;		A4 = R4 * MemR4					; 3-6		n 8
	vmulpd	zmm20, zmm12, zmm24		;;		B4 = I4 * MemR4					; 3-6		n 8

	vmovapd	zmm24, [srcreg+rbp]		;; MemR1			MemR1a
	vmulpd	zmm23, zmm1, zmm24		;; A1 = R1 * MemR1		R1a*MemR1a (R1a)		; 4-7		n 10
	vmulpd	zmm22 {k7}{z}, zmm11, zmm24	;; B1 = I1 * MemR1		0				; 4-7		n 10

	vmovapd zmm24, [srcreg+rbp+d1+64]	;;		MemI2
	zfnmaddpd zmm17, zmm3, zmm24, zmm17	;;		A2 - I2 * MemI2 (R2)				; 5-8		n 10
	zfmaddpd zmm16, zmm2, zmm24, zmm16	;;		B2 + R2 * MemI2 (I2)				; 5-8		n 18

	vmovapd zmm24, [srcreg+rbp+d2+64]	;;		MemI3
	zfnmaddpd zmm19, zmm15, zmm24, zmm19	;;		A3 - I3 * MemI3 (R3 & R5)			; 6-9		n 19
	zfmaddpd zmm18, zmm13, zmm24, zmm18	;;		B3 + R3 * MemI3 (I3 & I5)			; 6-9		n 20

	vmovapd	zmm24, [srcreg+rbp+d4]		;;		MemR5
	vmulpd	zmm3, zmm10, zmm24		;;		A5 = R5 * MemR5					; 7-10		n 12
	vmulpd	zmm2, zmm7, zmm24		;;		B5 = I5 * MemR5					; 7-10		n 13

	vmovapd zmm24, [srcreg+rbp+d2+d1+64]	;;		MemI4
	zfnmaddpd zmm21, zmm12, zmm24, zmm21	;;		A4 - I4 * MemI4 (R4 & R6)			; 8-11		n 19
	zfmaddpd zmm20, zmm9, zmm24, zmm20	;;		B4 + R4 * MemI4 (I4 & I6)			; 8-11		n 20
	vmovsd	Q [srcreg-16], xmm23		;;				Save product of FFT sums	; 8

	vmovapd	zmm24, [srcreg+rbp+d4+d1]	;;		MemR6
	vmulpd	zmm15, zmm6, zmm24		;;		A6 = R6 * MemR6					; 9-12		n 14
	vmulpd	zmm13, zmm8, zmm24		;;		B6 = I6 * MemR6					; 9-12		n 15

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

back_to_orig:					;;				R2/I2 becomes R3/R4
	vaddpd	zmm17 {k7}, zmm22, zmm16	;; I1 + I2 (new I1)		R3				; 18-47		n 25
	vsubpd	zmm16 {k7}, zmm22, zmm16	;; I1 - I2 (new I2)		R4				; 19-47		n 26

	vaddpd	zmm8, zmm21, zmm19		;; R4 + R3 (new R3)		R6 + R5 (new R5)		; 19-52		n 25
	vsubpd	zmm21, zmm21, zmm19		;; R4 - R3 (new I4)		R6 - R5 (new negR6)		; 20-52		n 33

	vaddpd	zmm6, zmm18, zmm20		;; I3 + I4 (new I3)		I5 + I6 (new I5)		; 20-53		n 27
	vsubpd	zmm18, zmm18, zmm20		;; I3 - I4 (new R4)		I5 - I6 (new I6)		; 21-53		n 26

	vaddpd	zmm4, zmm3, zmm15		;; R5 + R6 (new R5)		R9 + R10 (new R9)		; 21-48		n 34
	vsubpd	zmm3, zmm3, zmm15		;; R5 - R6 (new R6)		R9 - R10 (new R10)		; 22-48		n 29

	vaddpd	zmm14, zmm2, zmm13		;; I5 + I6 (new I5)		I9 + I10 (new I9)		; 22-49		n 35
	vsubpd	zmm2, zmm2, zmm13		;; I5 - I6 (new I6)		I9 - I10 (new I10)		; 23-49		n 

	vaddpd	zmm12, zmm11, zmm9		;; R8 + R7 (new R7)		R12 + R11 (new R11)		; 23-50		n 34
	vsubpd	zmm11, zmm11, zmm9		;; R8 - R7 (new I8)		R12 - R11 (new I12)		; 24-50		n 29

	vaddpd	zmm5, zmm1, zmm7		;; I7 + I8 (new I7)		I11 + I12 (new I11)		; 24-51		n 35
	vsubpd	zmm1, zmm1, zmm7		;; I7 - I8 (new R8)		I11 - I12 (new R12)		; 25-51		n 32

	vblendmpd zmm28 {k6}, zmm26, ZMM_HALF{1to8} ;; 1, 1, 1, 1, 1, 1, 1	HALF				; 25***		n 
	vblendmpd zmm0 {k6}, zmm8, zmm17	;; R3				R3				; 25		n 26
	vblendmpd zmm15 {k6}, zmm18, zmm16	;; R4				R4				; 26		n 28

	zfmaddpd zmm13, zmm10, zmm28, zmm0	;; R1*1 + R3 (newer R1)		R1*HALF + R3 (newer R1)		; 26-57		n 
	zfmsubpd zmm10, zmm10, zmm28, zmm0	;; R1*1 - R3 (newer R3)		R1*HALF - R3 (newer R3)		; 27-57		n 


	vaddpd	zmm9 {k7}{z}, zmm17, zmm6	;; I1 + I3 (newer I1)		0				; 27-59		n 38
	vsubpd	zmm17 {k7}{z}, zmm17, zmm6	;; I1 - I3 (newer I3)		0				; 28-59		n 39

	zfmaddpd zmm7, zmm23, zmm28, zmm15	;; R2*1 + R4 (newer R2)		R2*HALF + R4 (newer R2)		; 28-61		n 45
	zfmsubpd zmm23, zmm23, zmm28, zmm15	;; R2*1 - R4 (newer R4)		R2*HALF - R4 (newer R4)		; 29-61		n 46

	vblendmpd zmm0 {k6}, zmm3, zmm11	;; R6				I12				; 29		n 
	vblendmpd zmm11 {k6}, zmm11, zmm3	;; I8				R10				; 30		n 

						;; multiply R6/I6 by SQRTHALF - i*SQRTHALF
	vaddpd	zmm15, zmm2, zmm0		;; I6 + R6 (new2 R6/SQRTHALF)	I10 + I12 (newer I10)		; 30-54		n 36
	vsubpd	zmm2, zmm2, zmm0		;; I6 - R6 (new2 I6/SQRTHALF)	I10 - I12 (newer I12)		; 31-54		n 37

	vblendmpd zmm16 {k6}, zmm16, zmm18	;; I2				I6				; 31		n 

						;; multiply R8/I8 by SQRTHALF - i*SQRTHALF
	vaddpd	zmm3, zmm11, zmm1		;; I8 + R8 (new2 R8/SQRTHALF)	R10 + R12 (newer R10)		; 32-55		n 36
	vsubpd	zmm11, zmm11, zmm1		;; I8 - R8 (new2 I8/SQRTHALF)	R10 - R12 (newer R12)		; 32-55		n 37

						;;				mul R6/I6 by w^2 = .707 - .707i
	vaddpd	zmm0, zmm16, zmm21		;; I2 + I4 (newer I2)		I6 + negR6 (newer I6/SQRTHALF) 	; 33-60		n 44
	vsubpd	zmm16, zmm16, zmm21		;; I2 - I4 (newer I4)		I6 - negR6 (newer R6/SQRTHALF) 	; 33-60		n 44

	vsubpd	zmm1, zmm12, zmm4		;; R7 - R5 (newer I7)		R11 - R9 (newer negR11)		; 34-58		n 38
	vaddpd	zmm12, zmm12, zmm4		;; R7 + R5 (newer R5)		R11 + R9 (newer R9)		; 34-37		n 38

	vaddpd	zmm4, zmm14, zmm5		;; I5 + I7 (newer I5)		I9 + I11 (newer I9)		; 35-56		n 42
	vsubpd	zmm14, zmm14, zmm5		;; I5 - I7 (newer R7)		I9 - I11 (newer I11)		; 35-56		n 39

						;;				mul R10/I10 by w^1 = .924 - .383i
	zfmaddpd zmm5, zmm3, zmm26, zmm15	;; R6 + R8*1 (newer R6/SQRTHALF)    I10 + R10*.924/.383 (R10/.383) ; 36-71	n 44
	zfmsubpd zmm15, zmm15, zmm26, zmm3	;; R6*1 - R8 (newer negI8/SQRTHALF) I10*.924/.383 - R10 (I10/.383) ; 36-71	n 

						;;				mul R12/I12 by w^3 = .383 - .924i
	zfmaddpd zmm3, zmm2, zmm26, zmm11	;; I6*1 + I8 (newer I6/SQRTHALF)  I12*.924/.383 + R12 (R12/.383); 37-63		n 47
	zfnmaddpd zmm11, zmm11, zmm26, zmm2	;; I6 - I8*1 (newer R8/SQRTHALF)  I12 - R12*.924/.383 (I12/.383); 37-63		n 44

						;;				R5/I5 becomes new R5/R7
	vblendmpd zmm8 {k6}, zmm12, zmm8	;; R5				R5				; 38		n 40
	vsubpd	zmm1 {k6}, zmm9, zmm1		;; I7				R11 = 0 - negR11		; 38-55		n 43

	vblendmpd zmm6 {k6}, zmm14, zmm6	;; R7				R7				; 39		n 41
	vblendmpd zmm14 {k6}, zmm17, zmm14	;; I3				I11				; 39		n 43

;; four aparts

	vaddpd	zmm2, zmm13, zmm8		;; R1 + R5 (final R1)		R1 + R5 (final R1)		; 40-66		n 
	vsubpd	zmm13, zmm13, zmm8		;; R1 - R5 (final R5)		R1 - R5 (final R5)		; 40-66		n 

	vaddpd	zmm8, zmm10, zmm6		;; R3 + R7 (final R3)		R3 + R7 (final R3)		; 41-64		n 
	vsubpd	zmm10, zmm10, zmm6		;; R3 - R7 (final R7)		R3 - R7 (final R7)		; 41-64		n 

						;;				R9/I9 becomes newer R9/R13
	vaddpd	zmm12 {k7}, zmm9, zmm4		;; I1 + I5 (final I1)		R9				; 42-67		n 
	vsubpd	zmm4 {k7}, zmm9, zmm4		;; I1 - I5 (final I5)		R13				; 42-67		n 

						;;				R11/I11 becomes newer R11/R15
						;;				mul R11/I11 by SQRTHALF - i*SQRTHALF
	vaddpd	zmm6, zmm14, zmm1		;; I3 + I7 (final I3)		I11 + R11 (final R11/SQRTHALF)	; 43-65		n 
	vsubpd	zmm14, zmm14, zmm1		;; I3 - I7 (final I7)		I11 - R11 (final I11/SQRTHALF)	; 43-65		n 

						;;				R6/I6 becomes new R6/R8
	vblendmpd zmm9 {k6}, zmm5, zmm16	;; R6/SQRTHALF			R6/SQRTHALF			; 44		n 
	vblendmpd zmm1 {k6}, zmm11, zmm0	;; R8/SQRTHALF			R8/SQRTHALF			; 44		n 

	zfmaddpd zmm17, zmm9, zmm31, zmm7	;; R2 + R6*SQRTHALF (final R2)	R2 + R6 * SQRTHALF (final R2)	; 45-68		n 
	zfnmaddpd zmm9, zmm9, zmm31, zmm7	;; R2 - R6*SQRTHALF (final R6)	R2 - R6 * SQRTHALF (final R6)	; 45-68		n 

	zfmaddpd zmm7, zmm1, zmm31, zmm23	;; R4 + R8*SQRTHALF (final R4)	R4 + R8 * SQRTHALF (final R4)	; 46-70		n 
	zfnmaddpd zmm1, zmm1, zmm31, zmm23	;; R4 - R8*SQRTHALF (final R8)	R4 - R8 * SQRTHALF (final R8)	; 46-70		n 

						;;				R10/I10 becomes newer R10/R14
	vmovapd	zmm18, zmm15			;; negI8			R14/.383
;; bug - hidden vmovapd uops
	zfmaddpd zmm5 {k7}, zmm3, zmm31, zmm0	;; I2 + I6*SQRTHALF (final I2)	R10/.383			; 47-71		n 
	zfnmaddpd zmm15 {k7}, zmm3, zmm31, zmm0	;; I2 - I6*SQRTHALF (final I6)	R14/.383			; 47-71		n 

						;;				R12/I12 becomes newer R12/R16
;; bug - hidden vmovapd uops
	zfmaddpd zmm11 {k7}, zmm18, zmm31, zmm16;; I4 + negI8*SQRTHALF (final I8)  R16/.383			; 48-69		n 
	zfnmaddpd zmm3 {k7}, zmm18, zmm31, zmm16;; I4 - negI8*SQRTHALF (final I4)  R12/.383			; 48-69		n 

	;; shuffle inputs are:
	;; R1 = R8_0 R7_0 R4_0 R3_0 R6_0 R5_0 R2_0 R1_0	goal new R1 is  R1_7 R1_6 R1_5 R1_4 R1_3 R1_2 R1_1 R1_0
	;; R2 = R8_1 R7_1 R4_1 R3_1 R6_1 R5_1 R2_1 R1_1	goal new R2 is  R2_7 R2_6 R2_5 R2_4 R2_3 R2_2 R2_1 R2_0, etc.
	;; I1 = I8_0 I7_0 I4_0 I3_0 I6_0 I5_0 I2_0 I1_0	goal new I1 is  I1_7 I1_6 I1_5 I1_4 I1_3 I1_2 I1_1 I1_0, etc.

	vshuff64x2 zmm0, zmm2, zmm8, 01000100b	;; R6_2 R5_2 R2_2 R1_2 R6_0 R5_0 R2_0 R1_0 (R13L)		; 126-128	n 139
	vshuff64x2 zmm2, zmm2, zmm8, 11101110b	;; R8_2 R7_2 R4_2 R3_2 R8_0 R7_0 R4_0 R3_0 (R13H)		; 125-127	n 131

	vshuff64x2 zmm8, zmm13, zmm10, 01000100b;; R6_6 R5_6 R2_6 R1_6 R6_4 R5_4 R2_4 R1_4 (R57L)		; 124-126	n 139
	vshuff64x2 zmm13, zmm13, zmm10, 11101110b;; R8_6 R7_6 R4_6 R3_6 R8_4 R7_4 R4_4 R3_4 (R57H)		; 123-125	n 131

	vshuff64x2 zmm10, zmm12, zmm6, 01000100b;; I6_2 I5_2 I2_2 I1_2 I6_0 I5_0 I2_0 I1_0 (I13L)		; 120-122	n 137
	vshuff64x2 zmm12, zmm12, zmm6, 11101110b;; I8_2 I7_2 I4_2 I3_2 I8_0 I7_0 I4_0 I3_0 (I13H)		; 119-121	n 129

	vshuff64x2 zmm6, zmm4, zmm14, 01000100b	;; I6_6 I5_6 I2_6 I1_6 I6_4 I5_4 I2_4 I1_4 (I57L)		; 122-124	n 137
	vshuff64x2 zmm4, zmm4, zmm14, 11101110b	;; I8_6 I7_6 I4_6 I3_6 I8_4 I7_4 I4_4 I3_4 (I57H)		; 121-123	n 129

	vshuff64x2 zmm14, zmm17, zmm7, 01000100b;; R6_3 R5_3 R2_3 R1_3 R6_1 R5_1 R2_1 R1_1 (R24L)		; 134-136	n 143
	vshuff64x2 zmm17, zmm17, zmm7, 11101110b;; R8_3 R7_3 R4_3 R3_3 R8_1 R7_1 R4_1 R3_1 (R24H)		; 132-134	n 135	r132

	vshuff64x2 zmm7, zmm9, zmm1, 01000100b	;; R6_7 R5_7 R2_7 R1_7 R6_5 R5_5 R2_5 R1_5 (R68L)		; 133-135	n 143
	vshuff64x2 zmm9, zmm9, zmm1, 11101110b	;; R8_7 R7_7 R4_7 R3_7 R8_5 R7_5 R4_5 R3_5 (R68H)		; 131-133	n 135

	vshuff64x2 zmm1, zmm5, zmm3, 01000100b	;; I6_3 I5_3 I2_3 I1_3 I6_1 I5_1 I2_1 I1_1 (I24L)		; 128-130	n 141
	vshuff64x2 zmm5, zmm5, zmm3, 11101110b	;; I8_3 I7_3 I4_3 I3_3 I8_1 I7_1 I4_1 I3_1 (I24H)		; 127-129	n 133

	vshuff64x2 zmm3, zmm15, zmm11, 01000100b;; I6_7 I5_7 I2_7 I1_7 I6_5 I5_5 I2_5 I1_5 (I68L)		; 130-132	n 141
	vshuff64x2 zmm15, zmm15, zmm11, 11101110b;; I8_7 I7_7 I4_7 I3_7 I8_5 I7_5 I4_5 I3_5 (I68H)		; 129-131	n 133

	;; shuffle the four aparts
	vshuff64x2 zmm11, zmm2, zmm13, 10001000b;; R4_6 R3_6 R4_4 R3_4 R4_2 R3_2 R4_0 R3_0 (R1357LH)		; 141-143	n 148	r136
	vshuff64x2 zmm2, zmm2, zmm13, 11011101b	;; R8_6 R7_6 R8_4 R7_4 R8_2 R7_2 R8_0 R7_0 (R1357HH)		; 137-139	n 144	r136

	vshuff64x2 zmm13, zmm12, zmm4, 10001000b;; I4_6 I3_6 I4_4 I3_4 I4_2 I3_2 I4_0 I3_0 (I1357LH)		; 136-138	n 147	r134
	vshuff64x2 zmm12, zmm12, zmm4, 11011101b;; I8_6 I7_6 I8_4 I7_4 I8_2 I7_2 I8_0 I7_0 (I1357HH)		; 135-137	n 143	r134

	vshuff64x2 zmm4, zmm17, zmm9, 10001000b	;; R4_7 R3_7 R4_5 R3_5 R4_3 R3_3 R4_1 R3_1 (R2468LH)		; 142-144	n 148	r140
	vshuff64x2 zmm17, zmm17, zmm9, 11011101b;; R8_7 R7_7 R8_5 R7_5 R8_3 R7_3 R8_1 R7_1 (R2468HH)		; 140-142	n 144	r140

	vshuff64x2 zmm9, zmm5, zmm15, 10001000b	;; I4_7 I3_7 I4_5 I3_5 I4_3 I3_3 I4_1 I3_1 (I2468LH)		; 139-141	n 147	r138
	vshuff64x2 zmm5, zmm5, zmm15, 11011101b ;; I8_7 I7_7 I8_5 I7_5 I8_3 I7_3 I8_1 I7_1 (I2468HH)		; 138-140	n 143	r138

	vshuff64x2 zmm15, zmm0, zmm8, 10001000b	;; R2_6 R1_6 R2_4 R1_4 R2_2 R1_2 R2_0 R1_0 (R1357LL)		; 162-164	n 166
	vshuff64x2 zmm0, zmm0, zmm8, 11011101b	;; R6_6 R5_6 R6_4 R5_4 R6_2 R5_2 R6_0 R5_0 (R1357HL)		; 154-156	n 158

	vshuff64x2 zmm8, zmm14, zmm7, 10001000b	;; R2_7 R1_7 R2_5 R1_5 R2_3 R1_3 R2_1 R1_1 (R2468LL)		; 163-165	n 166
	vshuff64x2 zmm14, zmm14, zmm7, 11011101b;; R6_7 R5_7 R6_5 R5_5 R6_3 R5_3 R6_1 R5_1 (R2468HL)		; 153-155	n 158

	vshuff64x2 zmm7, zmm10, zmm6, 10001000b	;; I2_6 I1_6 I2_4 I1_4 I2_2 I1_2 I2_0 I1_0 (I1357LL)		; 155-157	n 161
	vshuff64x2 zmm10, zmm10, zmm6, 11011101b;; I6_6 I5_6 I6_4 I5_4 I6_2 I5_2 I6_0 I5_0 (I1357HL)		; 151-153	n 157

	vshuff64x2 zmm6, zmm1, zmm3, 10001000b	;; I2_7 I1_7 I2_5 I1_5 I2_3 I1_3 I2_1 I1_1 (I2468LL)		; 156-158	n 161
	vshuff64x2 zmm1, zmm1, zmm3, 11011101b	;; I6_7 I5_7 I6_5 I5_5 I6_3 I5_3 I6_1 I5_1 (I2468HL)		; 152-154	n 157

	;; shufpd the one aparts
	vshufpd	zmm3, zmm11, zmm4, 00000000b	;; R3_7 R3_6 R3_5 R3_4 R3_3 R3_2 R3_1 R3_0 (next R3)	; 148		n 149
	vshufpd	zmm16, zmm13, zmm9, 00000000b	;; I3_7 I3_6 I3_5 I3_4 I3_3 I3_2 I3_1 I3_0 (next I3)	; 147		n 149

	vshufpd	zmm18, zmm15, zmm8, 11111111b	;; R2_7 R2_6 R2_5 R2_4 R2_3 R2_2 R2_1 R2_0 (next R2)	; 166		n 168
	vshufpd	zmm19, zmm7, zmm6, 11111111b	;; I2_7 I2_6 I2_5 I2_4 I2_3 I2_2 I2_1 I2_0 (next I2)	; 164		n 168

 	vshufpd	zmm11, zmm11, zmm4, 11111111b	;; R4_7 R4_6 R4_5 R4_4 R4_3 R4_2 R4_1 R4_0 (next R4)	; 150		n 151
	vshufpd	zmm13, zmm13, zmm9, 11111111b	;; I4_7 I4_6 I4_5 I4_4 I4_3 I4_2 I4_1 I4_0 (next I4)	; 149		n 151

	vshufpd	zmm4, zmm2, zmm17, 00000000b	;; R7_7 R7_6 R7_5 R7_4 R7_3 R7_2 R7_1 R7_0 (next R7)	; 146		n 147
	vshufpd	zmm9, zmm12, zmm5, 00000000b	;; I7_7 I7_6 I7_5 I7_4 I7_3 I7_2 I7_1 I7_0 (next I7)	; 145		n 147

	vshufpd	zmm2, zmm2, zmm17, 11111111b	;; R8_7 R8_6 R8_5 R8_4 R8_3 R8_2 R8_1 R8_0 (next R8)	; 144		n 145
	vshufpd	zmm12, zmm12, zmm5, 11111111b	;; I8_7 I8_6 I8_5 I8_4 I8_3 I8_2 I8_1 I8_0 (next I8)	; 143		n 145

	vshufpd	zmm17, zmm0, zmm14, 11111111b	;; R6_7 R6_6 R6_5 R6_4 R6_3 R6_2 R6_1 R6_0 (next R6)	; 160		n 161
	vshufpd	zmm5, zmm10, zmm1, 11111111b	;; I6_7 I6_6 I6_5 I6_4 I6_3 I6_2 I6_1 I6_0 (next I6)	; 159		n 161

	vshufpd	zmm0, zmm0, zmm14, 00000000b	;; R5_7 R5_6 R5_5 R5_4 R5_3 R5_2 R5_1 R5_0 (next R5)	; 158		n 159
	vshufpd	zmm10, zmm10, zmm1, 00000000b	;; I5_7 I5_6 I5_5 I5_4 I5_3 I5_2 I5_1 I5_0 (next I5)	; 157		n 159

	vshufpd	zmm15, zmm15, zmm8, 00000000b	;; R1_7 R1_6 R1_5 R1_4 R1_3 R1_2 R1_1 R1_0 (next R1)	; 167		n 169
	vshufpd	zmm7, zmm7, zmm6, 00000000b	;; I1_7 I1_6 I1_5 I1_4 I1_3 I1_2 I1_1 I1_0 (next I1)	; 161		n 163

	vmovapd	zmm28, [screg2+0*128+64]	;; cosine/sine for R3/I3 (w^2 = complex w^1)
	zfmaddpd zmm1, zmm3, zmm28, zmm16	;; A3 = R3 * cosine/sine + I3				; 1-4		n 5
	zfmsubpd zmm16, zmm16, zmm28, zmm3	;; B3 = I3 * cosine/sine - R3				; 1-4		n 5

	vmovapd	zmm28, [screg1+0*128+64]	;; cosine/sine for R2/I2 (w^1)
	zfmaddpd zmm6, zmm18, zmm28, zmm19	;; A2 = R2 * cosine/sine + I2				; 2-5		n 7
	zfmsubpd zmm19, zmm19, zmm28, zmm18	;; B2 = I2 * cosine/sine - R2				; 2-5		n 7

	vmovapd	zmm28, [screg1+1*128+64]	;; cosine/sine for R4/I4 (w^3)
	zfmaddpd zmm8, zmm11, zmm28, zmm13	;; A4 = R4 * cosine/sine + I4				; 3-6		n 9
	zfmsubpd zmm13, zmm13, zmm28, zmm11	;; B4 = I4 * cosine/sine - R4				; 3-6		n 9

	vmovapd	zmm28, [screg2+2*128+64]	;; cosine/sine for R7/I7 (w^6 = complex w^3)
	zfmaddpd zmm14, zmm4, zmm28, zmm9	;; A7 = R7 * cosine/sine + I7				; 4-7		n 11
	zfmsubpd zmm9, zmm9, zmm28, zmm4	;; B7 = I7 * cosine/sine - R7				; 4-7		n 12

	vmovapd	zmm28, [screg2+0*128]		;; sine for R3/I3
	vmulpd	zmm1, zmm1, zmm28		;; R3 = A3 * sine					; 5-8		n 11
	vmulpd	zmm16, zmm16, zmm28		;; I3 = B3 * sine					; 5-8		n 12

	vmovapd	zmm28, [screg1+3*128+64]	;; cosine/sine for R8/I8 (w^7)
	zfmaddpd zmm3, zmm2, zmm28, zmm12	;; A8 = R8 * cosine/sine + I8				; 6-9		n 13
	zfmsubpd zmm12, zmm12, zmm28, zmm2	;; B8 = I8 * cosine/sine - R8				; 6-9		n 14

	vmovapd	zmm28, [screg1+0*128]		;; sine for R2/I2
	vmulpd	zmm6, zmm6, zmm28		;; R2 = A2 * sine					; 7-10		n 13
	vmulpd	zmm19, zmm19, zmm28		;; I2 = B2 * sine					; 7-10		n 14

	vmovapd	zmm28, [screg1+2*128+64]	;; cosine/sine for R6/I6 (w^5)
	zfmaddpd zmm18, zmm17, zmm28, zmm5	;; A6 = R6 * cosine/sine + I6				; 8-11		n 15
	zfmsubpd zmm5, zmm5, zmm28, zmm17	;; B6 = I6 * cosine/sine - R6				; 8-11		n 16

	vmovapd	zmm28, [screg1+1*128]		;; sine for R4/I4
	vmulpd	zmm8, zmm8, zmm28		;; R4 = A4 * sine					; 9-12		n 15
	vmulpd	zmm13, zmm13, zmm28		;; I4 = B4 * sine					; 9-12		n 16

	vmovapd	zmm28, [screg2+1*128+64]	;; cosine/sine for R5/I5 (w^4 = complex w^2)
	zfmaddpd zmm11, zmm0, zmm28, zmm10	;; A5 = R5 * cosine/sine + I5				; 10-13		n 17
	zfmsubpd zmm10, zmm10, zmm28, zmm0	;; B5 = I5 * cosine/sine - R5				; 10-13		n 18

	vbroadcastf64x4 zmm27, ZMM_383_707_383_1 ;; .383 SQRTHALF .383 1 .383 SQRTHALF .383 1
	zfmaddpd zmm4, zmm7, zmm27, zmm15	;; R1+(.383,SQRTHALF,.383,1,.383,SQRTHALF,.383,1)*R9		; 88-89		n 
	zfnmaddpd zmm7, zmm7, zmm27, zmm15	;; R1-(.383,SQRTHALF,.383,1,.383,SQRTHALF,.383,1)*R9		; 88-89		n 

	vmovapd	zmm28, [screg2+2*128]		;; sine for R7/I7
	zfnmaddpd zmm20, zmm14, zmm28, zmm1	;; R3-(R7 = A7 * sine)					; 11-14		n 19
	zfmaddpd zmm14, zmm14, zmm28, zmm1	;; R3+(R7 = A7 * sine)					; 11-14		n 22

	zfmaddpd zmm2, zmm9, zmm28, zmm16	;; I3+(I7 = B7 * sine)					; 12-15		n 19
	zfnmaddpd zmm9, zmm9, zmm28, zmm16	;; I3-(I7 = B7 * sine)					; 12-15		n 23

	vmovapd	zmm28, [screg1+3*128]		;; sine for R8/I8
	zfmaddpd zmm17, zmm3, zmm28, zmm6	;; R2+(R8 = A8 * sine)					; 13-16		n 20
	zfnmaddpd zmm3, zmm3, zmm28, zmm6	;; R2-(R8 = A8 * sine)					; 13-16		n 24

	zfnmaddpd zmm0, zmm12, zmm28, zmm19	;; I2-(I8 = B8 * sine)					; 14-17		n 21
	zfmaddpd zmm12, zmm12, zmm28, zmm19	;; I2+(I8 = B8 * sine)					; 14-17		n 25

	vmovapd	zmm28, [screg1+2*128]		;; sine for R6/I6
	zfmaddpd zmm15, zmm18, zmm28, zmm8	;; R4+(R6 = A6 * sine)					; 15-18		n 20
	zfnmaddpd zmm18, zmm18, zmm28, zmm8	;; R4-(R6 = A6 * sine)					; 15-18		n 25

	zfnmaddpd zmm1, zmm5, zmm28, zmm13	;; I4-(I6 = B6 * sine)					; 16-19		n 21
	zfmaddpd zmm5, zmm5, zmm28, zmm13	;; I4+(I6 = B6 * sine)					; 16-19		n 24

	vmovapd	zmm28, [screg2+1*128]		;; sine for R5/I5
	zfmaddpd zmm16, zmm11, zmm28, zmm4	;; r1++ = (r1+r9) + r5*sine				; 17-20		n 22
	zfnmaddpd zmm11, zmm11, zmm28, zmm4	;; r1+- = (r1+r9) - r5*sine				; 17-20		n 23

	zfmaddpd zmm6, zmm10, zmm28, zmm7	;; r1-+ = (r1-r9) + i5*sine				; 18-21		n 29
	zfnmaddpd zmm10, zmm10, zmm28, zmm7	;; r1-- = (r1-r9) - i5*sine				; 18-21		n 31
	bump	screg1, scinc1

	vaddpd	zmm8, zmm20, zmm2		;; r3-+ = (r3-r7) + (i3+i7)				; 19-22		n 29
	vsubpd	zmm20, zmm20, zmm2		;; r3-- = (r3-r7) - (i3+i7)				; 19-22		n 31
	bump	screg2, scinc2

	vaddpd	zmm13, zmm17, zmm15		;; r2++ = (r2+r8) + (r4+r6)				; 20-23		n 26
	vsubpd	zmm17, zmm17, zmm15		;; r2+- = (r2+r8) - (r4+r6)				; 20-23		n 28

	vsubpd	zmm7, zmm0, zmm1		;; i2-- = (i2-i8) - (i4-i6)				; 21-24		n 27
	vaddpd	zmm0, zmm0, zmm1		;; i2-+ = (i2-i8) + (i4-i6)				; 21-24		n 28

	vaddpd	zmm2, zmm16, zmm14		;; r1+++ = (r1++) + (r3+r7)				; 22-25		n 26
	vsubpd	zmm16, zmm16, zmm14		;; r1++- = (r1++) - (r3+r7)				; 22-25		n 27

	vaddpd	zmm15, zmm11, zmm9		;; r1+-+ = (r1+-) + (i3-i7)				; 23-26		n 33
	vsubpd	zmm11, zmm11, zmm9		;; r1+-- = (r1+-) - (i3-i7)				; 23-26		n 34

	vaddpd	zmm1, zmm3, zmm5		;; r2-+ = (r2-r8) + (i4+i6)				; 24-27		n 30
	vsubpd	zmm3, zmm3, zmm5		;; r2-- = (r2-r8) - (i4+i6)				; 24-27		n 32

	vaddpd	zmm14, zmm12, zmm18		;; i2++ = (i2+i8) + (r4-r6)				; 25-28		n 30
	vsubpd	zmm12, zmm12, zmm18		;; i2+- = (i2+i8) - (r4-r6)				; 25-28		n 32

	vaddpd	zmm9, zmm2, zmm13		;; R1 = (r1+++) + (r2++)				; 26-29
	vsubpd	zmm2, zmm2, zmm13		;; R9 = (r1+++) - (r2++)				; 26-29

	vaddpd	zmm5, zmm16, zmm7		;; R5  = (r1++-) + (i2--)				; 27-30
	vsubpd	zmm16, zmm16, zmm7		;; R13 = (r1++-) - (i2--)				; 27-30

	vaddpd	zmm13, zmm17, zmm0		;; r2+-+ = (r2+-) + (i2-+)				; 28-31		n 33
	vsubpd	zmm17, zmm17, zmm0		;; r2+-- = (r2+-) - (i2-+)				; 28-31		n 34

	zfmaddpd zmm7, zmm8, zmm31, zmm6	;; r2_10o = (r1-+) + .707(r3-+)				; 29-32		n 35
	zfnmaddpd zmm8, zmm8, zmm31, zmm6	;; r6_14o = (r1-+) - .707(r3-+)				; 29-32		n 36

	zfmaddpd zmm0, zmm1, zmm30, zmm14	;; r2_10e/.383 = .924/.383(r2-+) + (i2++)		; 30-33		n 35
	zfmsubpd zmm14, zmm14, zmm30, zmm1	;; r6_14e/.383 = .924/.383(i2++) - (r2-+)		; 30-33		n 36
	zstore	[srcreg+r8], zmm9		;; Save R1						; 30

	zfnmaddpd zmm6, zmm20, zmm31, zmm10	;; r4_12o = (r1--) - .707(r3--)				; 31-34		n 37
	zfmaddpd zmm20, zmm20, zmm31, zmm10	;; r8_16o = (r1--) + .707(r3--)				; 31-34		n 38
	zstore	[srcreg+r8+64], zmm2		;; Save R9						; 30+1

	zfmaddpd zmm1, zmm12, zmm30, zmm3	;; r4_12e/.383 = .924/.383(i2+-) + (r2--)		; 32-35		n 37
	zfnmaddpd zmm3, zmm3, zmm30, zmm12	;; r8_16e/.383 = (i2+-) - .924/.383(r2--)		; 32-35		n 38
	zstore	[srcreg+r8+d4], zmm5		;; Save R5						; 31+1

	zfmaddpd zmm10, zmm13, zmm31, zmm15	;; R3  = (r1+-+) + .707(r2+-+)				; 33-36
	zfnmaddpd zmm13, zmm13, zmm31, zmm15	;; R11 = (r1+-+) - .707(r2+-+)				; 33-36
	zstore	[srcreg+r8+d4+64], zmm16	;; Save R13						; 31+2

	zfnmaddpd zmm12, zmm17, zmm31, zmm11	;; R7  = (r1+--) - .707(r2+--)				; 34-37
	zfmaddpd zmm17, zmm17, zmm31, zmm11	;; R15 = (r1+--) + .707(r2+--)				; 34-37

	zfmaddpd zmm15, zmm0, zmm29, zmm7	;; R2  = r2_10o + .383*r2_10e				; 35-38
	zfnmaddpd zmm0, zmm0, zmm29, zmm7	;; R10 = r2_10o - .383*r2_10e				; 35-38

	zfmaddpd zmm11, zmm14, zmm29, zmm8	;; R6  = r6_14o + .383*r6_14e				; 36-39
	zfnmaddpd zmm14, zmm14, zmm29, zmm8	;; R14 = r6_14o - .383*r6_14e				; 36-39

	zfmaddpd zmm7, zmm1, zmm29, zmm6	;; R4  = r4_12o + .383*r4_12e				; 37-40
	zfnmaddpd zmm1, zmm1, zmm29, zmm6	;; R12 = r4_12o - .383*r4_12e				; 37-40
	zstore	[srcreg+r8+d2], zmm10		;; Save R3						; 37

	zfmaddpd zmm8, zmm3, zmm29, zmm20	;; R8  = r8_16o + .383*r8_16e				; 38-41
	zfnmaddpd zmm3, zmm3, zmm29, zmm20	;; R16 = r8_16o - .383*r8_16e				; 38-41
	zstore	[srcreg+r8+d2+64], zmm13	;; Save R11						; 37+1

	zstore	[srcreg+r8+d4+d2], zmm12	;; Save R7						; 38+1
	zstore	[srcreg+r8+d4+d2+64], zmm17	;; Save R15						; 38+2
	zstore	[srcreg+r8+d1], zmm15		;; Save R2						; 39+2
	zstore	[srcreg+r8+d1+64], zmm0		;; Save R10						; 39+3
	zstore	[srcreg+r8+d4+d1], zmm11	;; Save R6						; 40+3
	zstore	[srcreg+r8+d4+d1+64], zmm14	;; Save R14						; 40+4
	zstore	[srcreg+r8+d2+d1], zmm7		;; Save R4						; 41+4
	zstore	[srcreg+r8+d2+d1+64], zmm1	;; Save R12						; 41+5
	zstore	[srcreg+r8+d4+d2+d1], zmm8	;; Save R8						; 42+5
	zstore	[srcreg+r8+d4+d2+d1+64], zmm3	;; Save R16						; 42+6
	bump	srcreg, srcinc
	ENDM


zr64f_hundredtwentyeight_real_with_mulf_preload MACRO
	;; preload would be silly for a macro that is executed only once
	ENDM
zr64f_hundredtwentyeight_real_with_mulf MACRO srcreg,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	LOCAL	orig, back_to_orig, submul, fma, muladd, muladdhard, mulsub, mulsubhard

	;; For zero pad
	zmult7	srcreg+rbx, srcreg+rbp

	mov	eax, 11111110b			;; We're pretty sure callers of this macro call zloop_init next
	kmovw	k7, eax				;; Set k7 to 11111110b						; ?		n 
	knotw	k6, k7				;; Set k6 to 00000001b						; ?		n 

	mov	al, mul4_opcode			;; Load the mul4_opcode
	cmp	al, 0				;; See if we need to do more than the original type-4 FFT multiply
	je	orig
	mov	r9, SRC2ARG			;; Distance to s2 arg from gwaddmul4 or gwsubmul4 or s3 arg in gwmuladd4 or gwmulsub4
	cmp	al, 2				;; Case off opcode
	jg	fma				;; 3,4=muladd,mulsub
	;jl	addmul				;; 1=addmul, fall through
	je	submul				;; 2=submul

	vmovapd	zmm2, [srcreg+d1][rbx]		;;		R2
	vmovapd	zmm24, [srcreg+rbp+d1]		;;		MemR2
	vaddpd	zmm24, zmm24, [srcreg+r9+d1]	;;		MemR2
	vmulpd	zmm17, zmm2, zmm24		;;		A2 = R2 * MemR2					; 1-4		n 5
	vmovapd	zmm3, [srcreg+d1+64][rbx]	;;		I2
	vmulpd	zmm16, zmm3, zmm24		;;		B2 = I2 * MemR2					; 1-4		n 5

	vmovapd	zmm13, [srcreg+d2][rbx]		;;		R3
	vmovapd	zmm24, [srcreg+rbp+d2]		;;		MemR3
	vaddpd	zmm24, zmm24, [srcreg+r9+d2]	;;		MemR3
	vmulpd	zmm19, zmm13, zmm24		;;		A3 = R3 * MemR3					; 2-5		n 6
	vmovapd	zmm15, [srcreg+d2+64][rbx]	;;		I3
	vmulpd	zmm18, zmm15, zmm24		;;		B3 = I3 * MemR3					; 2-5		n 6

	vmovapd	zmm9, [srcreg+d2+d1][rbx]	;;		R4
	vmovapd	zmm24, [srcreg+rbp+d2+d1]	;;		MemR4
	vaddpd	zmm24, zmm24, [srcreg+r9+d2+d1];;		MemR4
	vmulpd	zmm21, zmm9, zmm24		;;		A4 = R4 * MemR4					; 3-6		n 8
	vmovapd	zmm12, [srcreg+d2+d1+64][rbx]	;;		I4
	vmulpd	zmm20, zmm12, zmm24		;;		B4 = I4 * MemR4					; 3-6		n 8

	vmovapd	zmm1, [srcreg][rbx]		;; R1				R1a
	vmovapd	zmm24, [srcreg+rbp]		;; MemR1			MemR1a
	vaddpd	zmm24, zmm24, [srcreg+r9]	;; MemR1			MemR1a
	vmulpd	zmm23, zmm1, zmm24		;; A1 = R1 * MemR1		R1a*MemR1a (R1a)		; 4-7		n 10
	vmovapd	zmm11, [srcreg+64][rbx]		;; I1				R1b
	vmulpd	zmm22 {k7}{z}, zmm11, zmm24	;; B1 = I1 * MemR1		0				; 4-7		n 10

	vmovapd zmm24, [srcreg+rbp+d1+64]	;;		MemI2
	vaddpd	zmm24, zmm24, [srcreg+r9+d1+64];;		MemI2
	zfnmaddpd zmm17, zmm3, zmm24, zmm17	;;		A2 - I2 * MemI2 (R2)				; 5-8		n 10
	zfmaddpd zmm16, zmm2, zmm24, zmm16	;;		B2 + R2 * MemI2 (I2)				; 5-8		n 18

	vmovapd zmm24, [srcreg+rbp+d2+64]	;;		MemI3
	vaddpd	zmm24, zmm24, [srcreg+r9+d2+64];;		MemI3
	zfnmaddpd zmm19, zmm15, zmm24, zmm19	;;		A3 - I3 * MemI3 (R3 & R5)			; 6-9		n 19
	zfmaddpd zmm18, zmm13, zmm24, zmm18	;;		B3 + R3 * MemI3 (I3 & I5)			; 6-9		n 20

	vmovapd	zmm10, [srcreg+d4][rbx]		;;		R5
	vmovapd	zmm24, [srcreg+rbp+d4]		;;		MemR5
	vaddpd	zmm24, zmm24, [srcreg+r9+d4]	;;		MemR5
	vmulpd	zmm3, zmm10, zmm24		;;		A5 = R5 * MemR5					; 7-10		n 12
	vmovapd	zmm7, [srcreg+d4+64][rbx]	;;		I5
	vmulpd	zmm2, zmm7, zmm24		;;		B5 = I5 * MemR5					; 7-10		n 13

	vmovapd zmm24, [srcreg+rbp+d2+d1+64]	;;		MemI4
	vaddpd	zmm24, zmm24, [srcreg+r9+d2+d1+64];;		MemI4
	zfnmaddpd zmm21, zmm12, zmm24, zmm21	;;		A4 - I4 * MemI4 (R4 & R6)			; 8-11		n 19
	zfmaddpd zmm20, zmm9, zmm24, zmm20	;;		B4 + R4 * MemI4 (I4 & I6)			; 8-11		n 20
	vmovsd	Q [srcreg-16], xmm23		;;				Save product of FFT sums	; 8

	vmovapd	zmm6, [srcreg+d4+d1][rbx]	;;		R6
	vmovapd	zmm24, [srcreg+rbp+d4+d1]	;;		MemR6
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+d1];;		MemR6
	vmulpd	zmm15, zmm6, zmm24		;;		A6 = R6 * MemR6					; 9-12		n 14
	vmovapd	zmm8, [srcreg+d4+d1+64][rbx]	;;		I6
	vmulpd	zmm13, zmm8, zmm24		;;		B6 = I6 * MemR6					; 9-12		n 15

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	vaddpd	zmm24, zmm24, [srcreg+r9+64]	;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm14, [srcreg+d4+d2][rbx]	;;		R7
	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+d2];;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmovapd	zmm4, [srcreg+d4+d2+64][rbx]	;;		I7
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+64];;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm0, [srcreg+d4+d2+d1][rbx]	;;		R8
	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+d2+d1];;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmovapd	zmm5, [srcreg+d4+d2+d1+64][rbx] ;;		I8
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+d1+64];;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+d2+64];;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	vaddpd	zmm24, zmm24, [srcreg+r9+d4+d2+d1+64];;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

	jmp	back_to_orig

submul:
	vmovapd	zmm2, [srcreg+d1][rbx]		;;		R2
	vmovapd	zmm24, [srcreg+rbp+d1]		;;		MemR2
	vsubpd	zmm24, zmm24, [srcreg+r9+d1]	;;		MemR2
	vmulpd	zmm17, zmm2, zmm24		;;		A2 = R2 * MemR2					; 1-4		n 5
	vmovapd	zmm3, [srcreg+d1+64][rbx]	;;		I2
	vmulpd	zmm16, zmm3, zmm24		;;		B2 = I2 * MemR2					; 1-4		n 5

	vmovapd	zmm13, [srcreg+d2][rbx]		;;		R3
	vmovapd	zmm24, [srcreg+rbp+d2]		;;		MemR3
	vsubpd	zmm24, zmm24, [srcreg+r9+d2]	;;		MemR3
	vmulpd	zmm19, zmm13, zmm24		;;		A3 = R3 * MemR3					; 2-5		n 6
	vmovapd	zmm15, [srcreg+d2+64][rbx]	;;		I3
	vmulpd	zmm18, zmm15, zmm24		;;		B3 = I3 * MemR3					; 2-5		n 6

	vmovapd	zmm9, [srcreg+d2+d1][rbx]	;;		R4
	vmovapd	zmm24, [srcreg+rbp+d2+d1]	;;		MemR4
	vsubpd	zmm24, zmm24, [srcreg+r9+d2+d1];;		MemR4
	vmulpd	zmm21, zmm9, zmm24		;;		A4 = R4 * MemR4					; 3-6		n 8
	vmovapd	zmm12, [srcreg+d2+d1+64][rbx]	;;		I4
	vmulpd	zmm20, zmm12, zmm24		;;		B4 = I4 * MemR4					; 3-6		n 8

	vmovapd	zmm1, [srcreg][rbx]		;; R1				R1a
	vmovapd	zmm24, [srcreg+rbp]		;; MemR1			MemR1a
	vsubpd	zmm24, zmm24, [srcreg+r9]	;; MemR1			MemR1a
	vmulpd	zmm23, zmm1, zmm24		;; A1 = R1 * MemR1		R1a*MemR1a (R1a)		; 4-7		n 10
	vmovapd	zmm11, [srcreg+64][rbx]		;; I1				R1b
	vmulpd	zmm22 {k7}{z}, zmm11, zmm24	;; B1 = I1 * MemR1		0				; 4-7		n 10

	vmovapd zmm24, [srcreg+rbp+d1+64]	;;		MemI2
	vsubpd	zmm24, zmm24, [srcreg+r9+d1+64];;		MemI2
	zfnmaddpd zmm17, zmm3, zmm24, zmm17	;;		A2 - I2 * MemI2 (R2)				; 5-8		n 10
	zfmaddpd zmm16, zmm2, zmm24, zmm16	;;		B2 + R2 * MemI2 (I2)				; 5-8		n 18

	vmovapd zmm24, [srcreg+rbp+d2+64]	;;		MemI3
	vsubpd	zmm24, zmm24, [srcreg+r9+d2+64];;		MemI3
	zfnmaddpd zmm19, zmm15, zmm24, zmm19	;;		A3 - I3 * MemI3 (R3 & R5)			; 6-9		n 19
	zfmaddpd zmm18, zmm13, zmm24, zmm18	;;		B3 + R3 * MemI3 (I3 & I5)			; 6-9		n 20

	vmovapd	zmm10, [srcreg+d4][rbx]		;;		R5
	vmovapd	zmm24, [srcreg+rbp+d4]		;;		MemR5
	vsubpd	zmm24, zmm24, [srcreg+r9+d4]	;;		MemR5
	vmulpd	zmm3, zmm10, zmm24		;;		A5 = R5 * MemR5					; 7-10		n 12
	vmovapd	zmm7, [srcreg+d4+64][rbx]	;;		I5
	vmulpd	zmm2, zmm7, zmm24		;;		B5 = I5 * MemR5					; 7-10		n 13

	vmovapd zmm24, [srcreg+rbp+d2+d1+64]	;;		MemI4
	vsubpd	zmm24, zmm24, [srcreg+r9+d2+d1+64];;		MemI4
	zfnmaddpd zmm21, zmm12, zmm24, zmm21	;;		A4 - I4 * MemI4 (R4 & R6)			; 8-11		n 19
	zfmaddpd zmm20, zmm9, zmm24, zmm20	;;		B4 + R4 * MemI4 (I4 & I6)			; 8-11		n 20
	vmovsd	Q [srcreg-16], xmm23		;;				Save product of FFT sums	; 8

	vmovapd	zmm6, [srcreg+d4+d1][rbx]	;;		R6
	vmovapd	zmm24, [srcreg+rbp+d4+d1]	;;		MemR6
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+d1];;		MemR6
	vmulpd	zmm15, zmm6, zmm24		;;		A6 = R6 * MemR6					; 9-12		n 14
	vmovapd	zmm8, [srcreg+d4+d1+64][rbx]	;;		I6
	vmulpd	zmm13, zmm8, zmm24		;;		B6 = I6 * MemR6					; 9-12		n 15

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	vsubpd	zmm24, zmm24, [srcreg+r9+64]	;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm14, [srcreg+d4+d2][rbx]	;;		R7
	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+d2];;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmovapd	zmm4, [srcreg+d4+d2+64][rbx]	;;		I7
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+64];;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm0, [srcreg+d4+d2+d1][rbx]	;;		R8
	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+d2+d1];;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmovapd	zmm5, [srcreg+d4+d2+d1+64][rbx] ;;		I8
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+d1+64];;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+d2+64];;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	vsubpd	zmm24, zmm24, [srcreg+r9+d4+d2+d1+64];;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

	jmp	back_to_orig

fma:
	vmovapd	zmm2, [srcreg+d1][rbx]		;;		R2
	vmovapd	zmm24, [srcreg+rbp+d1]		;;		MemR2
	vmulpd	zmm17, zmm2, zmm24		;;		A2 = R2 * MemR2					; 1-4		n 5
	vmovapd	zmm3, [srcreg+d1+64][rbx]	;;		I2
	vmulpd	zmm16, zmm3, zmm24		;;		B2 = I2 * MemR2					; 1-4		n 5

	vmovapd	zmm13, [srcreg+d2][rbx]		;;		R3
	vmovapd	zmm24, [srcreg+rbp+d2]		;;		MemR3
	vmulpd	zmm19, zmm13, zmm24		;;		A3 = R3 * MemR3					; 2-5		n 6
	vmovapd	zmm15, [srcreg+d2+64][rbx]	;;		I3
	vmulpd	zmm18, zmm15, zmm24		;;		B3 = I3 * MemR3					; 2-5		n 6

	vmovapd	zmm9, [srcreg+d2+d1][rbx]	;;		R4
	vmovapd	zmm24, [srcreg+rbp+d2+d1]	;;		MemR4
	vmulpd	zmm21, zmm9, zmm24		;;		A4 = R4 * MemR4					; 3-6		n 8
	vmovapd	zmm12, [srcreg+d2+d1+64][rbx]	;;		I4
	vmulpd	zmm20, zmm12, zmm24		;;		B4 = I4 * MemR4					; 3-6		n 8

	vmovapd	zmm1, [srcreg][rbx]		;; R1				R1a
	vmovapd	zmm24, [srcreg+rbp]		;; MemR1			MemR1a
	vmulpd	zmm23, zmm1, zmm24		;; A1 = R1 * MemR1		R1a*MemR1a (R1a)		; 4-7		n 10
	vmovapd	zmm11, [srcreg+64][rbx]		;; I1				R1b
	vmulpd	zmm22 {k7}{z}, zmm11, zmm24	;; B1 = I1 * MemR1		0				; 4-7		n 10

	vmovapd zmm24, [srcreg+rbp+d1+64]	;;		MemI2
	zfnmaddpd zmm17, zmm3, zmm24, zmm17	;;		A2 - I2 * MemI2 (R2)				; 5-8		n 10
	zfmaddpd zmm16, zmm2, zmm24, zmm16	;;		B2 + R2 * MemI2 (I2)				; 5-8		n 18

	vmovapd zmm24, [srcreg+rbp+d2+64]	;;		MemI3
	zfnmaddpd zmm19, zmm15, zmm24, zmm19	;;		A3 - I3 * MemI3 (R3 & R5)			; 6-9		n 19
	zfmaddpd zmm18, zmm13, zmm24, zmm18	;;		B3 + R3 * MemI3 (I3 & I5)			; 6-9		n 20

	vmovapd	zmm10, [srcreg+d4][rbx]		;;		R5
	vmovapd	zmm24, [srcreg+rbp+d4]		;;		MemR5
	vmulpd	zmm3, zmm10, zmm24		;;		A5 = R5 * MemR5					; 7-10		n 12
	vmovapd	zmm7, [srcreg+d4+64][rbx]	;;		I5
	vmulpd	zmm2, zmm7, zmm24		;;		B5 = I5 * MemR5					; 7-10		n 13

	vmovapd zmm24, [srcreg+rbp+d2+d1+64]	;;		MemI4
	zfnmaddpd zmm21, zmm12, zmm24, zmm21	;;		A4 - I4 * MemI4 (R4 & R6)			; 8-11		n 19
	zfmaddpd zmm20, zmm9, zmm24, zmm20	;;		B4 + R4 * MemI4 (I4 & I6)			; 8-11		n 20
	vmovsd	Q [srcreg-16], xmm23		;;				Save product of FFT sums	; 8

	vmovapd	zmm6, [srcreg+d4+d1][rbx]	;;		R6
	vmovapd	zmm24, [srcreg+rbp+d4+d1]	;;		MemR6
	vmulpd	zmm15, zmm6, zmm24		;;		A6 = R6 * MemR6					; 9-12		n 14
	vmovapd	zmm8, [srcreg+d4+d1+64][rbx]	;;		I6
	vmulpd	zmm13, zmm8, zmm24		;;		B6 = I6 * MemR6					; 9-12		n 15

	mov	r10, SRC3ARG			;; Load distance to FFT(1)
	cmp	al, 4				;; Case off opcode
	;jb	muladd				;; 3=muladd, fall through
	je	mulsub				;; 4=mulsub

muladd:	cmp	r10, 1				;; Check if we need to multiply add-in by FFT(1)
	jne	muladdhard			;; Yes, do it the hard way

	vaddpd	zmm17, zmm17, [srcreg+r9+d1]		;; R2 = R2 + MemR2
	vaddpd	zmm16, zmm16, [srcreg+r9+d1+64]		;; I2 = I2 + MemI2
	vaddpd	zmm19, zmm19, [srcreg+r9+d2]		;; R3 = R3 + MemR3
	vaddpd	zmm18, zmm18, [srcreg+r9+d2+64]		;; I3 = I3 + MemI3
	vaddpd	zmm21, zmm21, [srcreg+r9+d2+d1]		;; R4 = R4 + MemR4
	vaddpd	zmm20, zmm20, [srcreg+r9+d2+d1+64]	;; I4 = I4 + MemI4

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm14, [srcreg+d4+d2][rbx]	;;		R7
	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmovapd	zmm4, [srcreg+d4+d2+64][rbx]	;;		I7
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm0, [srcreg+d4+d2+d1][rbx]	;;		R8
	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmovapd	zmm5, [srcreg+d4+d2+d1+64][rbx] ;;		I8
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

	vaddpd	zmm23, zmm23, [srcreg+r9]		;; R1 = R1 + MemR1
	vaddpd	zmm12{k6}, zmm12, [srcreg+r9+64]	;; R1b = R1b + MemI1
	vaddpd	zmm22{k7}, zmm22, [srcreg+r9+64]	;; I1 = I1 + MemI1
	vaddpd	zmm3, zmm3, [srcreg+r9+d4]		;; R5 = R5 + MemR5
	vaddpd	zmm2, zmm2, [srcreg+r9+d4+64]		;; I5 = I5 + MemI5

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

	vaddpd	zmm15, zmm15, [srcreg+r9+d4+d1]		;; R6 = R6 + MemR6
	vaddpd	zmm13, zmm13, [srcreg+r9+d4+d1+64]	;; I6 = I6 + MemI6
	vaddpd	zmm9, zmm9, [srcreg+r9+d4+d2]		;; R7 = R7 + MemR7
	vaddpd	zmm1, zmm1, [srcreg+r9+d4+d2+64]	;; I7 = I7 + MemI7
	vaddpd	zmm11, zmm11, [srcreg+r9+d4+d2+d1]	;; R8 = R8 + MemR8
	vaddpd	zmm7, zmm7, [srcreg+r9+d4+d2+d1+64]	;; I8 = I8 + MemI8

	jmp	back_to_orig

muladdhard:
	vmovapd	zmm9, [srcreg+r9+d1]			;; MemR2
	vmovapd	zmm12, [srcreg+r10+d1]			;; MemR2#2
	vmovapd	zmm24, [srcreg+r10+d1+64]		;; MemI2#2
	zfmaddpd zmm17, zmm9, zmm12, zmm17		;; R2 = R2 + MemR2*MemR2#2
	zfmaddpd zmm16, zmm9, zmm24, zmm16		;; I2 = I2 + MemR2*MemI2#2
	vmovapd	zmm9, [srcreg+r9+d1+64]			;; MemI2
	zfnmaddpd zmm17, zmm9, zmm24, zmm17		;; R2 = R2 - MemI2*MemI2#2
	zfmaddpd zmm16, zmm9, zmm12, zmm16		;; I2 = I2 + MemI2*MemR2#2

	vmovapd	zmm9, [srcreg+r9+d2]			;; MemR3
	vmovapd	zmm12, [srcreg+r10+d2]			;; MemR3#2
	vmovapd	zmm24, [srcreg+r10+d2+64]		;; MemI3#2
	zfmaddpd zmm19, zmm9, zmm12, zmm19		;; R3 = R3 + MemR3*MemR3#2
	zfmaddpd zmm18, zmm9, zmm24, zmm18		;; I3 = I3 + MemR3*MemI3#2
	vmovapd	zmm9, [srcreg+r9+d2+64]			;; MemI3
	zfnmaddpd zmm19, zmm9, zmm24, zmm19		;; R3 = R3 - MemI3*MemI3#2
	zfmaddpd zmm18, zmm9, zmm12, zmm18		;; I3 = I3 + MemI3*MemR3#2

	vmovapd	zmm9, [srcreg+r9+d2+d1]			;; MemR4
	vmovapd	zmm12, [srcreg+r10+d2+d1]		;; MemR4#2
	vmovapd	zmm24, [srcreg+r10+d2+d1+64]		;; MemI4#2
	zfmaddpd zmm21, zmm9, zmm12, zmm21		;; R4 = R4 + MemR4*MemR4#2
	zfmaddpd zmm20, zmm9, zmm24, zmm20		;; I4 = I4 + MemR4*MemI4#2
	vmovapd	zmm9, [srcreg+r9+d2+d1+64]		;; MemI4
	zfnmaddpd zmm21, zmm9, zmm24, zmm21		;; R4 = R4 - MemI4*MemI4#2
	zfmaddpd zmm20, zmm9, zmm12, zmm20		;; I4 = I4 + MemI4*MemR4#2

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm14, [srcreg+d4+d2][rbx]	;;		R7
	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmovapd	zmm4, [srcreg+d4+d2+64][rbx]	;;		I7
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm0, [srcreg+d4+d2+d1][rbx]	;;		R8
	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmovapd	zmm5, [srcreg+d4+d2+d1+64][rbx] ;;		I8
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

	vmovapd	zmm10, [srcreg+r9]			;; MemR1
	vmovapd	zmm6, [srcreg+r10]			;; MemR1#2
	vmovapd	zmm8, [srcreg+r10+64]			;; MemI1#2
	zfmaddpd zmm23, zmm10, zmm6, zmm23		;; R1 = R1 + MemR1*MemR1#2
	zfmaddpd zmm22{k7}{z}, zmm10, zmm8, zmm22	;; I1 = I1 + MemR1*MemI1#2
	vmovapd	zmm10, [srcreg+r9+64]			;; MemI1
	zfnmaddpd zmm23{k7}, zmm10, zmm8, zmm23		;; R1 = R1 - MemI1*MemI1#2
	zfmaddpd zmm12{k6}, zmm10, zmm8, zmm12		;; R1b = R1b + MemI1*MemI1#2
	zfmaddpd zmm22{k7}{z}, zmm10, zmm6, zmm22	;; I1 = I1 + MemI1*MemR1#2

	vmovapd	zmm10, [srcreg+r9+d4]			;; MemR5
	vmovapd	zmm6, [srcreg+r10+d4]			;; MemR5#2
	vmovapd	zmm8, [srcreg+r10+d4+64]		;; MemI5#2
	zfmaddpd zmm3, zmm10, zmm6, zmm3		;; R5 = R5 + MemR5*MemR5#2
	zfmaddpd zmm2, zmm10, zmm8, zmm2		;; I5 = I5 + MemR5*MemI5#2
	vmovapd	zmm10, [srcreg+r9+d4+64]		;; MemI5
	zfnmaddpd zmm3, zmm10, zmm8, zmm3		;; R5 = R5 - MemI5*MemI5#2
	zfmaddpd zmm2, zmm10, zmm6, zmm2		;; I5 = I5 + MemI5*MemR5#2

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

	vmovapd	zmm4, [srcreg+r9+d4+d1]			;; MemR6
	vmovapd	zmm6, [srcreg+r10+d4+d1]		;; MemR6#2
	vmovapd	zmm8, [srcreg+r10+d4+d1+64]		;; MemI6#2
	zfmaddpd zmm15, zmm4, zmm6, zmm15		;; R6 = R6 + MemR6*MemR6#2
	zfmaddpd zmm13, zmm4, zmm8, zmm13		;; I6 = I6 + MemR6*MemI6#2
	vmovapd	zmm4, [srcreg+r9+d4+d1+64]		;; MemI6
	zfnmaddpd zmm15, zmm4, zmm8, zmm15		;; R6 = R6 - MemI6*MemI6#2
	zfmaddpd zmm13, zmm4, zmm6, zmm13		;; I6 = I6 + MemI6*MemR6#2

	vmovapd	zmm4, [srcreg+r9+d4+d2]			;; MemR7
	vmovapd	zmm6, [srcreg+r10+d4+d2]		;; MemR7#2
	vmovapd	zmm8, [srcreg+r10+d4+d2+64]		;; MemI7#2
	zfmaddpd zmm9, zmm4, zmm6, zmm9			;; R7 = R7 + MemR7*MemR7#2
	zfmaddpd zmm1, zmm4, zmm8, zmm1			;; I7 = I7 + MemR7*MemI7#2
	vmovapd	zmm4, [srcreg+r9+d4+d2+64]		;; MemI7
	zfnmaddpd zmm9, zmm4, zmm8, zmm9		;; R7 = R7 - MemI7*MemI7#2
	zfmaddpd zmm1, zmm4, zmm6, zmm1			;; I7 = I7 + MemI7*MemR7#2

	vmovapd	zmm4, [srcreg+r9+d4+d2+d1]		;; MemR8
	vmovapd	zmm6, [srcreg+r10+d4+d2+d1]		;; MemR8#2
	vmovapd	zmm8, [srcreg+r10+d4+d2+d1+64]		;; MemI8#2
	zfmaddpd zmm11, zmm4, zmm6, zmm11		;; R8 = R8 + MemR8*MemR8#2
	zfmaddpd zmm7, zmm4, zmm8, zmm7			;; I8 = I8 + MemR8*MemI8#2
	vmovapd	zmm4, [srcreg+r9+d4+d2+d1+64]		;; MemI8
	zfnmaddpd zmm11, zmm4, zmm8, zmm11		;; R8 = R8 - MemI8*MemI8#2
	zfmaddpd zmm7, zmm4, zmm6, zmm7			;; I8 = I8 + MemI8*MemR8#2

	jmp	back_to_orig

mulsub:	cmp	r10, 1				;; Check if we need to multiply add-in by FFT(1)
	jne	mulsubhard			;; Yes, do it the hard way

	vsubpd	zmm17, zmm17, [srcreg+r9+d1]		;; R2 = R2 - MemR2
	vsubpd	zmm16, zmm16, [srcreg+r9+d1+64]		;; I2 = I2 - MemI2
	vsubpd	zmm19, zmm19, [srcreg+r9+d2]		;; R3 = R3 - MemR3
	vsubpd	zmm18, zmm18, [srcreg+r9+d2+64]		;; I3 = I3 - MemI3
	vsubpd	zmm21, zmm21, [srcreg+r9+d2+d1]		;; R4 = R4 - MemR4
	vsubpd	zmm20, zmm20, [srcreg+r9+d2+d1+64]	;; I4 = I4 - MemI4

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm14, [srcreg+d4+d2][rbx]	;;		R7
	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmovapd	zmm4, [srcreg+d4+d2+64][rbx]	;;		I7
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm0, [srcreg+d4+d2+d1][rbx]	;;		R8
	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmovapd	zmm5, [srcreg+d4+d2+d1+64][rbx] ;;		I8
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

	vsubpd	zmm23, zmm23, [srcreg+r9]		;; R1 = R1 - MemR1
	vsubpd	zmm12{k6}, zmm12, [srcreg+r9+64]	;; R1b = R1b - MemI1
	vsubpd	zmm22{k7}, zmm22, [srcreg+r9+64]	;; I1 = I1 - MemI1
	vsubpd	zmm3, zmm3, [srcreg+r9+d4]		;; R5 = R5 - MemR5
	vsubpd	zmm2, zmm2, [srcreg+r9+d4+64]		;; I5 = I5 - MemI5

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

	vsubpd	zmm15, zmm15, [srcreg+r9+d4+d1]		;; R6 = R6 - MemR6
	vsubpd	zmm13, zmm13, [srcreg+r9+d4+d1+64]	;; I6 = I6 - MemI6
	vsubpd	zmm9, zmm9, [srcreg+r9+d4+d2]		;; R7 = R7 - MemR7
	vsubpd	zmm1, zmm1, [srcreg+r9+d4+d2+64]	;; I7 = I7 - MemI7
	vsubpd	zmm11, zmm11, [srcreg+r9+d4+d2+d1]	;; R8 = R8 - MemR8
	vsubpd	zmm7, zmm7, [srcreg+r9+d4+d2+d1+64]	;; I8 = I8 - MemI8

	jmp	back_to_orig

mulsubhard:
	vmovapd	zmm9, [srcreg+r9+d1]			;; MemR2
	vmovapd	zmm12, [srcreg+r10+d1]			;; MemR2#2
	vmovapd	zmm24, [srcreg+r10+d1+64]		;; MemI2#2
	zfnmaddpd zmm17, zmm9, zmm12, zmm17		;; R2 = R2 - MemR2*MemR2#2
	zfnmaddpd zmm16, zmm9, zmm24, zmm16		;; I2 = I2 - MemR2*MemI2#2
	vmovapd	zmm9, [srcreg+r9+d1+64]			;; MemI2
	zfmaddpd zmm17, zmm9, zmm24, zmm17		;; R2 = R2 + MemI2*MemI2#2
	zfnmaddpd zmm16, zmm9, zmm12, zmm16		;; I2 = I2 - MemI2*MemR2#2

	vmovapd	zmm9, [srcreg+r9+d2]			;; MemR3
	vmovapd	zmm12, [srcreg+r10+d2]			;; MemR3#2
	vmovapd	zmm24, [srcreg+r10+d2+64]		;; MemI3#2
	zfnmaddpd zmm19, zmm9, zmm12, zmm19		;; R3 = R3 - MemR3*MemR3#2
	zfnmaddpd zmm18, zmm9, zmm24, zmm18		;; I3 = I3 - MemR3*MemI3#2
	vmovapd	zmm9, [srcreg+r9+d2+64]			;; MemI3
	zfmaddpd zmm19, zmm9, zmm24, zmm19		;; R3 = R3 + MemI3*MemI3#2
	zfnmaddpd zmm18, zmm9, zmm12, zmm18		;; I3 = I3 - MemI3*MemR3#2

	vmovapd	zmm9, [srcreg+r9+d2+d1]			;; MemR4
	vmovapd	zmm12, [srcreg+r10+d2+d1]		;; MemR4#2
	vmovapd	zmm24, [srcreg+r10+d2+d1+64]		;; MemI4#2
	zfnmaddpd zmm21, zmm9, zmm12, zmm21		;; R4 = R4 - MemR4*MemR4#2
	zfnmaddpd zmm20, zmm9, zmm24, zmm20		;; I4 = I4 - MemR4*MemI4#2
	vmovapd	zmm9, [srcreg+r9+d2+d1+64]		;; MemI4
	zfmaddpd zmm21, zmm9, zmm24, zmm21		;; R4 = R4 + MemI4*MemI4#2
	zfnmaddpd zmm20, zmm9, zmm12, zmm20		;; I4 = I4 - MemI4*MemR4#2

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm14, [srcreg+d4+d2][rbx]	;;		R7
	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmovapd	zmm4, [srcreg+d4+d2+64][rbx]	;;		I7
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm0, [srcreg+d4+d2+d1][rbx]	;;		R8
	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmovapd	zmm5, [srcreg+d4+d2+d1+64][rbx] ;;		I8
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

	vmovapd	zmm10, [srcreg+r9]			;; MemR1
	vmovapd	zmm6, [srcreg+r10]			;; MemR1#2
	vmovapd	zmm8, [srcreg+r10+64]			;; MemI1#2
	zfnmaddpd zmm23, zmm10, zmm6, zmm23		;; R1 = R1 - MemR1*MemR1#2
	zfnmaddpd zmm22{k7}{z}, zmm10, zmm8, zmm22	;; I1 = I1 - MemR1*MemI1#2
	vmovapd	zmm10, [srcreg+r9+64]			;; MemI1
	zfmaddpd zmm23{k7}, zmm10, zmm8, zmm23		;; R1 = R1 + MemI1*MemI1#2
	zfnmaddpd zmm12{k6}, zmm10, zmm8, zmm12		;; R1b = R1b - MemI1*MemI1#2
	zfnmaddpd zmm22{k7}{z}, zmm10, zmm6, zmm22	;; I1 = I1 - MemI1*MemR1#2

	vmovapd	zmm10, [srcreg+r9+d4]			;; MemR5
	vmovapd	zmm6, [srcreg+r10+d4]			;; MemR5#2
	vmovapd	zmm8, [srcreg+r10+d4+64]		;; MemI5#2
	zfnmaddpd zmm3, zmm10, zmm6, zmm3		;; R5 = R5 - MemR5*MemR5#2
	zfnmaddpd zmm2, zmm10, zmm8, zmm2		;; I5 = I5 - MemR5*MemI5#2
	vmovapd	zmm10, [srcreg+r9+d4+64]		;; MemI5
	zfmaddpd zmm3, zmm10, zmm8, zmm3		;; R5 = R5 + MemI5*MemI5#2
	zfnmaddpd zmm2, zmm10, zmm6, zmm2		;; I5 = I5 - MemI5*MemR5#2

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

	vmovapd	zmm4, [srcreg+r9+d4+d1]			;; MemR6
	vmovapd	zmm6, [srcreg+r10+d4+d1]		;; MemR6#2
	vmovapd	zmm8, [srcreg+r10+d4+d1+64]		;; MemI6#2
	zfnmaddpd zmm15, zmm4, zmm6, zmm15		;; R6 = R6 - MemR6*MemR6#2
	zfnmaddpd zmm13, zmm4, zmm8, zmm13		;; I6 = I6 - MemR6*MemI6#2
	vmovapd	zmm4, [srcreg+r9+d4+d1+64]		;; MemI6
	zfmaddpd zmm15, zmm4, zmm8, zmm15		;; R6 = R6 + MemI6*MemI6#2
	zfnmaddpd zmm13, zmm4, zmm6, zmm13		;; I6 = I6 - MemI6*MemR6#2

	vmovapd	zmm4, [srcreg+r9+d4+d2]			;; MemR7
	vmovapd	zmm6, [srcreg+r10+d4+d2]		;; MemR7#2
	vmovapd	zmm8, [srcreg+r10+d4+d2+64]		;; MemI7#2
	zfnmaddpd zmm9, zmm4, zmm6, zmm9		;; R7 = R7 - MemR7*MemR7#2
	zfnmaddpd zmm1, zmm4, zmm8, zmm1		;; I7 = I7 - MemR7*MemI7#2
	vmovapd	zmm4, [srcreg+r9+d4+d2+64]		;; MemI7
	zfmaddpd zmm9, zmm4, zmm8, zmm9			;; R7 = R7 + MemI7*MemI7#2
	zfnmaddpd zmm1, zmm4, zmm6, zmm1		;; I7 = I7 - MemI7*MemR7#2

	vmovapd	zmm4, [srcreg+r9+d4+d2+d1]		;; MemR8
	vmovapd	zmm6, [srcreg+r10+d4+d2+d1]		;; MemR8#2
	vmovapd	zmm8, [srcreg+r10+d4+d2+d1+64]		;; MemI8#2
	zfnmaddpd zmm11, zmm4, zmm6, zmm11		;; R8 = R8 - MemR8*MemR8#2
	zfnmaddpd zmm7, zmm4, zmm8, zmm7		;; I8 = I8 - MemR8*MemI8#2
	vmovapd	zmm4, [srcreg+r9+d4+d2+d1+64]		;; MemI8
	zfmaddpd zmm11, zmm4, zmm8, zmm11		;; R8 = R8 + MemI8*MemI8#2
	zfnmaddpd zmm7, zmm4, zmm6, zmm7		;; I8 = I8 - MemI8*MemR8#2

	jmp	back_to_orig

orig:

	vmovapd	zmm2, [srcreg+d1][rbx]		;;		R2
	vmovapd	zmm24, [srcreg+rbp+d1]		;;		MemR2
	vmulpd	zmm17, zmm2, zmm24		;;		A2 = R2 * MemR2					; 1-4		n 5
	vmovapd	zmm3, [srcreg+d1+64][rbx]	;;		I2
	vmulpd	zmm16, zmm3, zmm24		;;		B2 = I2 * MemR2					; 1-4		n 5

	vmovapd	zmm13, [srcreg+d2][rbx]		;;		R3
	vmovapd	zmm24, [srcreg+rbp+d2]		;;		MemR3
	vmulpd	zmm19, zmm13, zmm24		;;		A3 = R3 * MemR3					; 2-5		n 6
	vmovapd	zmm15, [srcreg+d2+64][rbx]	;;		I3
	vmulpd	zmm18, zmm15, zmm24		;;		B3 = I3 * MemR3					; 2-5		n 6

	vmovapd	zmm9, [srcreg+d2+d1][rbx]	;;		R4
	vmovapd	zmm24, [srcreg+rbp+d2+d1]	;;		MemR4
	vmulpd	zmm21, zmm9, zmm24		;;		A4 = R4 * MemR4					; 3-6		n 8
	vmovapd	zmm12, [srcreg+d2+d1+64][rbx]	;;		I4
	vmulpd	zmm20, zmm12, zmm24		;;		B4 = I4 * MemR4					; 3-6		n 8

	vmovapd	zmm1, [srcreg][rbx]		;; R1				R1a
	vmovapd	zmm24, [srcreg+rbp]		;; MemR1			MemR1a
	vmulpd	zmm23, zmm1, zmm24		;; A1 = R1 * MemR1		R1a*MemR1a (R1a)		; 4-7		n 10
	vmovapd	zmm11, [srcreg+64][rbx]		;; I1				R1b
	vmulpd	zmm22 {k7}{z}, zmm11, zmm24	;; B1 = I1 * MemR1		0				; 4-7		n 10

	vmovapd zmm24, [srcreg+rbp+d1+64]	;;		MemI2
	zfnmaddpd zmm17, zmm3, zmm24, zmm17	;;		A2 - I2 * MemI2 (R2)				; 5-8		n 10
	zfmaddpd zmm16, zmm2, zmm24, zmm16	;;		B2 + R2 * MemI2 (I2)				; 5-8		n 18

	vmovapd zmm24, [srcreg+rbp+d2+64]	;;		MemI3
	zfnmaddpd zmm19, zmm15, zmm24, zmm19	;;		A3 - I3 * MemI3 (R3 & R5)			; 6-9		n 19
	zfmaddpd zmm18, zmm13, zmm24, zmm18	;;		B3 + R3 * MemI3 (I3 & I5)			; 6-9		n 20

	vmovapd	zmm10, [srcreg+d4][rbx]		;;		R5
	vmovapd	zmm24, [srcreg+rbp+d4]		;;		MemR5
	vmulpd	zmm3, zmm10, zmm24		;;		A5 = R5 * MemR5					; 7-10		n 12
	vmovapd	zmm7, [srcreg+d4+64][rbx]	;;		I5
	vmulpd	zmm2, zmm7, zmm24		;;		B5 = I5 * MemR5					; 7-10		n 13

	vmovapd zmm24, [srcreg+rbp+d2+d1+64]	;;		MemI4
	zfnmaddpd zmm21, zmm12, zmm24, zmm21	;;		A4 - I4 * MemI4 (R4 & R6)			; 8-11		n 19
	zfmaddpd zmm20, zmm9, zmm24, zmm20	;;		B4 + R4 * MemI4 (I4 & I6)			; 8-11		n 20
	vmovsd	Q [srcreg-16], xmm23		;;				Save product of FFT sums	; 8

	vmovapd	zmm6, [srcreg+d4+d1][rbx]	;;		R6
	vmovapd	zmm24, [srcreg+rbp+d4+d1]	;;		MemR6
	vmulpd	zmm15, zmm6, zmm24		;;		A6 = R6 * MemR6					; 9-12		n 14
	vmovapd	zmm8, [srcreg+d4+d1+64][rbx]	;;		I6
	vmulpd	zmm13, zmm8, zmm24		;;		B6 = I6 * MemR6					; 9-12		n 15

	vmovapd	zmm12, zmm17			;;		Copy R2
	vmovapd zmm24, [srcreg+rbp+64]		;; MemI1			MemR1b
	zfnmaddpd zmm23 {k7}, zmm11, zmm24, zmm23;; A1 - I1 * MemI1 (R1)	R1a				; 10-13		n 16
	zfmaddpd zmm22 {k7}{z}, zmm1, zmm24, zmm22;; B1 + R1 * MemI1 (I1)	0				; 10-13		n 18

	vmulpd	zmm12 {k6}, zmm11, zmm24	;; R2				R1b * MemR1b (R1b)		; 11-14		n 16

	vmovapd	zmm14, [srcreg+d4+d2][rbx]	;;		R7
	vmovapd	zmm24, [srcreg+rbp+d4+d2]	;;		MemR7
	vmulpd	zmm9, zmm14, zmm24		;;		A7 = R7 * MemR7					; 11-14		n 15
	vmovapd	zmm4, [srcreg+d4+d2+64][rbx]	;;		I7
	vmulpd	zmm1, zmm4, zmm24		;;		B7 = I7 * MemR7					; 12-15		n 16

	vmovapd zmm24, [srcreg+rbp+d4+64]	;;		MemI5
	zfnmaddpd zmm3, zmm7, zmm24, zmm3	;;		A5 - I5 * MemI5 (R5 & R9)			; 12-15		n 21
	zfmaddpd zmm2, zmm10, zmm24, zmm2	;;		B5 + R5 * MemI5 (I5 & I9)			; 13-16		n 22

	vmovapd	zmm0, [srcreg+d4+d2+d1][rbx]	;;		R8
	vmovapd	zmm24, [srcreg+rbp+d4+d2+d1]	;;		MemR8
	vmulpd	zmm11, zmm0, zmm24		;;		A8 = R8 * MemR8					; 13-16		n 17
	vmovapd	zmm5, [srcreg+d4+d2+d1+64][rbx] ;;		I8
	vmulpd	zmm7, zmm5, zmm24		;;		B8 = I8 * MemR8					; 14-17		n 18

	vmovapd zmm24, [srcreg+rbp+d4+d1+64]	;;		MemI6
	zfnmaddpd zmm15, zmm8, zmm24, zmm15	;;		A6 - I6 * MemI6 (R6 & R10)			; 14-17		n 21
	zfmaddpd zmm13, zmm6, zmm24, zmm13	;;		B6 + R6 * MemI6 (I6 & I10)			; 15-18		n 22

	vmovapd zmm24, [srcreg+rbp+d4+d2+64]	;;		MemI7
	zfnmaddpd zmm9, zmm4, zmm24, zmm9	;;		A7 - I7 * MemI7 (R7 & R11)			; 15-18		n 23
	zfmaddpd zmm1, zmm14, zmm24, zmm1	;;		B7 + R7 * MemI7 (I7 & I11)			; 16-19		n 24

						;;				R1a/R1b becomes R1/R2
	vaddpd	zmm10, zmm23, zmm12		;; R1 + R2 (new R1)		R1 + R2 (new R1/HALF)		; 16-19		n 26
	vsubpd	zmm23, zmm23, zmm12		;; R1 - R2 (new R2)		R1 - R2 (new R2/HALF)		; 17-20		n 28

	vmovapd zmm24, [srcreg+rbp+d4+d2+d1+64] ;;		MemI8
	zfnmaddpd zmm11, zmm5, zmm24, zmm11	;;		A8 - I8 * MemI8 (R8 & R12)			; 17-20		n 23
	zfmaddpd zmm7, zmm0, zmm24, zmm7	;;		B8 + R8 * MemI8 (I8 & I12)			; 18-21		n 24

back_to_orig:					;;				R2/I2 becomes R3/R4
	vaddpd	zmm17 {k7}, zmm22, zmm16	;; I1 + I2 (new I1)		R3				; 18-47		n 25
	vsubpd	zmm16 {k7}, zmm22, zmm16	;; I1 - I2 (new I2)		R4				; 19-47		n 26
	vbroadcastsd zmm30, ZMM_P924_P383

	vaddpd	zmm8, zmm21, zmm19		;; R4 + R3 (new R3)		R6 + R5 (new R5)		; 19-52		n 25
	vsubpd	zmm21, zmm21, zmm19		;; R4 - R3 (new I4)		R6 - R5 (new negR6)		; 20-52		n 33
	vbroadcastsd zmm31, ZMM_SQRTHALF

	vaddpd	zmm6, zmm18, zmm20		;; I3 + I4 (new I3)		I5 + I6 (new I5)		; 20-53		n 27
	vsubpd	zmm18, zmm18, zmm20		;; I3 - I4 (new R4)		I5 - I6 (new I6)		; 21-53		n 26
	vbroadcastf64x4 zmm27, ZMM_383_707_383_1 ;; .383 SQRTHALF .383 1 .383 SQRTHALF .383 1

	vaddpd	zmm4, zmm3, zmm15		;; R5 + R6 (new R5)		R9 + R10 (new R9)		; 21-48		n 34
	vsubpd	zmm3, zmm3, zmm15		;; R5 - R6 (new R6)		R9 - R10 (new R10)		; 22-48		n 29
	vbroadcastsd zmm29, ZMM_P383

	vaddpd	zmm14, zmm2, zmm13		;; I5 + I6 (new I5)		I9 + I10 (new I9)		; 22-49		n 35
	vsubpd	zmm2, zmm2, zmm13		;; I5 - I6 (new I6)		I9 - I10 (new I10)		; 23-49		n 
	L1prefetchw srcreg+L1pd, L1pt

	vaddpd	zmm12, zmm11, zmm9		;; R8 + R7 (new R7)		R12 + R11 (new R11)		; 23-50		n 34
	vsubpd	zmm11, zmm11, zmm9		;; R8 - R7 (new I8)		R12 - R11 (new I12)		; 24-50		n 29
	L1prefetchw srcreg+64+L1pd, L1pt

	vaddpd	zmm5, zmm1, zmm7		;; I7 + I8 (new I7)		I11 + I12 (new I11)		; 24-51		n 35
	vsubpd	zmm1, zmm1, zmm7		;; I7 - I8 (new R8)		I11 - I12 (new R12)		; 25-51		n 32
	L1prefetchw srcreg+d1+L1pd, L1pt

	vblendmpd zmm26 {k7}, zmm30, ZMM_ONE{1to8} ;; 1, 1, 1, 1, 1, 1, 1	.924/.383			; ?		n 
	vblendmpd zmm28 {k6}, zmm26, ZMM_HALF{1to8} ;; 1, 1, 1, 1, 1, 1, 1	HALF				; 25***		n 
	vblendmpd zmm0 {k6}, zmm8, zmm17	;; R3				R3				; 25		n 26
	vblendmpd zmm15 {k6}, zmm18, zmm16	;; R4				R4				; 26		n 28

	zfmaddpd zmm13, zmm10, zmm28, zmm0	;; R1*1 + R3 (newer R1)		R1*HALF + R3 (newer R1)		; 26-57		n 
	zfmsubpd zmm10, zmm10, zmm28, zmm0	;; R1*1 - R3 (newer R3)		R1*HALF - R3 (newer R3)		; 27-57		n 
	L1prefetchw srcreg+d1+64+L1pd, L1pt

	vaddpd	zmm9 {k7}{z}, zmm17, zmm6	;; I1 + I3 (newer I1)		0				; 27-59		n 38
	vsubpd	zmm17 {k7}{z}, zmm17, zmm6	;; I1 - I3 (newer I3)		0				; 28-59		n 39
	L1prefetchw srcreg+d2+L1pd, L1pt

	zfmaddpd zmm7, zmm23, zmm28, zmm15	;; R2*1 + R4 (newer R2)		R2*HALF + R4 (newer R2)		; 28-61		n 45
	zfmsubpd zmm23, zmm23, zmm28, zmm15	;; R2*1 - R4 (newer R4)		R2*HALF - R4 (newer R4)		; 29-61		n 46
	L1prefetchw srcreg+d2+64+L1pd, L1pt

	vblendmpd zmm0 {k6}, zmm3, zmm11	;; R6				I12				; 29		n 
	vblendmpd zmm11 {k6}, zmm11, zmm3	;; I8				R10				; 30		n 

						;; multiply R6/I6 by SQRTHALF - i*SQRTHALF
	vaddpd	zmm15, zmm2, zmm0		;; I6 + R6 (new2 R6/SQRTHALF)	I10 + I12 (newer I10)		; 30-54		n 36
	vsubpd	zmm2, zmm2, zmm0		;; I6 - R6 (new2 I6/SQRTHALF)	I10 - I12 (newer I12)		; 31-54		n 37
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vblendmpd zmm16 {k6}, zmm16, zmm18	;; I2				I6				; 31		n 

						;; multiply R8/I8 by SQRTHALF - i*SQRTHALF
	vaddpd	zmm3, zmm11, zmm1		;; I8 + R8 (new2 R8/SQRTHALF)	R10 + R12 (newer R10)		; 32-55		n 36
	vsubpd	zmm11, zmm11, zmm1		;; I8 - R8 (new2 I8/SQRTHALF)	R10 - R12 (newer R12)		; 32-55		n 37
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt

						;;				mul R6/I6 by w^2 = .707 - .707i
	vaddpd	zmm0, zmm16, zmm21		;; I2 + I4 (newer I2)		I6 + negR6 (newer I6/SQRTHALF) 	; 33-60		n 44
	vsubpd	zmm16, zmm16, zmm21		;; I2 - I4 (newer I4)		I6 - negR6 (newer R6/SQRTHALF) 	; 33-60		n 44
	L1prefetchw srcreg+d4+L1pd, L1pt

	vsubpd	zmm1, zmm12, zmm4		;; R7 - R5 (newer I7)		R11 - R9 (newer negR11)		; 34-58		n 38
	vaddpd	zmm12, zmm12, zmm4		;; R7 + R5 (newer R5)		R11 + R9 (newer R9)		; 34-37		n 38
	L1prefetchw srcreg+d4+64+L1pd, L1pt

	vaddpd	zmm4, zmm14, zmm5		;; I5 + I7 (newer I5)		I9 + I11 (newer I9)		; 35-56		n 42
	vsubpd	zmm14, zmm14, zmm5		;; I5 - I7 (newer R7)		I9 - I11 (newer I11)		; 35-56		n 39
	L1prefetchw srcreg+d4+d1+L1pd, L1pt

						;;				mul R10/I10 by w^1 = .924 - .383i
	zfmaddpd zmm5, zmm3, zmm26, zmm15	;; R6 + R8*1 (newer R6/SQRTHALF)    I10 + R10*.924/.383 (R10/.383) ; 36-71	n 44
	zfmsubpd zmm15, zmm15, zmm26, zmm3	;; R6*1 - R8 (newer negI8/SQRTHALF) I10*.924/.383 - R10 (I10/.383) ; 36-71	n 
	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt

						;;				mul R12/I12 by w^3 = .383 - .924i
	zfmaddpd zmm3, zmm2, zmm26, zmm11	;; I6*1 + I8 (newer I6/SQRTHALF)  I12*.924/.383 + R12 (R12/.383); 37-63		n 47
	zfnmaddpd zmm11, zmm11, zmm26, zmm2	;; I6 - I8*1 (newer R8/SQRTHALF)  I12 - R12*.924/.383 (I12/.383); 37-63		n 44
	L1prefetchw srcreg+d4+d2+L1pd, L1pt

						;;				R5/I5 becomes new R5/R7
	vblendmpd zmm8 {k6}, zmm12, zmm8	;; R5				R5				; 38		n 40
	vsubpd	zmm1 {k6}, zmm9, zmm1		;; I7				R11 = 0 - negR11		; 38-55		n 43
	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt

	vblendmpd zmm6 {k6}, zmm14, zmm6	;; R7				R7				; 39		n 41
	vblendmpd zmm14 {k6}, zmm17, zmm14	;; I3				I11				; 39		n 43

;; four aparts

	vaddpd	zmm2, zmm13, zmm8		;; R1 + R5 (final R1)		R1 + R5 (final R1)		; 40-66		n 
	vsubpd	zmm13, zmm13, zmm8		;; R1 - R5 (final R5)		R1 - R5 (final R5)		; 40-66		n 
	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	vaddpd	zmm8, zmm10, zmm6		;; R3 + R7 (final R3)		R3 + R7 (final R3)		; 41-64		n 
	vsubpd	zmm10, zmm10, zmm6		;; R3 - R7 (final R7)		R3 - R7 (final R7)		; 41-64		n 
	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt

						;;				R9/I9 becomes newer R9/R13
	vaddpd	zmm12 {k7}, zmm9, zmm4		;; I1 + I5 (final I1)		R9				; 42-67		n 
	vsubpd	zmm4 {k7}, zmm9, zmm4		;; I1 - I5 (final I5)		R13				; 42-67		n 

						;;				R11/I11 becomes newer R11/R15
						;;				mul R11/I11 by SQRTHALF - i*SQRTHALF
	vaddpd	zmm6, zmm14, zmm1		;; I3 + I7 (final I3)		I11 + R11 (final R11/SQRTHALF)	; 43-65		n 
	vsubpd	zmm14, zmm14, zmm1		;; I3 - I7 (final I7)		I11 - R11 (final I11/SQRTHALF)	; 43-65		n 

						;;				R6/I6 becomes new R6/R8
	vblendmpd zmm9 {k6}, zmm5, zmm16	;; R6/SQRTHALF			R6/SQRTHALF			; 44		n 
	vblendmpd zmm1 {k6}, zmm11, zmm0	;; R8/SQRTHALF			R8/SQRTHALF			; 44		n 

	zfmaddpd zmm17, zmm9, zmm31, zmm7	;; R2 + R6*SQRTHALF (final R2)	R2 + R6 * SQRTHALF (final R2)	; 45-68		n 
	zfnmaddpd zmm9, zmm9, zmm31, zmm7	;; R2 - R6*SQRTHALF (final R6)	R2 - R6 * SQRTHALF (final R6)	; 45-68		n 

	zfmaddpd zmm7, zmm1, zmm31, zmm23	;; R4 + R8*SQRTHALF (final R4)	R4 + R8 * SQRTHALF (final R4)	; 46-70		n 
	zfnmaddpd zmm1, zmm1, zmm31, zmm23	;; R4 - R8*SQRTHALF (final R8)	R4 - R8 * SQRTHALF (final R8)	; 46-70		n 

						;;				R10/I10 becomes newer R10/R14
	vmovapd	zmm18, zmm15			;; negI8			R14/.383
;; bug - hidden vmovapd uops
	zfmaddpd zmm5 {k7}, zmm3, zmm31, zmm0	;; I2 + I6*SQRTHALF (final I2)	R10/.383			; 47-71		n 
	zfnmaddpd zmm15 {k7}, zmm3, zmm31, zmm0	;; I2 - I6*SQRTHALF (final I6)	R14/.383			; 47-71		n 

						;;				R12/I12 becomes newer R12/R16
;; bug - hidden vmovapd uops
	zfmaddpd zmm11 {k7}, zmm18, zmm31, zmm16;; I4 + negI8*SQRTHALF (final I8)  R16/.383			; 48-69		n 
	zfnmaddpd zmm3 {k7}, zmm18, zmm31, zmm16;; I4 - negI8*SQRTHALF (final I4)  R12/.383			; 48-69		n 

	;; shuffle inputs are:
	;; R1 = R8_0 R7_0 R4_0 R3_0 R6_0 R5_0 R2_0 R1_0	goal new R1 is  R1_7 R1_6 R1_5 R1_4 R1_3 R1_2 R1_1 R1_0
	;; R2 = R8_1 R7_1 R4_1 R3_1 R6_1 R5_1 R2_1 R1_1	goal new R2 is  R2_7 R2_6 R2_5 R2_4 R2_3 R2_2 R2_1 R2_0, etc.
	;; I1 = I8_0 I7_0 I4_0 I3_0 I6_0 I5_0 I2_0 I1_0	goal new I1 is  I1_7 I1_6 I1_5 I1_4 I1_3 I1_2 I1_1 I1_0, etc.

	vshuff64x2 zmm0, zmm2, zmm8, 01000100b	;; R6_2 R5_2 R2_2 R1_2 R6_0 R5_0 R2_0 R1_0 (R13L)		; 126-128	n 139
	vshuff64x2 zmm2, zmm2, zmm8, 11101110b	;; R8_2 R7_2 R4_2 R3_2 R8_0 R7_0 R4_0 R3_0 (R13H)		; 125-127	n 131

	vshuff64x2 zmm8, zmm13, zmm10, 01000100b;; R6_6 R5_6 R2_6 R1_6 R6_4 R5_4 R2_4 R1_4 (R57L)		; 124-126	n 139
	vshuff64x2 zmm13, zmm13, zmm10, 11101110b;; R8_6 R7_6 R4_6 R3_6 R8_4 R7_4 R4_4 R3_4 (R57H)		; 123-125	n 131

	vshuff64x2 zmm10, zmm12, zmm6, 01000100b;; I6_2 I5_2 I2_2 I1_2 I6_0 I5_0 I2_0 I1_0 (I13L)		; 120-122	n 137
	vshuff64x2 zmm12, zmm12, zmm6, 11101110b;; I8_2 I7_2 I4_2 I3_2 I8_0 I7_0 I4_0 I3_0 (I13H)		; 119-121	n 129

	vshuff64x2 zmm6, zmm4, zmm14, 01000100b	;; I6_6 I5_6 I2_6 I1_6 I6_4 I5_4 I2_4 I1_4 (I57L)		; 122-124	n 137
	vshuff64x2 zmm4, zmm4, zmm14, 11101110b	;; I8_6 I7_6 I4_6 I3_6 I8_4 I7_4 I4_4 I3_4 (I57H)		; 121-123	n 129

	vshuff64x2 zmm14, zmm17, zmm7, 01000100b;; R6_3 R5_3 R2_3 R1_3 R6_1 R5_1 R2_1 R1_1 (R24L)		; 134-136	n 143
	vshuff64x2 zmm17, zmm17, zmm7, 11101110b;; R8_3 R7_3 R4_3 R3_3 R8_1 R7_1 R4_1 R3_1 (R24H)		; 132-134	n 135	r132

	vshuff64x2 zmm7, zmm9, zmm1, 01000100b	;; R6_7 R5_7 R2_7 R1_7 R6_5 R5_5 R2_5 R1_5 (R68L)		; 133-135	n 143
	vshuff64x2 zmm9, zmm9, zmm1, 11101110b	;; R8_7 R7_7 R4_7 R3_7 R8_5 R7_5 R4_5 R3_5 (R68H)		; 131-133	n 135

	vshuff64x2 zmm1, zmm5, zmm3, 01000100b	;; I6_3 I5_3 I2_3 I1_3 I6_1 I5_1 I2_1 I1_1 (I24L)		; 128-130	n 141
	vshuff64x2 zmm5, zmm5, zmm3, 11101110b	;; I8_3 I7_3 I4_3 I3_3 I8_1 I7_1 I4_1 I3_1 (I24H)		; 127-129	n 133

	vshuff64x2 zmm3, zmm15, zmm11, 01000100b;; I6_7 I5_7 I2_7 I1_7 I6_5 I5_5 I2_5 I1_5 (I68L)		; 130-132	n 141
	vshuff64x2 zmm15, zmm15, zmm11, 11101110b;; I8_7 I7_7 I4_7 I3_7 I8_5 I7_5 I4_5 I3_5 (I68H)		; 129-131	n 133

	;; shuffle the four aparts
	vshuff64x2 zmm11, zmm2, zmm13, 10001000b;; R4_6 R3_6 R4_4 R3_4 R4_2 R3_2 R4_0 R3_0 (R1357LH)		; 141-143	n 148	r136
	vshuff64x2 zmm2, zmm2, zmm13, 11011101b	;; R8_6 R7_6 R8_4 R7_4 R8_2 R7_2 R8_0 R7_0 (R1357HH)		; 137-139	n 144	r136

	vshuff64x2 zmm13, zmm12, zmm4, 10001000b;; I4_6 I3_6 I4_4 I3_4 I4_2 I3_2 I4_0 I3_0 (I1357LH)		; 136-138	n 147	r134
	vshuff64x2 zmm12, zmm12, zmm4, 11011101b;; I8_6 I7_6 I8_4 I7_4 I8_2 I7_2 I8_0 I7_0 (I1357HH)		; 135-137	n 143	r134

	vshuff64x2 zmm4, zmm17, zmm9, 10001000b	;; R4_7 R3_7 R4_5 R3_5 R4_3 R3_3 R4_1 R3_1 (R2468LH)		; 142-144	n 148	r140
	vshuff64x2 zmm17, zmm17, zmm9, 11011101b;; R8_7 R7_7 R8_5 R7_5 R8_3 R7_3 R8_1 R7_1 (R2468HH)		; 140-142	n 144	r140

	vshuff64x2 zmm9, zmm5, zmm15, 10001000b	;; I4_7 I3_7 I4_5 I3_5 I4_3 I3_3 I4_1 I3_1 (I2468LH)		; 139-141	n 147	r138
	vshuff64x2 zmm5, zmm5, zmm15, 11011101b ;; I8_7 I7_7 I8_5 I7_5 I8_3 I7_3 I8_1 I7_1 (I2468HH)		; 138-140	n 143	r138

	vshuff64x2 zmm15, zmm0, zmm8, 10001000b	;; R2_6 R1_6 R2_4 R1_4 R2_2 R1_2 R2_0 R1_0 (R1357LL)		; 162-164	n 166
	vshuff64x2 zmm0, zmm0, zmm8, 11011101b	;; R6_6 R5_6 R6_4 R5_4 R6_2 R5_2 R6_0 R5_0 (R1357HL)		; 154-156	n 158

	vshuff64x2 zmm8, zmm14, zmm7, 10001000b	;; R2_7 R1_7 R2_5 R1_5 R2_3 R1_3 R2_1 R1_1 (R2468LL)		; 163-165	n 166
	vshuff64x2 zmm14, zmm14, zmm7, 11011101b;; R6_7 R5_7 R6_5 R5_5 R6_3 R5_3 R6_1 R5_1 (R2468HL)		; 153-155	n 158

	vshuff64x2 zmm7, zmm10, zmm6, 10001000b	;; I2_6 I1_6 I2_4 I1_4 I2_2 I1_2 I2_0 I1_0 (I1357LL)		; 155-157	n 161
	vshuff64x2 zmm10, zmm10, zmm6, 11011101b;; I6_6 I5_6 I6_4 I5_4 I6_2 I5_2 I6_0 I5_0 (I1357HL)		; 151-153	n 157

	vshuff64x2 zmm6, zmm1, zmm3, 10001000b	;; I2_7 I1_7 I2_5 I1_5 I2_3 I1_3 I2_1 I1_1 (I2468LL)		; 156-158	n 161
	vshuff64x2 zmm1, zmm1, zmm3, 11011101b	;; I6_7 I5_7 I6_5 I5_5 I6_3 I5_3 I6_1 I5_1 (I2468HL)		; 152-154	n 157

	;; shufpd the one aparts
	vshufpd	zmm3, zmm11, zmm4, 00000000b	;; R3_7 R3_6 R3_5 R3_4 R3_3 R3_2 R3_1 R3_0 (next R3)	; 148		n 149
	vshufpd	zmm16, zmm13, zmm9, 00000000b	;; I3_7 I3_6 I3_5 I3_4 I3_3 I3_2 I3_1 I3_0 (next I3)	; 147		n 149

	vshufpd	zmm18, zmm15, zmm8, 11111111b	;; R2_7 R2_6 R2_5 R2_4 R2_3 R2_2 R2_1 R2_0 (next R2)	; 166		n 168
	vshufpd	zmm19, zmm7, zmm6, 11111111b	;; I2_7 I2_6 I2_5 I2_4 I2_3 I2_2 I2_1 I2_0 (next I2)	; 164		n 168

 	vshufpd	zmm11, zmm11, zmm4, 11111111b	;; R4_7 R4_6 R4_5 R4_4 R4_3 R4_2 R4_1 R4_0 (next R4)	; 150		n 151
	vshufpd	zmm13, zmm13, zmm9, 11111111b	;; I4_7 I4_6 I4_5 I4_4 I4_3 I4_2 I4_1 I4_0 (next I4)	; 149		n 151

	vshufpd	zmm4, zmm2, zmm17, 00000000b	;; R7_7 R7_6 R7_5 R7_4 R7_3 R7_2 R7_1 R7_0 (next R7)	; 146		n 147
	vshufpd	zmm9, zmm12, zmm5, 00000000b	;; I7_7 I7_6 I7_5 I7_4 I7_3 I7_2 I7_1 I7_0 (next I7)	; 145		n 147

	vshufpd	zmm2, zmm2, zmm17, 11111111b	;; R8_7 R8_6 R8_5 R8_4 R8_3 R8_2 R8_1 R8_0 (next R8)	; 144		n 145
	vshufpd	zmm12, zmm12, zmm5, 11111111b	;; I8_7 I8_6 I8_5 I8_4 I8_3 I8_2 I8_1 I8_0 (next I8)	; 143		n 145

	vshufpd	zmm17, zmm0, zmm14, 11111111b	;; R6_7 R6_6 R6_5 R6_4 R6_3 R6_2 R6_1 R6_0 (next R6)	; 160		n 161
	vshufpd	zmm5, zmm10, zmm1, 11111111b	;; I6_7 I6_6 I6_5 I6_4 I6_3 I6_2 I6_1 I6_0 (next I6)	; 159		n 161

	vshufpd	zmm0, zmm0, zmm14, 00000000b	;; R5_7 R5_6 R5_5 R5_4 R5_3 R5_2 R5_1 R5_0 (next R5)	; 158		n 159
	vshufpd	zmm10, zmm10, zmm1, 00000000b	;; I5_7 I5_6 I5_5 I5_4 I5_3 I5_2 I5_1 I5_0 (next I5)	; 157		n 159

	vshufpd	zmm15, zmm15, zmm8, 00000000b	;; R1_7 R1_6 R1_5 R1_4 R1_3 R1_2 R1_1 R1_0 (next R1)	; 167		n 169
	vshufpd	zmm7, zmm7, zmm6, 00000000b	;; I1_7 I1_6 I1_5 I1_4 I1_3 I1_2 I1_1 I1_0 (next I1)	; 161		n 163

	vmovapd	zmm28, [screg2+0*128+64]	;; cosine/sine for R3/I3 (w^2 = complex w^1)
	zfmaddpd zmm1, zmm3, zmm28, zmm16	;; A3 = R3 * cosine/sine + I3				; 1-4		n 5
	zfmsubpd zmm16, zmm16, zmm28, zmm3	;; B3 = I3 * cosine/sine - R3				; 1-4		n 5

	vmovapd	zmm28, [screg1+0*128+64]	;; cosine/sine for R2/I2 (w^1)
	zfmaddpd zmm6, zmm18, zmm28, zmm19	;; A2 = R2 * cosine/sine + I2				; 2-5		n 7
	zfmsubpd zmm19, zmm19, zmm28, zmm18	;; B2 = I2 * cosine/sine - R2				; 2-5		n 7

	vmovapd	zmm28, [screg1+1*128+64]	;; cosine/sine for R4/I4 (w^3)
	zfmaddpd zmm8, zmm11, zmm28, zmm13	;; A4 = R4 * cosine/sine + I4				; 3-6		n 9
	zfmsubpd zmm13, zmm13, zmm28, zmm11	;; B4 = I4 * cosine/sine - R4				; 3-6		n 9

	vmovapd	zmm28, [screg2+2*128+64]	;; cosine/sine for R7/I7 (w^6 = complex w^3)
	zfmaddpd zmm14, zmm4, zmm28, zmm9	;; A7 = R7 * cosine/sine + I7				; 4-7		n 11
	zfmsubpd zmm9, zmm9, zmm28, zmm4	;; B7 = I7 * cosine/sine - R7				; 4-7		n 12

	vmovapd	zmm28, [screg2+0*128]		;; sine for R3/I3
	vmulpd	zmm1, zmm1, zmm28		;; R3 = A3 * sine					; 5-8		n 11
	vmulpd	zmm16, zmm16, zmm28		;; I3 = B3 * sine					; 5-8		n 12

	vmovapd	zmm28, [screg1+3*128+64]	;; cosine/sine for R8/I8 (w^7)
	zfmaddpd zmm3, zmm2, zmm28, zmm12	;; A8 = R8 * cosine/sine + I8				; 6-9		n 13
	zfmsubpd zmm12, zmm12, zmm28, zmm2	;; B8 = I8 * cosine/sine - R8				; 6-9		n 14

	vmovapd	zmm28, [screg1+0*128]		;; sine for R2/I2
	vmulpd	zmm6, zmm6, zmm28		;; R2 = A2 * sine					; 7-10		n 13
	vmulpd	zmm19, zmm19, zmm28		;; I2 = B2 * sine					; 7-10		n 14

	vmovapd	zmm28, [screg1+2*128+64]	;; cosine/sine for R6/I6 (w^5)
	zfmaddpd zmm18, zmm17, zmm28, zmm5	;; A6 = R6 * cosine/sine + I6				; 8-11		n 15
	zfmsubpd zmm5, zmm5, zmm28, zmm17	;; B6 = I6 * cosine/sine - R6				; 8-11		n 16

	vmovapd	zmm28, [screg1+1*128]		;; sine for R4/I4
	vmulpd	zmm8, zmm8, zmm28		;; R4 = A4 * sine					; 9-12		n 15
	vmulpd	zmm13, zmm13, zmm28		;; I4 = B4 * sine					; 9-12		n 16

	vmovapd	zmm28, [screg2+1*128+64]	;; cosine/sine for R5/I5 (w^4 = complex w^2)
	zfmaddpd zmm11, zmm0, zmm28, zmm10	;; A5 = R5 * cosine/sine + I5				; 10-13		n 17
	zfmsubpd zmm10, zmm10, zmm28, zmm0	;; B5 = I5 * cosine/sine - R5				; 10-13		n 18

	zfmaddpd zmm4, zmm7, zmm27, zmm15	;; R1+(.383,SQRTHALF,.383,1,.383,SQRTHALF,.383,1)*R9		; 88-89		n 
	zfnmaddpd zmm7, zmm7, zmm27, zmm15	;; R1-(.383,SQRTHALF,.383,1,.383,SQRTHALF,.383,1)*R9		; 88-89		n 

	vmovapd	zmm28, [screg2+2*128]		;; sine for R7/I7
	zfnmaddpd zmm20, zmm14, zmm28, zmm1	;; R3-(R7 = A7 * sine)					; 11-14		n 19
	zfmaddpd zmm14, zmm14, zmm28, zmm1	;; R3+(R7 = A7 * sine)					; 11-14		n 22

	zfmaddpd zmm2, zmm9, zmm28, zmm16	;; I3+(I7 = B7 * sine)					; 12-15		n 19
	zfnmaddpd zmm9, zmm9, zmm28, zmm16	;; I3-(I7 = B7 * sine)					; 12-15		n 23

	vmovapd	zmm28, [screg1+3*128]		;; sine for R8/I8
	zfmaddpd zmm17, zmm3, zmm28, zmm6	;; R2+(R8 = A8 * sine)					; 13-16		n 20
	zfnmaddpd zmm3, zmm3, zmm28, zmm6	;; R2-(R8 = A8 * sine)					; 13-16		n 24

	zfnmaddpd zmm0, zmm12, zmm28, zmm19	;; I2-(I8 = B8 * sine)					; 14-17		n 21
	zfmaddpd zmm12, zmm12, zmm28, zmm19	;; I2+(I8 = B8 * sine)					; 14-17		n 25

	vmovapd	zmm28, [screg1+2*128]		;; sine for R6/I6
	zfmaddpd zmm15, zmm18, zmm28, zmm8	;; R4+(R6 = A6 * sine)					; 15-18		n 20
	zfnmaddpd zmm18, zmm18, zmm28, zmm8	;; R4-(R6 = A6 * sine)					; 15-18		n 25

	zfnmaddpd zmm1, zmm5, zmm28, zmm13	;; I4-(I6 = B6 * sine)					; 16-19		n 21
	zfmaddpd zmm5, zmm5, zmm28, zmm13	;; I4+(I6 = B6 * sine)					; 16-19		n 24

	vmovapd	zmm28, [screg2+1*128]		;; sine for R5/I5
	zfmaddpd zmm16, zmm11, zmm28, zmm4	;; r1++ = (r1+r9) + r5*sine				; 17-20		n 22
	zfnmaddpd zmm11, zmm11, zmm28, zmm4	;; r1+- = (r1+r9) - r5*sine				; 17-20		n 23

	zfmaddpd zmm6, zmm10, zmm28, zmm7	;; r1-+ = (r1-r9) + i5*sine				; 18-21		n 29
	zfnmaddpd zmm10, zmm10, zmm28, zmm7	;; r1-- = (r1-r9) - i5*sine				; 18-21		n 31
	bump	screg1, scinc1

	vaddpd	zmm8, zmm20, zmm2		;; r3-+ = (r3-r7) + (i3+i7)				; 19-22		n 29
	vsubpd	zmm20, zmm20, zmm2		;; r3-- = (r3-r7) - (i3+i7)				; 19-22		n 31
	bump	screg2, scinc2

	vaddpd	zmm13, zmm17, zmm15		;; r2++ = (r2+r8) + (r4+r6)				; 20-23		n 26
	vsubpd	zmm17, zmm17, zmm15		;; r2+- = (r2+r8) - (r4+r6)				; 20-23		n 28

	vsubpd	zmm7, zmm0, zmm1		;; i2-- = (i2-i8) - (i4-i6)				; 21-24		n 27
	vaddpd	zmm0, zmm0, zmm1		;; i2-+ = (i2-i8) + (i4-i6)				; 21-24		n 28

	vaddpd	zmm2, zmm16, zmm14		;; r1+++ = (r1++) + (r3+r7)				; 22-25		n 26
	vsubpd	zmm16, zmm16, zmm14		;; r1++- = (r1++) - (r3+r7)				; 22-25		n 27

	vaddpd	zmm15, zmm11, zmm9		;; r1+-+ = (r1+-) + (i3-i7)				; 23-26		n 33
	vsubpd	zmm11, zmm11, zmm9		;; r1+-- = (r1+-) - (i3-i7)				; 23-26		n 34

	vaddpd	zmm1, zmm3, zmm5		;; r2-+ = (r2-r8) + (i4+i6)				; 24-27		n 30
	vsubpd	zmm3, zmm3, zmm5		;; r2-- = (r2-r8) - (i4+i6)				; 24-27		n 32

	vaddpd	zmm14, zmm12, zmm18		;; i2++ = (i2+i8) + (r4-r6)				; 25-28		n 30
	vsubpd	zmm12, zmm12, zmm18		;; i2+- = (i2+i8) - (r4-r6)				; 25-28		n 32

	vaddpd	zmm9, zmm2, zmm13		;; R1 = (r1+++) + (r2++)				; 26-29
	vsubpd	zmm2, zmm2, zmm13		;; R9 = (r1+++) - (r2++)				; 26-29

	vaddpd	zmm5, zmm16, zmm7		;; R5  = (r1++-) + (i2--)				; 27-30
	vsubpd	zmm16, zmm16, zmm7		;; R13 = (r1++-) - (i2--)				; 27-30

	vaddpd	zmm13, zmm17, zmm0		;; r2+-+ = (r2+-) + (i2-+)				; 28-31		n 33
	vsubpd	zmm17, zmm17, zmm0		;; r2+-- = (r2+-) - (i2-+)				; 28-31		n 34

	zfmaddpd zmm7, zmm8, zmm31, zmm6	;; r2_10o = (r1-+) + .707(r3-+)				; 29-32		n 35
	zfnmaddpd zmm8, zmm8, zmm31, zmm6	;; r6_14o = (r1-+) - .707(r3-+)				; 29-32		n 36

	zfmaddpd zmm0, zmm1, zmm30, zmm14	;; r2_10e/.383 = .924/.383(r2-+) + (i2++)		; 30-33		n 35
	zfmsubpd zmm14, zmm14, zmm30, zmm1	;; r6_14e/.383 = .924/.383(i2++) - (r2-+)		; 30-33		n 36
	zstore	[srcreg], zmm9			;; Save R1						; 30

	zfnmaddpd zmm6, zmm20, zmm31, zmm10	;; r4_12o = (r1--) - .707(r3--)				; 31-34		n 37
	zfmaddpd zmm20, zmm20, zmm31, zmm10	;; r8_16o = (r1--) + .707(r3--)				; 31-34		n 38
	zstore	[srcreg+64], zmm2		;; Save R9						; 30+1

	zfmaddpd zmm1, zmm12, zmm30, zmm3	;; r4_12e/.383 = .924/.383(i2+-) + (r2--)		; 32-35		n 37
	zfnmaddpd zmm3, zmm3, zmm30, zmm12	;; r8_16e/.383 = (i2+-) - .924/.383(r2--)		; 32-35		n 38
	zstore	[srcreg+d4], zmm5		;; Save R5						; 31+1

	zfmaddpd zmm10, zmm13, zmm31, zmm15	;; R3  = (r1+-+) + .707(r2+-+)				; 33-36
	zfnmaddpd zmm13, zmm13, zmm31, zmm15	;; R11 = (r1+-+) - .707(r2+-+)				; 33-36
	zstore	[srcreg+d4+64], zmm16		;; Save R13						; 31+2

	zfnmaddpd zmm12, zmm17, zmm31, zmm11	;; R7  = (r1+--) - .707(r2+--)				; 34-37
	zfmaddpd zmm17, zmm17, zmm31, zmm11	;; R15 = (r1+--) + .707(r2+--)				; 34-37

	zfmaddpd zmm15, zmm0, zmm29, zmm7	;; R2  = r2_10o + .383*r2_10e				; 35-38
	zfnmaddpd zmm0, zmm0, zmm29, zmm7	;; R10 = r2_10o - .383*r2_10e				; 35-38

	zfmaddpd zmm11, zmm14, zmm29, zmm8	;; R6  = r6_14o + .383*r6_14e				; 36-39
	zfnmaddpd zmm14, zmm14, zmm29, zmm8	;; R14 = r6_14o - .383*r6_14e				; 36-39

	zfmaddpd zmm7, zmm1, zmm29, zmm6	;; R4  = r4_12o + .383*r4_12e				; 37-40
	zfnmaddpd zmm1, zmm1, zmm29, zmm6	;; R12 = r4_12o - .383*r4_12e				; 37-40
	zstore	[srcreg+d2], zmm10		;; Save R3						; 37

	zfmaddpd zmm8, zmm3, zmm29, zmm20	;; R8  = r8_16o + .383*r8_16e				; 38-41
	zfnmaddpd zmm3, zmm3, zmm29, zmm20	;; R16 = r8_16o - .383*r8_16e				; 38-41
	zstore	[srcreg+d2+64], zmm13		;; Save R11						; 37+1

	zstore	[srcreg+d4+d2], zmm12		;; Save R7						; 38+1
	zstore	[srcreg+d4+d2+64], zmm17	;; Save R15						; 38+2
	zstore	[srcreg+d1], zmm15		;; Save R2						; 39+2
	zstore	[srcreg+d1+64], zmm0		;; Save R10						; 39+3
	zstore	[srcreg+d4+d1], zmm11		;; Save R6						; 40+3
	zstore	[srcreg+d4+d1+64], zmm14	;; Save R14						; 40+4
	zstore	[srcreg+d2+d1], zmm7		;; Save R4						; 41+4
	zstore	[srcreg+d2+d1+64], zmm1		;; Save R12						; 41+5
	zstore	[srcreg+d4+d2+d1], zmm8		;; Save R8						; 42+5
	zstore	[srcreg+d4+d2+d1+64], zmm3	;; Save R16						; 42+6
	bump	srcreg, srcinc
	ENDM
