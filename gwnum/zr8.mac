; Copyright 2017-2019 - Mersenne Research, Inc.  All rights reserved
; Author:  George Woltman
; Email: woltman@alum.mit.edu
;

;;
;; Macros for an AVX-512 radix-8 step in an FFT.  This is used in our radix-4/8 FFTs
;; whenever possible.
;;


;;
;; ************************************* eight-complex-fft variants ******************************************
;;

;; This code does an eight complex forward FFT and then applies seven sin/cos multipliers.

; Basic eight-complex DJB FFT building block
zr8_eight_complex_djbfft_preload MACRO
	zr8_8c_djbfft_cmn_preload
	ENDM
zr8_eight_complex_djbfft MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	zr8_8c_djbfft_cmn srcreg,0,srcinc,d1,d2,d4,noexec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like the basic version except source data is offset by [rbx]
zr8f_eight_complex_djbfft_preload MACRO
	zr8_8c_djbfft_cmn_preload
	ENDM
zr8f_eight_complex_djbfft MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	zr8_8c_djbfft_cmn srcreg,rbx,srcinc,d1,d2,d4,noexec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like the basic version except vbroadcastsd is used to reduce sin/cos data
zr8b_eight_complex_djbfft_preload MACRO
	zr8_8c_djbfft_cmn_preload
	ENDM
zr8b_eight_complex_djbfft MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	zr8_8c_djbfft_cmn srcreg,0,srcinc,d1,d2,d4,exec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

;; Common macro to operate on 8 complex values doing 3 levels of the FFT, applying
;; the sin/cos multipliers afterwards.

;; To calculate a 8-complex FFT in a brute force way (using a shorthand notation):
;; c1 + c2 + ... + c8 * w^00000000
;; c1 + c2 + ... + c8 * w^01234567
;; c1 + c2 + ... + c8 * w^02468ACE
;; c1 + c2 + ... + c8 * w^0369C...
;; c1 + c2 + ... + c8 * w^048C....
;; c1 + c2 + ... + c8 * w^05A.....
;; c1 + c2 + ... + c8 * w^06C.....
;; c1 + c2 + ... + c8 * w^07E.....
;;
;; The sin/cos values (w = 8th root of unity) are:
;; w^1 = .707 + .707i
;; w^2 = 0 + 1i
;; w^3 = -.707 + .707i
;; w^4 = -1
;;
;; Applying the sin/cos values above:
;; reals:
;; r1     +r2 +r3     +r4 +r5     +r6 +r7     +r8
;; r1 +.707r2     -.707r4 -r5 -.707r6     +.707r8  -.707i2 -i3 -.707i4 +.707i6 +i7 +.707i8
;; r1         -r3         +r5         -r7              -i2         +i4     -i6         +i8
;; r1 -.707r2     +.707r4 -r5 +.707r6     -.707r8  -.707i2 +i3 -.707i4 +.707i6 -i7 +.707i8
;; r1     -r2 +r3     -r4 +r5     -r6 +r7     -r8
;; r1 -.707r2     +.707r4 -r5 +.707r6     -.707r8  +.707i2 -i3 +.707i4 -.707i6 +i7 -.707i8
;; r1         -r3         +r5         -r7              +i2         -i4     +i6         -i8
;; r1 +.707r2     -.707r4 -r5 -.707r6     +.707r8  +.707i2 +i3 +.707i4 -.707i6 -i7 -.707i8
;; imaginarys:
;;                                         +i1     +i2 +i3     +i4 +i5     +i6 +i7     +i8
;; +.707r2 +r3 +.707r4 -.707r6 -r7 -.707r8 +i1 +.707i2     -.707i4 -i5 -.707i6     +.707i8
;;     +r2         -r4     +r6         -r8 +i1         -i3         +i5         -i7
;; +.707r2 -r3 +.707r4 -.707r6 +r7 -.707r8 +i1 -.707i2     +.707i4 -i5 +.707i6     -.707i8
;;                                         +i1     -i2 +i3     -i4 +i5     -i6 +i7     -i8
;; -.707r2 +r3 -.707r4 +.707r6 -r7 +.707r8 +i1 -.707i2     +.707i4 -i5 +.707i6     -.707i8
;;     -r2         +r4     -r6         +r8 +i1         -i3         +i5         -i7
;; -.707r2 -r3 -.707r4 +.707r6 +r7 +.707r8 +i1 +.707i2     -.707i4 -i5 -.707i6     +.707i8
;;
;; Simplifying, we get:
;;R1 = ((r1+r5)+(r3+r7))      +((r2+r6)+(r4+r8))
;;R5 = ((r1+r5)+(r3+r7))      -((r2+r6)+(r4+r8))
;;R3 = ((r1+r5)-(r3+r7))                              -((i2+i6)-(i4+i8))
;;R7 = ((r1+r5)-(r3+r7))                              +((i2+i6)-(i4+i8))
;;R2 = ((r1-r5)+.707((r2-r6)-(r4-r8)))  - ((i3-i7)+.707((i2-i6)+(i4-i8)))
;;R8 = ((r1-r5)+.707((r2-r6)-(r4-r8)))  + ((i3-i7)+.707((i2-i6)+(i4-i8)))
;;R4 = ((r1-r5)-.707((r2-r6)-(r4-r8)))  + ((i3-i7)-.707((i2-i6)+(i4-i8))) 
;;R6 = ((r1-r5)-.707((r2-r6)-(r4-r8)))  - ((i3-i7)-.707((i2-i6)+(i4-i8))) 

;;I1 = ((i1+i5)+(i3+i7))      +((i2+i6)+(i4+i8))
;;I5 = ((i1+i5)+(i3+i7))      -((i2+i6)+(i4+i8))
;;I3 = ((i1+i5)-(i3+i7))                              +((r2+r6)-(r4+r8))                        
;;I7 = ((i1+i5)-(i3+i7))                              -((r2+r6)-(r4+r8))                        
;;I2 = ((i1-i5)+.707((i2-i6)-(i4-i8)))  + ((r3-r7)+.707((r2-r6)+(r4-r8)))
;;I8 = ((i1-i5)+.707((i2-i6)-(i4-i8)))  - ((r3-r7)+.707((r2-r6)+(r4-r8)))
;;I4 = ((i1-i5)-.707((i2-i6)-(i4-i8)))  - ((r3-r7)-.707((r2-r6)+(r4-r8)))
;;I6 = ((i1-i5)-.707((i2-i6)-(i4-i8)))  + ((r3-r7)-.707((r2-r6)+(r4-r8)))

zr8_8c_djbfft_cmn_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRTHALF
	ENDM
zr8_8c_djbfft_cmn MACRO srcreg,srcoff,srcinc,d1,d2,d4,bcast,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	zmm1, [srcreg+srcoff+d1]		;; R2
	vmovapd	zmm2, [srcreg+srcoff+d4+d1]		;; R6
	vsubpd	zmm0, zmm1, zmm2			;; R2-R6				; 1-4		n 9
	vaddpd	zmm1, zmm1, zmm2			;; R2+R6				; 1-4		n 11

	vmovapd	zmm3, [srcreg+srcoff+d2+d1]		;; R4
	vmovapd	zmm4, [srcreg+srcoff+d4+d2+d1]		;; R8
	vsubpd	zmm2, zmm3, zmm4			;; R4-R8				; 2-5		n 9
	vaddpd	zmm3, zmm3, zmm4			;; R4+R8				; 2-5		n 11

	vmovapd	zmm5, [srcreg+srcoff+d2+d1+64]		;; I4
	vmovapd	zmm6, [srcreg+srcoff+d4+d2+d1+64]	;; I8
	vsubpd	zmm4, zmm5, zmm6			;; I4-I8				; 3-6		n 10
	vaddpd	zmm5, zmm5, zmm6			;; I4+I8				; 3-6		n 12

	vmovapd	zmm7, [srcreg+srcoff+d1+64]		;; I2
	vmovapd	zmm8, [srcreg+srcoff+d4+d1+64]		;; I6
	vsubpd	zmm6, zmm7, zmm8			;; I2-I6				; 4-7		n 10
	vaddpd	zmm7, zmm7, zmm8			;; I2+I6				; 4-7		n 12

	vmovapd	zmm9, [srcreg+srcoff]			;; R1
	vmovapd	zmm10, [srcreg+srcoff+d4]		;; R5
	vaddpd	zmm8, zmm9, zmm10			;; R1+R5				; 5-8		n 13
	vsubpd	zmm9, zmm9, zmm10			;; R1-R5				; 5-8		n 14

	vmovapd	zmm11, [srcreg+srcoff+d2]		;; R3
	vmovapd	zmm12, [srcreg+srcoff+d4+d2]		;; R7
	vaddpd	zmm10, zmm11, zmm12			;; R3+R7				; 6-9		n 13
	vsubpd	zmm11, zmm11, zmm12			;; R3-R7				; 6-9		n 17

	vmovapd	zmm13, [srcreg+srcoff+64]		;; I1
	vmovapd	zmm14, [srcreg+srcoff+d4+64]		;; I5
	vaddpd	zmm12, zmm13, zmm14			;; I1+I5				; 7-10		n 16
	vsubpd	zmm13, zmm13, zmm14			;; I1-I5				; 7-10		n 15

	vmovapd	zmm15, [srcreg+srcoff+d2+64]		;; I3
	vmovapd	zmm16, [srcreg+srcoff+d4+d2+64]		;; I7
	vaddpd	zmm14, zmm15, zmm16			;; I3+I7				; 8-11		n 16
	vsubpd	zmm15, zmm15, zmm16			;; I3-I7				; 8-11		n 18

	vaddpd	zmm16, zmm0, zmm2		;; r2-+ = (r2-r6) + (r4-r8)			; 9-12		n 17
	vsubpd	zmm0, zmm0, zmm2		;; r2-- = (r2-r6) - (r4-r8)			; 9-12		n 14
bcast	vbroadcastsd zmm30, [screg+1*16]	;; sine for R3/I3 and R7/I7 (w^2)
no bcast vmovapd zmm30, [screg+1*128]		;; sine for R3/I3 and R7/I7

	vsubpd	zmm2, zmm6, zmm4		;; i2-- = (i2-i6) - (i4-i8)			; 10-13		n 15
	vaddpd	zmm6, zmm6, zmm4		;; i2-+ = (i2-i6) + (i4-i8)			; 10-13		n 18
bcast	vbroadcastsd zmm23, [screg+0*16]	;; sine for R2/I2 and R8/I8 (w^1)
no bcast vmovapd zmm23, [screg+0*128]		;; sine for R2/I2 and R8/I8

	vsubpd	zmm4, zmm1, zmm3		;; r2+- = (r2+r6) - (r4+r8)			; 11-14		n 19
	vaddpd	zmm1, zmm1, zmm3		;; r2++ = (r2+r6) + (r4+r8)			; 11-14		n 22
bcast	vbroadcastsd zmm24, [screg+2*16]	;; sine for R4/I4 and R6/I6 (w^3)
no bcast vmovapd zmm24, [screg+2*128]		;; sine for R4/I4 and R6/I6

	vsubpd	zmm3, zmm7, zmm5		;; i2+- = (i2+i6) - (i4+i8)			; 12-15		n 19
	vaddpd	zmm7, zmm7, zmm5		;; i2++ = (i2+i6) + (i4+i8)			; 12-15		n 23
bcast	vbroadcastsd zmm26, [screg+1*16+8]	;; cosine/sine for R3/I3 and R7/I7 (w^2)
no bcast vmovapd zmm26, [screg+1*128+64]	;; cosine/sine for R3/I3 and R7/I7

	vaddpd	zmm5, zmm8, zmm10		;; r1++ = (r1+r5) + (r3+r7)			; 13-16		n 22
	vsubpd	zmm8, zmm8, zmm10		;; r1+- = (r1+r5) - (r3+r7)			; 13-16		n 24
bcast	vbroadcastsd zmm29, [screg+3*16+8]	;; cosine/sine for R5/I5 (w^4)
no bcast vmovapd zmm29, [screg+3*128+64]	;; cosine/sine for R5/I5

	zfmaddpd zmm10, zmm0, zmm31, zmm9	;; r1-+ = (r1-r5) + .707*(r2--)			; 14-17		n 20
	zfnmaddpd zmm0, zmm0, zmm31, zmm9	;; r1-- = (r1-r5) - .707*(r2--)			; 14-17		n 21
bcast	vbroadcastsd zmm27, [screg+0*16+8]	;; cosine/sine for R2/I2 and R8/I8 (w^1)
no bcast vmovapd zmm27, [screg+0*128+64]	;; cosine/sine for R2/I2 and R8/I8

	zfmaddpd zmm9, zmm2, zmm31, zmm13	;; i1-+ = (i1-i5) + .707*(i2--)			; 15-18		n 20
	zfnmaddpd zmm2, zmm2, zmm31, zmm13	;; i1-- = (i1-i5) - .707*(i2--)			; 15-18		n 21
bcast	vbroadcastsd zmm28, [screg+2*16+8]	;; cosine/sine for R4/I4 and R6/I6 (w^3)
no bcast vmovapd zmm28, [screg+2*128+64]	;; cosine/sine for R4/I4 and R6/I6

	vaddpd	zmm13, zmm12, zmm14		;; i1++ = (i1+i5) + (i3+i7)			; 16-19		n 23
	vsubpd	zmm12, zmm12, zmm14		;; i1+- = (i1+i5) - (i3+i7)			; 16-19		n 24
bcast	vbroadcastsd zmm25, [screg+3*16]	;; sine for R5/I5 (w^4)
no bcast vmovapd zmm25, [screg+3*128]		;; sine for R5/I5

	zfmaddpd zmm14, zmm16, zmm31, zmm11	;; r3-+ = (r3-r7) + .707*(r2-+)			; 17-20		n 26
	zfnmaddpd zmm16, zmm16, zmm31, zmm11	;; r3-- = (r3-r7) - .707*(r2-+)			; 17-20		n 30
	bump	screg, scinc
	L1prefetchw srcreg+L1pd, L1pt

	zfmaddpd zmm11, zmm6, zmm31, zmm15	;; i3-+ = (i3-i7) + .707*(i2-+)			; 18-21		n 26
	zfnmaddpd zmm6, zmm6, zmm31, zmm15	;; i3-- = (i3-i7) - .707*(i2-+)			; 18-21		n 30
	L1prefetchw srcreg+64+L1pd, L1pt

	vmulpd	zmm4, zmm4, zmm30		;; r2+-s = r2+- * sine37			; 19-22		n 24
	vmulpd	zmm3, zmm3, zmm30		;; i2+-s = i2+- * sine37			; 19-22		n 24
	L1prefetchw srcreg+d1+L1pd, L1pt

	vmulpd	zmm10, zmm10, zmm23		;; r1-+s = r1-+ * sine28			; 20-23		n 26
	vmulpd	zmm9, zmm9, zmm23		;; i1-+s = i1-+ * sine28			; 20-23		n 26
	L1prefetchw srcreg+d1+64+L1pd, L1pt

	vmulpd	zmm0, zmm0, zmm24		;; r1--s = r1-- * sine46			; 21-24		n 30
	vmulpd	zmm2, zmm2, zmm24		;; i1--s = i1-- * sine46			; 21-24		n 30
	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	zmm15, zmm5, zmm1		;; R5 = (r1++) - (r2++)				; 22-25		n 32
	vaddpd	zmm5, zmm5, zmm1		;; R1 = (r1++) + (r2++)				; 22-25
	L1prefetchw srcreg+d2+64+L1pd, L1pt

	vsubpd	zmm1, zmm13, zmm7		;; I5 = (i1++) - (i2++)				; 23-26		n 32
	vaddpd	zmm13, zmm13, zmm7		;; I1 = (i1++) + (i2++)				; 23-26
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	zfmsubpd zmm7, zmm8, zmm30, zmm3	;; R3s = (r1+-)*sine37 - (i2+-s)		; 24-27		n 28
	zfmaddpd zmm17, zmm12, zmm30, zmm4	;; I3s = (i1+-)*sine37 + (r2+-s)		; 24-27		n 28
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt

	zfmaddpd zmm8, zmm8, zmm30, zmm3	;; R7s = (r1+-)*sine37 + (i2+-s)		; 25-28		n 32
	zfmsubpd zmm12, zmm12, zmm30, zmm4	;; I7s = (i1+-)*sine37 - (r2+-s)		; 25-28		n 32
	L1prefetchw srcreg+d4+L1pd, L1pt

	zfnmaddpd zmm3, zmm11, zmm23, zmm10	;; R2s = (r1-+s) - sine28*(i3-+)		; 26-29		n 33
	zfmaddpd zmm4, zmm14, zmm23, zmm9	;; I2s = (i1-+s) + sine28*(r3-+)		; 26-29		n 33
	zstore	[srcreg], zmm5			;; Store R1					; 26
	L1prefetchw srcreg+d4+64+L1pd, L1pt

	zfmaddpd zmm11, zmm11, zmm23, zmm10	;; R8s = (r1-+s) + sine28*(i3-+)		; 27-30		n 34
	zfnmaddpd zmm14, zmm14, zmm23, zmm9	;; I8s = (i1-+s) - sine28*(r3-+)		; 27-30		n 34
	zstore	[srcreg+64], zmm13		;; Store I1					; 27
	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	zfmsubpd zmm10, zmm7, zmm26, zmm17	;; R3s * cosine/sine - I3s (final R3)		; 28-31
	zfmaddpd zmm17, zmm17, zmm26, zmm7	;; I3s * cosine/sine + R3s (final I3)		; 28-31
	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt

	zfmaddpd zmm9, zmm8, zmm26, zmm12	;; R7s * cosine/sine + I7s (final R7)		; 29-32
	zfmsubpd zmm12, zmm12, zmm26, zmm8	;; I7s * cosine/sine - R7s (final I7)		; 29-32
	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	zfmaddpd zmm7, zmm6, zmm24, zmm0	;; R4s = (r1--s) + sine46*(i3--)		; 30-33		n 35
	zfnmaddpd zmm8, zmm16, zmm24, zmm2	;; I4s = (i1--s) - sine46*(r3--)		; 30-33		n 35
	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt

	zfnmaddpd zmm6, zmm6, zmm24, zmm0	;; R6s = (r1--s) - sine46*(i3--)		; 31-34		n 36
	zfmaddpd zmm16, zmm16, zmm24, zmm2	;; I6s = (i1--s) + sine46*(r3--)		; 31-34		n 36
	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	zfmsubpd zmm0, zmm15, zmm29, zmm1	;; A5 = R5 * cosine/sine - I5			; 32-35		n 37
	zfmaddpd zmm1, zmm1, zmm29, zmm15	;; B5 = I5 * cosine/sine + R5			; 32-35		n 37
	zstore	[srcreg+d2], zmm10		;; Store R3					; 32
	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt

	zfmsubpd zmm2, zmm3, zmm27, zmm4	;; R2s * cosine/sine - I2s (final R2)		; 33-36
	zfmaddpd zmm4, zmm4, zmm27, zmm3	;; I2s * cosine/sine + R2s (final I2)		; 33-36
	zstore	[srcreg+d2+64], zmm17		;; Store I3					; 32+1

	zfmaddpd zmm15, zmm11, zmm27, zmm14	;; R8s * cosine/sine + I8s (final R8)		; 34-37
	zfmsubpd zmm14, zmm14, zmm27, zmm11	;; I8s * cosine/sine - R8s (final I8)		; 34-37
	zstore	[srcreg+d4+d2], zmm9		;; Store R7					; 33+1

	zfmsubpd zmm3, zmm7, zmm28, zmm8	;; R4s * cosine/sine - I4s (final R4)		; 35-38
	zfmaddpd zmm8, zmm8, zmm28, zmm7	;; I4s * cosine/sine + R4s (final I4)		; 35-38
	zstore	[srcreg+d4+d2+64], zmm12	;; Store I7					; 33+2

	zfmaddpd zmm11, zmm6, zmm28, zmm16	;; R6s * cosine/sine + I6s (final R6)		; 36-39
	zfmsubpd zmm16, zmm16, zmm28, zmm6	;; I6s * cosine/sine - R6s (final I6)		; 36-39

	vmulpd	zmm0, zmm0, zmm25		;; A5 = A5 * sine (final R5)			; 37-40
	vmulpd	zmm1, zmm1, zmm25		;; B5 = B5 * sine (final I5)			; 37-40

	zstore	[srcreg+d1], zmm2		;; Store R2					; 37
	zstore	[srcreg+d1+64], zmm4		;; Store I2					; 37+1
	zstore	[srcreg+d4+d2+d1], zmm15	;; Store R8					; 38+1
	zstore	[srcreg+d4+d2+d1+64], zmm14	;; Store I8					; 38+2
	zstore	[srcreg+d2+d1], zmm3		;; Store R4					; 39+2
	zstore	[srcreg+d2+d1+64], zmm8		;; Store I4					; 39+3
	zstore	[srcreg+d4+d1], zmm11		;; Store R6					; 40+3
	zstore	[srcreg+d4+d1+64], zmm16	;; Store I6					; 40+4
	zstore	[srcreg+d4], zmm0		;; Store R5					; 41+4
	zstore	[srcreg+d4+64], zmm1		;; Store I5					; 41+5
	bump	srcreg, srcinc
	ENDM


;; ************************************* eight-complex-djbunfft variants ******************************************

;; This code applies the 7 sin/cos postmultipliers before a radix-8 butterfly.

; Basic eight-complex DJB inverse FFT building block
zr8_eight_complex_djbunfft_preload MACRO
	zr8_8c_djbunfft_cmn_preload
	ENDM
zr8_eight_complex_djbunfft MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	zr8_8c_djbunfft_cmn srcreg,srcinc,d1,d2,d4,noexec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like the basic version except vbroadcastsd is used to reduce sin/cos data
zr8b_eight_complex_djbunfft_preload MACRO
	zr8_8c_djbunfft_cmn_preload
	ENDM
zr8b_eight_complex_djbunfft MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	zr8_8c_djbunfft_cmn srcreg,srcinc,d1,d2,d4,exec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

;; Common macro to operate on 8 complex values doing 3 levels of the inverse FFT, applying
;; the sin/cos multipliers beforehand.

;; To calculate a 8-complex inverse FFT in a brute force way (using a shorthand notation):
;; c1 + c2 + ... + c8 * w^-00000000
;; c1 + c2 + ... + c8 * w^-01234567
;; c1 + c2 + ... + c8 * w^-02468ACE
;; c1 + c2 + ... + c8 * w^-0369C...
;; c1 + c2 + ... + c8 * w^-048C....
;; c1 + c2 + ... + c8 * w^-05A.....
;; c1 + c2 + ... + c8 * w^-06C.....
;; c1 + c2 + ... + c8 * w^-07E.....
;;
;; The sin/cos values (w = 8th root of unity) are:
;; w^-1 = .707 - .707i
;; w^-2 = 0 - 1i
;; w^-3 = -.707 - .707i
;; w^-4 = -1
;;
;; Applying the sin/cos values above, you get the same results as the forward FFT except that
;; for the real results we flip the sign of the imaginary inputs and for the imaginary results
;; we flip the sign of the real inputs.
;; reals:
;; r1     +r2 +r3     +r4 +r5     +r6 +r7     +r8
;; r1 +.707r2     -.707r4 -r5 -.707r6     +.707r8  +.707i2 +i3 +.707i4 -.707i6 -i7 -.707i8
;; r1         -r3         +r5         -r7              +i2         -i4     +i6         -i8
;; r1 -.707r2     +.707r4 -r5 +.707r6     -.707r8  +.707i2 -i3 +.707i4 -.707i6 +i7 -.707i8
;; r1     -r2 +r3     -r4 +r5     -r6 +r7     -r8
;; r1 -.707r2     +.707r4 -r5 +.707r6     -.707r8  -.707i2 +i3 -.707i4 +.707i6 -i7 +.707i8
;; r1         -r3         +r5         -r7              -i2         +i4     -i6         +i8
;; r1 +.707r2     -.707r4 -r5 -.707r6     +.707r8  -.707i2 -i3 -.707i4 +.707i6 +i7 +.707i8
;; imaginarys:
;;                                         +i1     +i2 +i3     +i4 +i5     +i6 +i7     +i8
;; -.707r2 -r3 -.707r4 +.707r6 +r7 +.707r8 +i1 +.707i2     -.707i4 -i5 -.707i6     +.707i8
;;     -r2         +r4     -r6         +r8 +i1         -i3         +i5         -i7
;; -.707r2 +r3 -.707r4 +.707r6 -r7 +.707r8 +i1 -.707i2     +.707i4 -i5 +.707i6     -.707i8
;;                                         +i1     -i2 +i3     -i4 +i5     -i6 +i7     -i8
;; +.707r2 -r3 +.707r4 -.707r6 +r7 -.707r8 +i1 -.707i2     +.707i4 -i5 +.707i6     -.707i8
;;     +r2         -r4     +r6         -r8 +i1         -i3         +i5         -i7
;; +.707r2 +r3 +.707r4 -.707r6 -r7 -.707r8 +i1 +.707i2     -.707i4 -i5 -.707i6     +.707i8
;;

;; Rearranging for maximum FMA usage:
;;R1 = ((r1+r5)+(r3+r7))      +((r2+r8)+(r4+r6))
;;R5 = ((r1+r5)+(r3+r7))      -((r2+r8)+(r4+r6))
;;R3 = ((r1+r5)-(r3+r7))                        +((i2-i8)-(i4-i6))
;;R7 = ((r1+r5)-(r3+r7))                        -((i2-i8)-(i4-i6))
;;R2 = ((r1-r5)+(i3-i7)) +.707(((r2+r8)-(r4+r6))+((i2-i8)+(i4-i6)))
;;R6 = ((r1-r5)+(i3-i7)) -.707(((r2+r8)-(r4+r6))+((i2-i8)+(i4-i6)))
;;R4 = ((r1-r5)-(i3-i7)) -.707(((r2+r8)-(r4+r6))-((i2-i8)+(i4-i6)))
;;R8 = ((r1-r5)-(i3-i7)) +.707(((r2+r8)-(r4+r6))-((i2-i8)+(i4-i6)))

;;I1 = ((i1+i5)+(i3+i7))                        +((i2+i8)+(i4+i6))
;;I5 = ((i1+i5)+(i3+i7))                        -((i2+i8)+(i4+i6))
;;I3 = ((i1+i5)-(i3+i7))      -((r2-r8)-(r4-r6))                        
;;I7 = ((i1+i5)-(i3+i7))      +((r2-r8)-(r4-r6))                        
;;I2 = ((i1-i5)-(r3-r7)) -.707(((r2-r8)+(r4-r6))-((i2+i8)-(i4+i6)))
;;I6 = ((i1-i5)-(r3-r7)) +.707(((r2-r8)+(r4-r6))-((i2+i8)-(i4+i6)))
;;I4 = ((i1-i5)+(r3-r7)) -.707(((r2-r8)+(r4-r6))+((i2+i8)-(i4+i6)))
;;I8 = ((i1-i5)+(r3-r7)) +.707(((r2-r8)+(r4-r6))+((i2+i8)-(i4+i6)))

zr8_8c_djbunfft_cmn_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRTHALF
	ENDM
zr8_8c_djbunfft_cmn MACRO srcreg,srcinc,d1,d2,d4,bcast,screg,scinc,maxrpt,L1pt,L1pd
bcast	vbroadcastsd zmm30, [screg+0*16+8]	;; cosine/sine for R2/I2 and R8/I8 (w^1)
no bcast vmovapd zmm30, [screg+0*128+64]	;; cosine/sine for R2/I2 and R8/I8
	vmovapd zmm4, [srcreg+d1]		;; Load R2
	vmovapd zmm12, [srcreg+d1+64]		;; Load I2
	zfmaddpd zmm16, zmm4, zmm30, zmm12	;; A2 = R2 * cosine/sine + I2			; 1-4		n 5
	zfmsubpd zmm12, zmm12, zmm30, zmm4	;; B2 = I2 * cosine/sine - R2			; 1-4		n 5

	vmovapd zmm7, [srcreg+d4+d2+d1]		;; Load R8
	vmovapd zmm15, [srcreg+d4+d2+d1+64]	;; Load I8
	zfmsubpd zmm4, zmm7, zmm30, zmm15	;; A8 = R8 * cosine/sine - I8			; 2-5		n 11
	zfmaddpd zmm15, zmm15, zmm30, zmm7	;; B8 = I8 * cosine/sine + R8			; 2-5		n 12

bcast	vbroadcastsd zmm30, [screg+2*16+8]	;; cosine/sine for R4/I4 and R6/I6 (w^3)
no bcast vmovapd zmm30, [screg+2*128+64]	;; cosine/sine for R4/I4 and R6/I6
	vmovapd zmm6, [srcreg+d2+d1]		;; Load R4
	vmovapd zmm14, [srcreg+d2+d1+64]	;; Load I4
	zfmaddpd zmm7, zmm6, zmm30, zmm14	;; A4 = R4 * cosine/sine + I4 (first R4/sine)	; 3-6		n 9
	zfmsubpd zmm14, zmm14, zmm30, zmm6	;; B4 = I4 * cosine/sine - R4 (first I4/sine)	; 3-6		n 10

	vmovapd zmm5, [srcreg+d4+d1]		;; Load R6
	vmovapd zmm13, [srcreg+d4+d1+64]	;; Load I6
	zfmsubpd zmm6, zmm5, zmm30, zmm13	;; A6 = R6 * cosine/sine - I6 (first R6/sine)	; 4-7		n 9
	zfmaddpd zmm13, zmm13, zmm30, zmm5	;; B6 = I6 * cosine/sine + R6 (first I6/sine)	; 4-7		n 10

bcast	vbroadcastsd zmm29, [screg+0*16]	;; sine for R2/I2 and R8/I8 (w^1)
no bcast vmovapd zmm29, [screg+0*128]		;; sine for R2/I2 and R8/I8
	vmulpd	zmm16, zmm16, zmm29		;; A2 = A2 * sine (first R2)			; 5-8		n 11
	vmulpd	zmm12, zmm12, zmm29		;; B2 = B2 * sine (first I2)			; 5-8		n 12

bcast	vbroadcastsd zmm30, [screg+3*16+8]	;; cosine/sine for R5/I5 (w^4)
no bcast vmovapd zmm30, [screg+3*128+64]	;; cosine/sine for R5/I5
	vmovapd zmm1, [srcreg+d4]		;; Load R5
	vmovapd zmm9, [srcreg+d4+64]		;; Load I5
	zfmaddpd zmm5, zmm1, zmm30, zmm9	;; A5 = R5 * cosine/sine + I5 (first R5/sine)	; 6-9		n 13
	zfmsubpd zmm9, zmm9, zmm30, zmm1	;; B5 = I5 * cosine/sine - R5 (first I5/sine)	; 6-9		n 14

bcast	vbroadcastsd zmm30, [screg+1*16+8]	;; cosine/sine for R3/I3 and R7/I7 (w^2)
no bcast vmovapd zmm30, [screg+1*128+64]	;; cosine/sine for R3/I3 and R7/I7
	vmovapd zmm2, [srcreg+d2]		;; Load R3
	vmovapd zmm10, [srcreg+d2+64]		;; Load I3
	zfmaddpd zmm1, zmm2, zmm30, zmm10	;; A3 = R3 * cosine/sine + I3 (first R3/sine)	; 7-10		n 15
	zfmsubpd zmm10, zmm10, zmm30, zmm2	;; B3 = I3 * cosine/sine - R3 (first I3/sine)	; 7-10		n 16

	vmovapd zmm3, [srcreg+d4+d2]		;; Load R7
	vmovapd zmm11, [srcreg+d4+d2+64]	;; Load I7
	zfmsubpd zmm2, zmm3, zmm30, zmm11	;; A7 = R7 * cosine/sine - I7 (first R7/sine)	; 8-11		n 15
	zfmaddpd zmm11, zmm11, zmm30, zmm3	;; B7 = I7 * cosine/sine + R7 (first I7/sine)	; 8-11		n 16

	vmovapd zmm0, [srcreg]			;; Load R1
	vaddpd	zmm3, zmm7, zmm6		;; R4/sine + R6/sine				; 9-12		n 17
	vsubpd	zmm7, zmm7, zmm6		;; R4/sine - R6/sine				; 9-12		n 18

	vmovapd zmm8, [srcreg+64]		;; Load I1
	vaddpd	zmm6, zmm14, zmm13		;; I4/sine + I6/sine				; 10-13		n 20
	vsubpd	zmm14, zmm14, zmm13		;; I4/sine - I6/sine				; 10-13		n 19

bcast	vbroadcastsd zmm28, [screg+3*16]	;; sine for R5/I5 (w^4)
no bcast vmovapd zmm28, [screg+3*128]		;; sine for R5/I5
	zfmaddpd zmm13, zmm4, zmm29, zmm16	;; R2 + R8 * sine				; 11-14		n 17
	zfnmaddpd zmm4, zmm4, zmm29, zmm16	;; R2 - R8 * sine				; 11-14		n 18

bcast	vbroadcastsd zmm27, [screg+2*16]	;; sine for R4/I4 and R6/I6 (w^3)
no bcast vmovapd zmm27, [screg+2*128]		;; sine for R4/I4 and R6/I6
	zfmaddpd zmm16, zmm15, zmm29, zmm12	;; I2 + I8 * sine				; 12-15		n 20
	zfnmaddpd zmm15, zmm15, zmm29, zmm12	;; I2 - I8 * sine				; 12-15		n 19

bcast	vbroadcastsd zmm26, [screg+1*16]	;; sine for R3/I3 and R7/I7 (w^2)
no bcast vmovapd zmm26, [screg+1*128]		;; sine for R3/I3
	zfmaddpd zmm12, zmm5, zmm28, zmm0	;; R1 + R5 * sine				; 13-16		n 21
	zfnmaddpd zmm5, zmm5, zmm28, zmm0	;; R1 - R5 * sine				; 13-16		n 23
	bump	screg, scinc

	L1prefetch srcreg+L1pd, L1pt
	zfmaddpd zmm0, zmm9, zmm28, zmm8	;; I1 + I5 * sine				; 14-17		n 22
	zfnmaddpd zmm9, zmm9, zmm28, zmm8	;; I1 - I5 * sine				; 14-17		n 24

	L1prefetch srcreg+64+L1pd, L1pt
	vaddpd	zmm8, zmm1, zmm2		;; R3/sine + R7/sine				; 15-18		n 21
	vsubpd	zmm1, zmm1, zmm2		;; R3/sine - R7/sine				; 15-18		n 24

	L1prefetch srcreg+d1+L1pd, L1pt
	vaddpd	zmm2, zmm10, zmm11		;; I3/sine + I7/sine				; 16-19		n 22
	vsubpd	zmm10, zmm10, zmm11		;; I3/sine - I7/sine				; 16-19		n 23

	L1prefetch srcreg+d1+64+L1pd, L1pt
	zfmaddpd zmm11, zmm3, zmm27, zmm13	;; r2++ = (r2+r8) + (r4+r6) * sine		; 17-20		n 27
	zfnmaddpd zmm3, zmm3, zmm27, zmm13	;; r2+- = (r2+r8) - (r4+r6) * sine		; 17-20		n 25

	L1prefetch srcreg+d2+L1pd, L1pt
	zfmaddpd zmm13, zmm7, zmm27, zmm4	;; r2-+ = (r2-r8) + (r4-r6) * sine		; 18-21		n 26
	zfnmaddpd zmm7, zmm7, zmm27, zmm4	;; r2-- = (r2-r8) - (r4-r6) * sine		; 18-21		n 30

	L1prefetch srcreg+d2+64+L1pd, L1pt
	zfmaddpd zmm4, zmm14, zmm27, zmm15	;; i2-+ = (i2-i8) + (i4-i6) * sine		; 19-22		n 25
	zfnmaddpd zmm14, zmm14, zmm27, zmm15	;; i2-- = (i2-i8) - (i4-i6) * sine		; 19-22		n 29

	L1prefetch srcreg+d2+d1+L1pd, L1pt
	zfmaddpd zmm15, zmm6, zmm27, zmm16	;; i2++ = (i2+i8) + (i4+i6) * sine		; 20-23		n 28
	zfnmaddpd zmm6, zmm6, zmm27, zmm16	;; i2+- = (i2+i8) - (i4+i6) * sine		; 20-23		n 26

	L1prefetch srcreg+d2+d1+64+L1pd, L1pt
	zfmaddpd zmm16, zmm8, zmm26, zmm12	;; r1++ = (r1+r5) + (r3+r7) * sine		; 21-24		n 27
	zfnmaddpd zmm8, zmm8, zmm26, zmm12	;; r1+- = (r1+r5) - (r3+r7) * sine		; 21-24		n 29

	L1prefetch srcreg+d4+L1pd, L1pt
	zfmaddpd zmm12, zmm2, zmm26, zmm0	;; i1++ = (i1+i5) + (i3+i7) * sine		; 22-25		n 28
	zfnmaddpd zmm2, zmm2, zmm26, zmm0	;; i1+- = (i1+i5) - (i3+i7) * sine		; 22-25		n 30

	L1prefetch srcreg+d4+64+L1pd, L1pt
	zfmaddpd zmm0, zmm10, zmm26, zmm5	;; r1-+ = (r1-r5) + (i3-i7) * sine		; 23-26		n 31
	zfnmaddpd zmm10, zmm10, zmm26, zmm5	;; r1-- = (r1-r5) - (i3-i7) * sine		; 23-26		n 33

	L1prefetch srcreg+d4+d1+L1pd, L1pt
	zfmaddpd zmm5, zmm1, zmm26, zmm9	;; i1-+ = (i1-i5) + (r3-r7) * sine		; 24-27		n 34
	zfnmaddpd zmm1, zmm1, zmm26, zmm9	;; i1-- = (i1-i5) - (r3-r7) * sine		; 24-27		n 32

	L1prefetch srcreg+d4+d1+64+L1pd, L1pt
	vaddpd	zmm9, zmm3, zmm4		;; r2+-+ = (r2+-) + (i2-+)			; 25-28		n 31
	vsubpd	zmm3, zmm3, zmm4		;; r2+-- = (r2+-) - (i2-+)			; 25-28		n 33

	L1prefetch srcreg+d4+d2+L1pd, L1pt
	vaddpd	zmm4, zmm13, zmm6		;; r2-++ = (r2-+) + (i2+-)			; 26-29		n 34
	vsubpd	zmm13, zmm13, zmm6		;; r2-+- = (r2-+) - (i2+-)			; 26-29		n 32

	L1prefetch srcreg+d4+d2+64+L1pd, L1pt
	vaddpd	zmm6, zmm16, zmm11		;; R1 = (r1++) + (r2++)				; 27-30
	vsubpd	zmm16, zmm16, zmm11		;; R5 = (r1++) - (r2++)				; 27-30

	L1prefetch srcreg+d4+d2+d1+L1pd, L1pt
	vaddpd	zmm11, zmm12, zmm15		;; I1 = (i1++) + (i2++)				; 28-31
	vsubpd	zmm12, zmm12, zmm15		;; I5 = (i1++) - (i2++)				; 28-31

	L1prefetch srcreg+d4+d2+d1+64+L1pd, L1pt
	vaddpd	zmm15, zmm8, zmm14		;; R3 = (r1+-) + (i2--)				; 29-32
	vsubpd	zmm8, zmm8, zmm14		;; R7 = (r1+-) - (i2--)				; 29-32

	vsubpd	zmm14, zmm2, zmm7		;; I3 = (i1+-) - (r2--)				; 30-33
	vaddpd	zmm2, zmm2, zmm7		;; I7 = (i1+-) + (r2--)				; 30-33

	zfmaddpd zmm7, zmm9, zmm31, zmm0	;; R2 = (r1-+) + .707*(r2+-+)			; 31-34
	zfnmaddpd zmm9, zmm9, zmm31, zmm0	;; R6 = (r1-+) - .707*(r2+-+)			; 31-34
	zstore	[srcreg], zmm6			;; Save R1					; 31

	zfnmaddpd zmm0, zmm13, zmm31, zmm1	;; I2 = (i1--) - .707*(r2-+-)			; 32-35
	zfmaddpd zmm13, zmm13, zmm31, zmm1	;; I6 = (i1--) + .707*(r2-+-)			; 32-35
	zstore	[srcreg+d4], zmm16		;; Save R5					; 31+1

	zfnmaddpd zmm1, zmm3, zmm31, zmm10	;; R4 = (r1--) - .707*(r2+--)			; 33-36
	zfmaddpd zmm3, zmm3, zmm31, zmm10	;; R8 = (r1--) + .707*(r2+--)			; 33-36
	zstore	[srcreg+64], zmm11		;; Save I1					; 32+1

	zfnmaddpd zmm10, zmm4, zmm31, zmm5	;; I4 = (i1-+) - .707*(r2-++)			; 34-37
	zfmaddpd zmm4, zmm4, zmm31, zmm5	;; I8 = (i1-+) + .707*(r2-++)			; 34-37
	zstore	[srcreg+d4+64], zmm12		;; Save I5					; 32+2

	zstore	[srcreg+d2], zmm15		;; Save R3					; 33+2
	zstore	[srcreg+d4+d2], zmm8		;; Save R7					; 33+3
	zstore	[srcreg+d2+64], zmm14		;; Save I3					; 34+3
	zstore	[srcreg+d4+d2+64], zmm2		;; Save I7					; 34+4
	zstore	[srcreg+d1], zmm7		;; Save R2					; 35+4
	zstore	[srcreg+d4+d1], zmm9		;; Save R6					; 35+5
	zstore	[srcreg+d1+64], zmm0		;; Save I2					; 36+5
	zstore	[srcreg+d4+d1+64], zmm13	;; Save I6					; 36+6
	zstore	[srcreg+d2+d1], zmm1		;; Save R4					; 37+6
	zstore	[srcreg+d4+d2+d1], zmm3		;; Save R8					; 37+7
	zstore	[srcreg+d2+d1+64], zmm10	;; Save I4					; 38+7
	zstore	[srcreg+d4+d2+d1+64], zmm4	;; Save I8					; 38+8
	bump	srcreg, srcinc
	ENDM


;;
;; ******************************* eight-complex first and last variants (two pass FFTs) ***********************************
;;

;; This code applies the roots-of-minus-1 premultipliers in an all-complex FFT as well as
;; the group weight multipliers (the column weights are applied later).
;; It also applies the seven sin/cos multipliers after the first radix-8 butterfly.
;; We save memory by splitting the roots-of-minus-1 premultipliers such that every
;; macro uses the same premultiplier data.  The rest of the premultiplier is applied
;; at the end of pass 1.

;; The sin/cos and premultiplier data is combined in one table.  The premultiplier data
;; is at screg the sin/cos data is at screg+8*64.  The premultiplier sine value has been
;; merged into the group multipliers.

;; Macro assumes these registers:
;; rbx = register to load compressed fudge index (top 56 bits are zero)
;; r12 = pointer to compressed fudges array
;; r13 = pointer to XOR masks
;; r14 = scratch register

zr8_csc_wpn_eight_complex_first_djbfft_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRTHALF
	vbroadcastsd zmm28, ZMM_ONE_OVER_B
	ENDM
zr8_csc_wpn_eight_complex_first_djbfft MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maskreg,maskinc,grpreg,grpinc,maxrpt,L1pt,L1pd
	mov	bl, [maskreg+0*1]		;; Load index into compressed fudges
	mov	r14, [r13+0*8]			;; Load the xor mask
	xor	r14, [r12+rbx*8]		;; Apply the xor mask to 64 bits of fudge flags

	kmovw	k1, r14d			;; Load R1 and I1 fudge factor mask		; 1
	kshiftrw k2, k1, 8			;; I1's fudge					; 2
	vmovapd zmm0, [grpreg+0*64]		;; group multiplier for R1
	vmulpd	zmm0{k1}, zmm0, zmm28		;; fudged group multiplier for R1		; 2-5
	vmovapd zmm8, [grpreg+1*64]		;; group multiplier for I1
	vmulpd	zmm8{k2}, zmm8, zmm28		;; fudged group multiplier for I1		; 3-6

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R2 and I2 fudge factor mask		; 3
	kshiftrw k2, k1, 8			;; I2's fudge					; 4
	vmovapd zmm1, [grpreg+2*64]		;; group multiplier for R2
	vmulpd	zmm1{k1}, zmm1, zmm28		;; fudged group multiplier for R2		; 4-7
	vmovapd zmm9, [grpreg+3*64]		;; group multiplier for I2
	vmulpd	zmm9{k2}, zmm9, zmm28		;; fudged group multiplier for I2		; 5-8

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R3 and I3 fudge factor mask		; 5
	kshiftrw k2, k1, 8			;; I3's fudge					; 6
	vmovapd zmm2, [grpreg+4*64]		;; group multiplier for R3
	vmulpd	zmm2{k1}, zmm2, zmm28		;; fudged group multiplier for R3		; 6-9
	vmovapd zmm10, [grpreg+5*64]		;; group multiplier for I3
	vmulpd	zmm10{k2}, zmm10, zmm28		;; fudged group multiplier for I3		; 7-10

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R4 and I4 fudge factor mask		; 7
	kshiftrw k2, k1, 8			;; I4's fudge					; 8
	vmovapd zmm3, [grpreg+6*64]		;; group multiplier for R4
	vmulpd	zmm3{k1}, zmm3, zmm28		;; fudged group multiplier for R4		; 8-11
	vmovapd zmm11, [grpreg+7*64]		;; group multiplier for I4
	vmulpd	zmm11{k2}, zmm11, zmm28		;; fudged group multiplier for I4		; 9-12

	vmulpd	zmm0, zmm0, [srcreg]		;; apply the fudged group multiplier to R1	; 9-12
	vmulpd	zmm8, zmm8, [srcreg+64]		;; apply the fudged group multiplier to I1	; 10-13
	vmulpd	zmm1, zmm1, [srcreg+d1]		;; apply the fudged group multiplier to R2	; 10-13
	vmulpd	zmm9, zmm9, [srcreg+d1+64]	;; apply the fudged group multiplier to I2	; 11-14
	vmulpd	zmm2, zmm2, [srcreg+d2]		;; apply the fudged group multiplier to R3	; 11-14
	vmulpd	zmm10, zmm10, [srcreg+d2+64]	;; apply the fudged group multiplier to I3	; 12-15
	vmulpd	zmm3, zmm3, [srcreg+d2+d1]	;; apply the fudged group multiplier to R4	; 12-15
	vmulpd	zmm11, zmm11, [srcreg+d2+d1+64]	;; apply the fudged group multiplier to I4	; 13-16

	mov	bl, [maskreg+1*1]		;; Load index into compressed fudges
	mov	r14, [r13+1*8]			;; Load the xor mask
	xor	r14, [r12+rbx*8]		;; Apply the xor mask to 64 bits of fudge flags

	kmovw	k1, r14d			;; Load R5 and I5 fudge factor mask		; 13
	kshiftrw k2, k1, 8			;; I5's fudge					; 14
	vmovapd zmm4, [grpreg+8*64]		;; group multiplier for R5
	vmulpd	zmm4{k1}, zmm4, zmm28		;; fudged group multiplier for R5		; 14-17
	vmovapd zmm12, [grpreg+9*64]		;; group multiplier for I5
	vmulpd	zmm12{k2}, zmm12, zmm28		;; fudged group multiplier for I5		; 15-18

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R6 and I6 fudge factor mask		; 15
	kshiftrw k2, k1, 8			;; I6's fudge					; 16
	vmovapd zmm5, [grpreg+10*64]		;; group multiplier for R6
	vmulpd	zmm5{k1}, zmm5, zmm28		;; fudged group multiplier for R6		; 16-19
	vmovapd zmm13, [grpreg+11*64]		;; group multiplier for I6
	vmulpd	zmm13{k2}, zmm13, zmm28		;; fudged group multiplier for I6		; 17-20

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R7 and I7 fudge factor mask		; 17
	kshiftrw k2, k1, 8			;; I7's fudge					; 18
	vmovapd zmm6, [grpreg+12*64]		;; group multiplier for R7
	vmulpd	zmm6{k1}, zmm6, zmm28		;; fudged group multiplier for R7		; 18-21
	vmovapd zmm14, [grpreg+13*64]		;; group multiplier for I7
	vmulpd	zmm14{k2}, zmm14, zmm28		;; fudged group multiplier for I7		; 19-22

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R8 and I8 fudge factor mask		; 19
	kshiftrw k2, k1, 8			;; I8's fudge					; 20
	vmovapd zmm7, [grpreg+14*64]		;; group multiplier for R8
	vmulpd	zmm7{k1}, zmm7, zmm28		;; fudged group multiplier for R8		; 20-23
	vmovapd zmm15, [grpreg+15*64]		;; group multiplier for I8
	vmulpd	zmm15{k2}, zmm15, zmm28		;; fudged group multiplier for I8		; 21-24

	vmulpd	zmm4, zmm4, [srcreg+d4]		;; apply the fudged group multiplier to R5	; 21-24
	vmulpd	zmm12, zmm12, [srcreg+d4+64]	;; apply the fudged group multiplier to I5	; 22-25
	vmulpd	zmm5, zmm5, [srcreg+d4+d1]	;; apply the fudged group multiplier to R6	; 22-25
	vmulpd	zmm13, zmm13, [srcreg+d4+d1+64]	;; apply the fudged group multiplier to I6	; 23-26
	vmulpd	zmm6, zmm6, [srcreg+d4+d2]	;; apply the fudged group multiplier to R7	; 23-26
	vmulpd	zmm14, zmm14, [srcreg+d4+d2+64]	;; apply the fudged group multiplier to I7	; 24-27
	vmulpd	zmm7, zmm7, [srcreg+d4+d2+d1]	;; apply the fudged group multiplier to R8	; 24-27
	vmulpd	zmm15, zmm15, [srcreg+d4+d2+d1+64] ;; apply the fudged group multiplier to I8	; 25-28

	bump	maskreg, maskinc
	bump	grpreg, grpinc

;; Apply the complex premultipliers

	vmovapd zmm30, [screg+1*64]		;; premultiplier cosine/sine for R2/I2
	zfmsubpd zmm16, zmm1, zmm30, zmm9	;; A2 = R2 * cosine - I2			; 1-4		n 
	zfmaddpd zmm9, zmm9, zmm30, zmm1	;; B2 = I2 * cosine + R2			; 1-4		n 

	vmovapd zmm30, [screg+3*64]		;; premultiplier cosine/sine for R4/I4
	zfmsubpd zmm1, zmm3, zmm30, zmm11	;; A4 = R4 * cosine - I4			; 2-5		n 
	zfmaddpd zmm11, zmm11, zmm30, zmm3	;; B4 = I4 * cosine + R4			; 2-5		n 

	vmovapd zmm30, [screg+5*64]		;; premultiplier cosine/sine for R6/I6
	zfmsubpd zmm3, zmm5, zmm30, zmm13	;; A6 = R6 * cosine - I6			; 3-6		n 
	zfmaddpd zmm13, zmm13, zmm30, zmm5	;; B6 = I6 * cosine + R6			; 3-6		n 

	vmovapd zmm30, [screg+7*64]		;; premultiplier cosine/sine for R8/I8
	zfmsubpd zmm5, zmm7, zmm30, zmm15	;; A8 = R8 * cosine - I8			; 4-7		n 
	zfmaddpd zmm15, zmm15, zmm30, zmm7	;; B8 = I8 * cosine + R8			; 4-7		n 

	vmovapd zmm30, [screg+0*64]		;; premultiplier cosine/sine for R1/I1
	zfmsubpd zmm7, zmm0, zmm30, zmm8	;; A1 = R1 * cosine - I1			; 5-8		n 
	zfmaddpd zmm8, zmm8, zmm30, zmm0	;; B1 = I1 * cosine + R1			; 5-8		n 

	vmovapd zmm30, [screg+2*64]		;; premultiplier cosine/sine for R3/I3
	zfmsubpd zmm0, zmm2, zmm30, zmm10	;; A3 = R3 * cosine - I3			; 6-9		n 
	zfmaddpd zmm10, zmm10, zmm30, zmm2	;; B3 = I3 * cosine + R3			; 6-9		n 

	vmovapd zmm30, [screg+4*64]		;; premultiplier cosine/sine for R5/I5
	zfmsubpd zmm2, zmm4, zmm30, zmm12	;; A5 = R5 * cosine - I5			; 7-10		n 
	zfmaddpd zmm12, zmm12, zmm30, zmm4	;; B5 = I5 * cosine + R5			; 7-10		n 

	vmovapd zmm30, [screg+6*64]		;; premultiplier cosine/sine for R7/I7
	zfmsubpd zmm4, zmm6, zmm30, zmm14	;; A7 = R7 * cosine - I7			; 8-11		n 
	zfmaddpd zmm14, zmm14, zmm30, zmm6	;; B7 = I7 * cosine + R7			; 8-11		n 

;; Copied from common 8-complex DJB FFT macro

	vsubpd	zmm6, zmm16, zmm3		;; R2-R6					; 1-4		n 9
	vaddpd	zmm16, zmm16, zmm3		;; R2+R6					; 1-4		n 11
	vmovapd zmm30, [screg+8*64+1*128]	;; sine for R3/I3 and R7/I7 (w^2)

	vsubpd	zmm3, zmm1, zmm5		;; R4-R8					; 2-5		n 9
	vaddpd	zmm1, zmm1, zmm5		;; R4+R8					; 2-5		n 11
	vmovapd zmm29, [screg+8*64+0*128]	;; sine for R2/I2 and R8/I8 (w^1)

	vsubpd	zmm5, zmm11, zmm15		;; I4-I8					; 3-6		n 10
	vaddpd	zmm11, zmm11, zmm15		;; I4+I8					; 3-6		n 12
	vmovapd zmm27, [screg+8*64+2*128]	;; sine for R4/I4 and R6/I6 (w^3)

	vsubpd	zmm15, zmm9, zmm13		;; I2-I6					; 4-7		n 10
	vaddpd	zmm9, zmm9, zmm13		;; I2+I6					; 4-7		n 12
	vmovapd zmm26, [screg+8*64+1*128+64]	;; cosine/sine for R3/I3 and R7/I7 (w^2)

	vaddpd	zmm13, zmm7, zmm2		;; R1+R5					; 5-8		n 13
	vsubpd	zmm7, zmm7, zmm2		;; R1-R5					; 5-8		n 14
	vmovapd zmm25, [screg+8*64+3*128+64]	;; cosine/sine for R5/I5 (w^4)

	vaddpd	zmm2, zmm0, zmm4		;; R3+R7					; 6-9		n 13
	vsubpd	zmm0, zmm0, zmm4		;; R3-R7					; 6-9		n 17
	vmovapd zmm24, [screg+8*64+0*128+64]	;; cosine/sine for R2/I2 and R8/I8 (w^1)

	vaddpd	zmm4, zmm8, zmm12		;; I1+I5					; 7-10		n 16
	vsubpd	zmm8, zmm8, zmm12		;; I1-I5					; 7-10		n 15
	vmovapd zmm23, [screg+8*64+2*128+64]	;; cosine/sine for R4/I4 and R6/I6 (w^3)

	vaddpd	zmm12, zmm10, zmm14		;; I3+I7					; 8-11		n 16
	vsubpd	zmm10, zmm10, zmm14		;; I3-I7					; 8-11		n 18
	vmovapd zmm22, [screg+8*64+3*128]	;; sine for R5/I5 (w^4)

	vaddpd	zmm14, zmm6, zmm3		;; r2-+ = (r2-r6) + (r4-r8)			; 9-12		n 17
	vsubpd	zmm6, zmm6, zmm3		;; r2-- = (r2-r6) - (r4-r8)			; 9-12		n 14
	bump	screg, scinc
	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	zmm3, zmm15, zmm5		;; i2-- = (i2-i6) - (i4-i8)			; 10-13		n 15
	vaddpd	zmm15, zmm15, zmm5		;; i2-+ = (i2-i6) + (i4-i8)			; 10-13		n 18
	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	zmm5, zmm16, zmm1		;; r2+- = (r2+r6) - (r4+r8)			; 11-14		n 19
	vaddpd	zmm16, zmm16, zmm1		;; r2++ = (r2+r6) + (r4+r8)			; 11-14		n 22
	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	zmm1, zmm9, zmm11		;; i2+- = (i2+i6) - (i4+i8)			; 12-15		n 19
	vaddpd	zmm9, zmm9, zmm11		;; i2++ = (i2+i6) + (i4+i8)			; 12-15		n 23
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vaddpd	zmm11, zmm13, zmm2		;; r1++ = (r1+r5) + (r3+r7)			; 13-16		n 22
	vsubpd	zmm13, zmm13, zmm2		;; r1+- = (r1+r5) - (r3+r7)			; 13-16		n 24
	L1prefetchw srcreg+d4+L1pd, L1pt

	zfmaddpd zmm2, zmm6, zmm31, zmm7	;; r1-+ = (r1-r5) + .707*(r2--)			; 14-17		n 20
	zfnmaddpd zmm6, zmm6, zmm31, zmm7	;; r1-- = (r1-r5) - .707*(r2--)			; 14-17		n 21
	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	zfmaddpd zmm7, zmm3, zmm31, zmm8	;; i1-+ = (i1-i5) + .707*(i2--)			; 15-18		n 20
	zfnmaddpd zmm3, zmm3, zmm31, zmm8	;; i1-- = (i1-i5) - .707*(i2--)			; 15-18		n 21
	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	vaddpd	zmm8, zmm4, zmm12		;; i1++ = (i1+i5) + (i3+i7)			; 16-19		n 23
	vsubpd	zmm4, zmm4, zmm12		;; i1+- = (i1+i5) - (i3+i7)			; 16-19		n 24
	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	zfmaddpd zmm12, zmm14, zmm31, zmm0	;; r3-+ = (r3-r7) + .707*(r2-+)			; 17-20		n 26
	zfnmaddpd zmm14, zmm14, zmm31, zmm0	;; r3-- = (r3-r7) - .707*(r2-+)			; 17-20		n 30
	L1prefetchw srcreg+64+L1pd, L1pt

	zfmaddpd zmm0, zmm15, zmm31, zmm10	;; i3-+ = (i3-i7) + .707*(i2-+)			; 18-21		n 26
	zfnmaddpd zmm15, zmm15, zmm31, zmm10	;; i3-- = (i3-i7) - .707*(i2-+)			; 18-21		n 30
	L1prefetchw srcreg+d1+64+L1pd, L1pt

	vmulpd	zmm5, zmm5, zmm30		;; r2+-s = r2+- * sine37			; 19-22		n 24
	vmulpd	zmm1, zmm1, zmm30		;; i2+-s = i2+- * sine37			; 19-22		n 24
	L1prefetchw srcreg+d2+64+L1pd, L1pt

	vmulpd	zmm2, zmm2, zmm29		;; r1-+s = r1-+ * sine28			; 20-23		n 26
	vmulpd	zmm7, zmm7, zmm29		;; i1-+s = i1-+ * sine28			; 20-23		n 26
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt

	vmulpd	zmm6, zmm6, zmm27		;; r1--s = r1-- * sine46			; 21-24		n 30
	vmulpd	zmm3, zmm3, zmm27		;; i1--s = i1-- * sine46			; 21-24		n 30
	L1prefetchw srcreg+d4+64+L1pd, L1pt

	vsubpd	zmm10, zmm11, zmm16		;; R5 = (r1++) - (r2++)				; 22-25		n 32
	vaddpd	zmm11, zmm11, zmm16		;; R1 = (r1++) + (r2++)				; 22-25
	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt

	vsubpd	zmm16, zmm8, zmm9		;; I5 = (i1++) - (i2++)				; 23-26		n 32
	vaddpd	zmm8, zmm8, zmm9		;; I1 = (i1++) + (i2++)				; 23-26
	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt

	zfmsubpd zmm9, zmm13, zmm30, zmm1	;; R3s = (r1+-)*sine37 - (i2+-s)		; 24-27		n 28
	zfmaddpd zmm17, zmm4, zmm30, zmm5	;; I3s = (i1+-)*sine37 + (r2+-s)		; 24-27		n 28
	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt

	zfmaddpd zmm13, zmm13, zmm30, zmm1	;; R7s = (r1+-)*sine37 + (i2+-s)		; 25-28		n 32
	zfmsubpd zmm4, zmm4, zmm30, zmm5	;; I7s = (i1+-)*sine37 - (r2+-s)		; 25-28		n 32

	zfnmaddpd zmm1, zmm0, zmm29, zmm2	;; R2s = (r1-+s) - sine28*(i3-+)		; 26-29		n 33
	zfmaddpd zmm5, zmm12, zmm29, zmm7	;; I2s = (i1-+s) + sine28*(r3-+)		; 26-29		n 33
	zstore	[srcreg], zmm11			;; Store R1					; 26

	zfmaddpd zmm0, zmm0, zmm29, zmm2	;; R8s = (r1-+s) + sine28*(i3-+)		; 27-30		n 34
	zfnmaddpd zmm12, zmm12, zmm29, zmm7	;; I8s = (i1-+s) - sine28*(r3-+)		; 27-30		n 34
	zstore	[srcreg+64], zmm8		;; Store I1					; 27

	zfmsubpd zmm2, zmm9, zmm26, zmm17	;; R3s * cosine/sine - I3s (final R3)		; 28-31
	zfmaddpd zmm17, zmm17, zmm26, zmm9	;; I3s * cosine/sine + R3s (final I3)		; 28-31

	zfmaddpd zmm7, zmm13, zmm26, zmm4	;; R7s * cosine/sine + I7s (final R7)		; 29-32
	zfmsubpd zmm4, zmm4, zmm26, zmm13	;; I7s * cosine/sine - R7s (final I7)		; 29-32

	zfmaddpd zmm9, zmm15, zmm27, zmm6	;; R4s = (r1--s) + sine46*(i3--)		; 30-33		n 35
	zfnmaddpd zmm13, zmm14, zmm27, zmm3	;; I4s = (i1--s) - sine46*(r3--)		; 30-33		n 35

	zfnmaddpd zmm15, zmm15, zmm27, zmm6	;; R6s = (r1--s) - sine46*(i3--)		; 31-34		n 36
	zfmaddpd zmm14, zmm14, zmm27, zmm3	;; I6s = (i1--s) + sine46*(r3--)		; 31-34		n 36

	zfmsubpd zmm6, zmm10, zmm25, zmm16	;; A5 = R5 * cosine/sine - I5			; 32-35		n 37
	zfmaddpd zmm16, zmm16, zmm25, zmm10	;; B5 = I5 * cosine/sine + R5			; 32-35		n 37
	zstore	[srcreg+d2], zmm2		;; Store R3					; 32

	zfmsubpd zmm3, zmm1, zmm24, zmm5	;; R2s * cosine/sine - I2s (final R2)		; 33-36
	zfmaddpd zmm5, zmm5, zmm24, zmm1	;; I2s * cosine/sine + R2s (final I2)		; 33-36
	zstore	[srcreg+d2+64], zmm17		;; Store I3					; 32+1

	zfmaddpd zmm10, zmm0, zmm24, zmm12	;; R8s * cosine/sine + I8s (final R8)		; 34-37
	zfmsubpd zmm12, zmm12, zmm24, zmm0	;; I8s * cosine/sine - R8s (final I8)		; 34-37
	zstore	[srcreg+d4+d2], zmm7		;; Store R7					; 33+1

	zfmsubpd zmm1, zmm9, zmm23, zmm13	;; R4s * cosine/sine - I4s (final R4)		; 35-38
	zfmaddpd zmm13, zmm13, zmm23, zmm9	;; I4s * cosine/sine + R4s (final I4)		; 35-38
	zstore	[srcreg+d4+d2+64], zmm4		;; Store I7					; 33+2

	zfmaddpd zmm0, zmm15, zmm23, zmm14	;; R6s * cosine/sine + I6s (final R6)		; 36-39
	zfmsubpd zmm14, zmm14, zmm23, zmm15	;; I6s * cosine/sine - R6s (final I6)		; 36-39

	vmulpd	zmm6, zmm6, zmm22		;; A5 = A5 * sine (final R5)			; 37-40
	vmulpd	zmm16, zmm16, zmm22		;; B5 = B5 * sine (final I5)			; 37-40

	zstore	[srcreg+d1], zmm3		;; Store R2					; 37
	zstore	[srcreg+d1+64], zmm5		;; Store I2					; 37+1
	zstore	[srcreg+d4+d2+d1], zmm10	;; Store R8					; 38+1
	zstore	[srcreg+d4+d2+d1+64], zmm12	;; Store I8					; 38+2
	zstore	[srcreg+d2+d1], zmm1		;; Store R4					; 39+2
	zstore	[srcreg+d2+d1+64], zmm13	;; Store I4					; 39+3
	zstore	[srcreg+d4+d1], zmm0		;; Store R6					; 40+3
	zstore	[srcreg+d4+d1+64], zmm14	;; Store I6					; 40+4
	zstore	[srcreg+d4], zmm6		;; Store R5					; 41+4
	zstore	[srcreg+d4+64], zmm16		;; Store I5					; 41+5
	bump	srcreg, srcinc
	ENDM


;; This code applies the sin/cos multipliers before a radix-8 butterfly.
;; Then it applies the postmultipliers since the all-complex inverse FFT is complete
;; as well as the group weight multipliers.
;; We save memory by splitting the roots-of-minus-1 premultipliers such that every
;; macro uses the same premultiplier data.  The rest of the premultiplier is applied
;; at the end of pass 1.

;; The sin/cos and premultiplier data is combined in one table.  The premultiplier data
;; is at screg the sin/cos data is at screg+8*64.  The premultiplier sine value has been
;; merged into the group multipliers.

;; Macro assumes these registers:
;; rbx = register to load compressed fudge index (top 56 bits are zero)
;; r12 = pointer to compressed fudges array
;; r13 = pointer to XOR masks
;; r14 = scratch register

zr8_csc_wpn_eight_complex_last_djbunfft_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRTHALF
	vbroadcastsd zmm28, ZMM_B
	ENDM
zr8_csc_wpn_eight_complex_last_djbunfft MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maskreg,maskinc,maxrpt,L1pt,L1pd
	vmovapd zmm30, [screg+8*64+0*128+64]	;; cosine/sine for R2/I2 and R8/I8
	vmovapd zmm4, [srcreg+d1]		;; Load R2
	vmovapd zmm12, [srcreg+d1+64]		;; Load I2
	zfmaddpd zmm16, zmm4, zmm30, zmm12	;; A2 = R2 * cosine/sine + I2			; 1-4		n 5
	zfmsubpd zmm12, zmm12, zmm30, zmm4	;; B2 = I2 * cosine/sine - R2			; 1-4		n 5

	vmovapd zmm7, [srcreg+d4+d2+d1]		;; Load R8
	vmovapd zmm15, [srcreg+d4+d2+d1+64]	;; Load I8
	zfmsubpd zmm4, zmm7, zmm30, zmm15	;; A8 = R8 * cosine/sine - I8			; 2-5		n 11
	zfmaddpd zmm15, zmm15, zmm30, zmm7	;; B8 = I8 * cosine/sine + R8			; 2-5		n 12

	vmovapd zmm30, [screg+8*64+2*128+64]	;; cosine/sine for R4/I4 and R6/I6
	vmovapd zmm6, [srcreg+d2+d1]		;; Load R4
	vmovapd zmm14, [srcreg+d2+d1+64]	;; Load I4
	zfmaddpd zmm7, zmm6, zmm30, zmm14	;; A4 = R4 * cosine/sine + I4 (first R4/sine)	; 3-6		n 9
	zfmsubpd zmm14, zmm14, zmm30, zmm6	;; B4 = I4 * cosine/sine - R4 (first I4/sine)	; 3-6		n 10

	vmovapd zmm5, [srcreg+d4+d1]		;; Load R6
	vmovapd zmm13, [srcreg+d4+d1+64]	;; Load I6
	zfmsubpd zmm6, zmm5, zmm30, zmm13	;; A6 = R6 * cosine/sine - I6 (first R6/sine)	; 4-7		n 9
	zfmaddpd zmm13, zmm13, zmm30, zmm5	;; B6 = I6 * cosine/sine + R6 (first I6/sine)	; 4-7		n 10

	vmovapd zmm29, [screg+8*64+0*128]	;; sine for R2/I2 and R8/I8
	vmulpd	zmm16, zmm16, zmm29		;; A2 = A2 * sine (first R2)			; 5-8		n 11
	vmulpd	zmm12, zmm12, zmm29		;; B2 = B2 * sine (first I2)			; 5-8		n 12

	vmovapd zmm30, [screg+8*64+3*128+64]	;; cosine/sine for R5/I5
	vmovapd zmm1, [srcreg+d4]		;; Load R5
	vmovapd zmm9, [srcreg+d4+64]		;; Load I5
	zfmaddpd zmm5, zmm1, zmm30, zmm9	;; A5 = R5 * cosine/sine + I5 (first R5/sine)	; 6-9		n 13
	zfmsubpd zmm9, zmm9, zmm30, zmm1	;; B5 = I5 * cosine/sine - R5 (first I5/sine)	; 6-9		n 14

	vmovapd zmm30, [screg+8*64+1*128+64]	;; cosine/sine for R3/I3 and R7/I7
	vmovapd zmm2, [srcreg+d2]		;; Load R3
	vmovapd zmm10, [srcreg+d2+64]		;; Load I3
	zfmaddpd zmm1, zmm2, zmm30, zmm10	;; A3 = R3 * cosine/sine + I3 (first R3/sine)	; 7-10		n 15
	zfmsubpd zmm10, zmm10, zmm30, zmm2	;; B3 = I3 * cosine/sine - R3 (first I3/sine)	; 7-10		n 16

	vmovapd zmm3, [srcreg+d4+d2]		;; Load R7
	vmovapd zmm11, [srcreg+d4+d2+64]	;; Load I7
	zfmsubpd zmm2, zmm3, zmm30, zmm11	;; A7 = R7 * cosine/sine - I7 (first R7/sine)	; 8-11		n 15
	zfmaddpd zmm11, zmm11, zmm30, zmm3	;; B7 = I7 * cosine/sine + R7 (first I7/sine)	; 8-11		n 16

	vmovapd zmm0, [srcreg]			;; Load R1
	vaddpd	zmm3, zmm7, zmm6		;; R4/sine + R6/sine				; 9-12		n 17
	vsubpd	zmm7, zmm7, zmm6		;; R4/sine - R6/sine				; 9-12		n 18

	vmovapd zmm25, [screg+8*64+3*128]	;; sine for R5/I5
	vaddpd	zmm6, zmm14, zmm13		;; I4/sine + I6/sine				; 10-13		n 22
	vsubpd	zmm14, zmm14, zmm13		;; I4/sine - I6/sine				; 10-13		n 20

	mov	bl, [maskreg+0*1]		;; Load index into compressed fudges
	zfmaddpd zmm13, zmm4, zmm29, zmm16	;; R2 + R8 * sine				; 11-14		n 17
	zfnmaddpd zmm4, zmm4, zmm29, zmm16	;; R2 - R8 * sine				; 11-14		n 18

	vmovapd zmm8, [srcreg+64]		;; Load I1
	zfmaddpd zmm16, zmm15, zmm29, zmm12	;; I2 + I8 * sine				; 12-15		n 22
	zfnmaddpd zmm15, zmm15, zmm29, zmm12	;; I2 - I8 * sine				; 12-15		n 20

	vmovapd zmm26, [screg+8*64+1*128]	;; sine for R3/I3 and R7/I7
	zfmaddpd zmm12, zmm5, zmm25, zmm0	;; R1 + R5 * sine				; 13-16		n 23
	zfnmaddpd zmm5, zmm5, zmm25, zmm0	;; R1 - R5 * sine				; 13-16		n 26

	vmovapd zmm27, [screg+8*64+2*128]	;; sine for R4/I4 and R6/I6
	zfmaddpd zmm0, zmm9, zmm25, zmm8	;; I1 + I5 * sine				; 14-17		n 24
	zfnmaddpd zmm9, zmm9, zmm25, zmm8	;; I1 - I5 * sine				; 14-17		n 27

	vaddpd	zmm8, zmm1, zmm2		;; R3/sine + R7/sine				; 15-18		n 23
	vsubpd	zmm1, zmm1, zmm2		;; R3/sine - R7/sine				; 15-18		n 27
	mov	r14, [r13+0*8]			;; Load the xor mask

	vaddpd	zmm2, zmm10, zmm11		;; I3/sine + I7/sine				; 16-19		n 24
	vsubpd	zmm10, zmm10, zmm11		;; I3/sine - I7/sine				; 16-19		n 26
	xor	r14, [r12+rbx*8]		;; Apply the xor mask to 64 bits of fudge flags

	zfmaddpd zmm11, zmm3, zmm27, zmm13	;; r2++ = (r2+r8) + (r4+r6) * sine		; 17-20		n 30
	zfnmaddpd zmm3, zmm3, zmm27, zmm13	;; r2+- = (r2+r8) - (r4+r6) * sine		; 17-20		n 28
	mov	bl, [maskreg+1*1]		;; Load index into compressed fudges
	bump	maskreg, maskinc

	kmovw	k1, r14d			;; Load R1 and I1 fudge factor mask		; 18		n 46
	zfmaddpd zmm13, zmm7, zmm27, zmm4	;; r2-+ = (r2-r8) + (r4-r6) * sine		; 18-21		n 29
	shr	r14, 16				;; Next 16 bits of fudge flags

	kmovw	k2, r14d			;; Load R2 and I2 fudge factor mask		; 19		n 52
	zfnmaddpd zmm7, zmm7, zmm27, zmm4	;; r2-- = (r2-r8) - (r4-r6) * sine		; 19-22		n 33
	shr	r14, 16				;; Next 16 bits of fudge flags

	kmovw	k3, r14d			;; Load R3 and I3 fudge factor mask		; 20		n 47
	zfmaddpd zmm4, zmm14, zmm27, zmm15	;; i2-+ = (i2-i8) + (i4-i6) * sine		; 20-23		n 28
	shr	r14, 16				;; Next 16 bits of fudge flags

	kmovw	k4, r14d			;; Load R4 and I4 fudge factor mask		; 21		n 54
	zfnmaddpd zmm14, zmm14, zmm27, zmm15	;; i2-- = (i2-i8) - (i4-i6) * sine		; 21-24		n 32
	mov	r14, [r13+1*8]			;; Load the xor mask

	zfmaddpd zmm15, zmm6, zmm27, zmm16	;; i2++ = (i2+i8) + (i4+i6) * sine		; 22-25		n 31
	zfnmaddpd zmm6, zmm6, zmm27, zmm16	;; i2+- = (i2+i8) - (i4+i6) * sine		; 22-25		n 29
	xor	r14, [r12+rbx*8]		;; Apply the xor mask to 64 bits of fudge flags

	zfmaddpd zmm16, zmm8, zmm26, zmm12	;; r1++ = (r1+r5) + (r3+r7) * sine		; 23-26		n 30
	zfnmaddpd zmm8, zmm8, zmm26, zmm12	;; r1+- = (r1+r5) - (r3+r7) * sine		; 23-26		n 32
	vmovapd zmm20, [screg+0*64]		;; premultiplier cosine/sine for R1/I1

	kmovw	k5, r14d			;; Load R5 and I5 fudge factor mask		; 24		n 49
	zfmaddpd zmm12, zmm2, zmm26, zmm0	;; i1++ = (i1+i5) + (i3+i7) * sine		; 24-27		n 31
	shr	r14, 16				;; Next 16 bits of fudge flags
	vmovapd zmm19, [screg+2*64]		;; premultiplier cosine/sine for R3/I3

	kmovw	k6, r14d			;; Load R6 and I6 fudge factor mask		; 25		n 55
	zfnmaddpd zmm2, zmm2, zmm26, zmm0	;; i1+- = (i1+i5) - (i3+i7) * sine		; 25-28		n 33
	shr	r14, 16				;; Next 16 bits of fudge flags
	vmovapd zmm18, [screg+4*64]		;; premultiplier cosine/sine for R5/I5

	kmovw	k7, r14d			;; Load R7 and I7 fudge factor mask		; 26		n 51
	zfmaddpd zmm0, zmm10, zmm26, zmm5	;; r1-+ = (r1-r5) + (i3-i7) * sine		; 26-29		n 34
	shr	r14, 16				;; Next 16 bits of fudge flags
	vmovapd zmm25, [screg+6*64]		;; premultiplier cosine/sine for R7/I7

	zfnmaddpd zmm10, zmm10, zmm26, zmm5	;; r1-- = (r1-r5) - (i3-i7) * sine		; 27-30		n 36
	zfmaddpd zmm5, zmm1, zmm26, zmm9	;; i1-+ = (i1-i5) + (r3-r7) * sine		; 27-30		n 37
	vmovapd zmm24, [screg+1*64]		;; premultiplier cosine/sine for R2/I2

	zfnmaddpd zmm1, zmm1, zmm26, zmm9	;; i1-- = (i1-i5) - (r3-r7) * sine		; 28-31		n 35
	vaddpd	zmm9, zmm3, zmm4		;; r2+-+ = (r2+-) + (i2-+)			; 28-31		n 34
	vmovapd zmm23, [screg+3*64]		;; premultiplier cosine/sine for R4/I4

	vsubpd	zmm3, zmm3, zmm4		;; r2+-- = (r2+-) - (i2-+)			; 29-32		n 36
	vaddpd	zmm4, zmm13, zmm6		;; r2-++ = (r2-+) + (i2+-)			; 29-32		n 37
	vmovapd zmm22, [screg+5*64]		;; premultiplier cosine/sine for R6/I6

	vsubpd	zmm13, zmm13, zmm6		;; r2-+- = (r2-+) - (i2+-)			; 30-33		n 35
	vaddpd	zmm6, zmm16, zmm11		;; R1 = (r1++) + (r2++)				; 30-33		n 38
	vmovapd zmm21, [screg+7*64]		;; premultiplier cosine/sine for R8/I8

	vsubpd	zmm16, zmm16, zmm11		;; R5 = (r1++) - (r2++)				; 31-34		n 40
	vaddpd	zmm11, zmm12, zmm15		;; I1 = (i1++) + (i2++)				; 31-34		n 38
	bump	screg, scinc
	L1prefetch srcreg+L1pd, L1pt

	vsubpd	zmm12, zmm12, zmm15		;; I5 = (i1++) - (i2++)				; 32-35		n 40
	vaddpd	zmm15, zmm8, zmm14		;; R3 = (r1+-) + (i2--)				; 32-35		n 39
	L1prefetch srcreg+64+L1pd, L1pt

	vsubpd	zmm8, zmm8, zmm14		;; R7 = (r1+-) - (i2--)				; 33-36		n 41
	vsubpd	zmm14, zmm2, zmm7		;; I3 = (i1+-) - (r2--)				; 33-36		n 39
	L1prefetch srcreg+d1+L1pd, L1pt

	vaddpd	zmm2, zmm2, zmm7		;; I7 = (i1+-) + (r2--)				; 34-37		n 41
	zfmaddpd zmm7, zmm9, zmm31, zmm0	;; R2 = (r1-+) + .707*(r2+-+)			; 34-37		n 42
	L1prefetch srcreg+d1+64+L1pd, L1pt

	zfnmaddpd zmm9, zmm9, zmm31, zmm0	;; R6 = (r1-+) - .707*(r2+-+)			; 35-38		n 44
	zfnmaddpd zmm0, zmm13, zmm31, zmm1	;; I2 = (i1--) - .707*(r2-+-)			; 35-38		n 42
	L1prefetch srcreg+d2+L1pd, L1pt

	zfmaddpd zmm13, zmm13, zmm31, zmm1	;; I6 = (i1--) + .707*(r2-+-)			; 36-39		n 44
	zfnmaddpd zmm1, zmm3, zmm31, zmm10	;; R4 = (r1--) - .707*(r2+--)			; 36-39		n 43
	L1prefetch srcreg+d2+64+L1pd, L1pt

	zfmaddpd zmm3, zmm3, zmm31, zmm10	;; R8 = (r1--) + .707*(r2+--)			; 37-40		n 45
	zfnmaddpd zmm10, zmm4, zmm31, zmm5	;; I4 = (i1-+) - .707*(r2-++)			; 37-40		n 43
	L1prefetch srcreg+d2+d1+L1pd, L1pt

	zfmaddpd zmm4, zmm4, zmm31, zmm5	;; I8 = (i1-+) + .707*(r2-++)			; 38-41		n 45
	zfmaddpd zmm5, zmm6, zmm20, zmm11	;; A1 = R1 * cosine + I1			; 38-41		n 46
	L1prefetch srcreg+d2+d1+64+L1pd, L1pt

	zfmsubpd zmm11, zmm11, zmm20, zmm6	;; B1 = I1 * cosine - R1			; 39-42		n 48
	zfmaddpd zmm6, zmm15, zmm19, zmm14	;; A3 = R3 * cosine + I3			; 39-42		n 47
	L1prefetch srcreg+d4+L1pd, L1pt

	zfmsubpd zmm14, zmm14, zmm19, zmm15	;; B3 = I3 * cosine - R3			; 40-43		n 50
	zfmaddpd zmm15, zmm16, zmm18, zmm12	;; A5 = R5 * cosine + I5			; 40-43		n 49
	L1prefetch srcreg+d4+64+L1pd, L1pt

	zfmsubpd zmm12, zmm12, zmm18, zmm16	;; B5 = I5 * cosine - R5			; 41-44		n 51
	zfmaddpd zmm16, zmm8, zmm25, zmm2	;; A7 = R7 * cosine + I7			; 41-44		n 51
	L1prefetch srcreg+d4+d1+L1pd, L1pt

	zfmsubpd zmm2, zmm2, zmm25, zmm8	;; B7 = I7 * cosine - R7			; 42-45		n 53
	zfmaddpd zmm8, zmm7, zmm24, zmm0	;; A2 = R2 * cosine + I2			; 42-45		n 52
	L1prefetch srcreg+d4+d1+64+L1pd, L1pt

	zfmsubpd zmm0, zmm0, zmm24, zmm7	;; B2 = I2 * cosine - R2			; 43-46		n 54
	zfmaddpd zmm7, zmm1, zmm23, zmm10	;; A4 = R4 * cosine + I4			; 43-46		n 54
	L1prefetch srcreg+d4+d2+L1pd, L1pt

	zfmsubpd zmm10, zmm10, zmm23, zmm1	;; B4 = I4 * cosine - R4			; 44-47		n 57
	zfmaddpd zmm1, zmm9, zmm22, zmm13	;; A6 = R6 * cosine + I6			; 44-47		n 55
	L1prefetch srcreg+d4+d2+64+L1pd, L1pt

	zfmsubpd zmm13, zmm13, zmm22, zmm9	;; B6 = I6 * cosine - R6			; 45-48		n 58
	zfmaddpd zmm9, zmm3, zmm21, zmm4	;; A8 = R8 * cosine + I8			; 45-48		n 56
	L1prefetch srcreg+d4+d2+d1+L1pd, L1pt

	zfmsubpd zmm4, zmm4, zmm21, zmm3	;; B8 = I8 * cosine - R8			; 46-49		n 58
	vmulpd	zmm5{k1}, zmm5, zmm28		;; apply fudge multiplier for R1		; 46-49
	L1prefetch srcreg+d4+d2+d1+64+L1pd, L1pt

	kshiftrw k1, k1, 8			;; I1's fudge					; 47		n 48
	vmulpd	zmm6{k3}, zmm6, zmm28		;; apply fudge multiplier for R3		; 47-50

	kshiftrw k3, k3, 8			;; I3's fudge					; 48		n 50
	vmulpd	zmm11{k1}, zmm11, zmm28		;; apply fudge multiplier for I1		; 48-51

	kmovw	k1, r14d			;; Load R8 and I8 fudge factor mask		; 49		n 56
	vmulpd	zmm15{k5}, zmm15, zmm28		;; apply fudge multiplier for R5		; 49-52

	kshiftrw k5, k5, 8			;; I5's fudge					; 50		n 51
	vmulpd	zmm14{k3}, zmm14, zmm28		;; apply fudge multiplier for I3		; 50-53
	zstore	[srcreg], zmm5			;; Save R1					; 50

	vmulpd	zmm16{k7}, zmm16, zmm28		;; apply fudge multiplier for R7		; 51-54
	vmulpd	zmm12{k5}, zmm12, zmm28		;; apply fudge multiplier for I5		; 51-54
	zstore	[srcreg+d2], zmm6		;; Save R3					; 51

	kshiftrw k7, k7, 8			;; I7's fudge					; 52		n 53
	vmulpd	zmm8{k2}, zmm8, zmm28		;; apply fudge multiplier for R2		; 52-55
	zstore	[srcreg+64], zmm11		;; Save I1					; 52

	kshiftrw k2, k2, 8			;; I2's fudge					; 53		n 54
	vmulpd	zmm2{k7}, zmm2, zmm28		;; apply fudge multiplier for I7		; 53-56
	zstore	[srcreg+d4], zmm15		;; Save R5					; 53

	vmulpd	zmm0{k2}, zmm0, zmm28		;; apply fudge multiplier for I2		; 54-57
	vmulpd	zmm7{k4}, zmm7, zmm28		;; apply fudge multiplier for R4		; 54-57
	zstore	[srcreg+d2+64], zmm14		;; Save I3					; 54

	kshiftrw k4, k4, 8			;; I4's fudge					; 55		n 57
	vmulpd	zmm1{k6}, zmm1, zmm28		;; apply fudge multiplier for R6		; 55-58
	zstore	[srcreg+d4+d2], zmm16		;; Save R7					; 55

	kshiftrw k6, k6, 8			;; I6's fudge					; 56		n 58
	vmulpd	zmm9{k1}, zmm9, zmm28		;; apply fudge multiplier for R8		; 56-59
	zstore	[srcreg+d4+64], zmm12		;; Save I5					; 55+1

	kshiftrw k1, k1, 8			;; I8's fudge					; 57		n 58
	vmulpd	zmm10{k4}, zmm10, zmm28		;; apply fudge multiplier for I4		; 57-60
	zstore	[srcreg+d1], zmm8		;; Save R2					; 56+1

	vmulpd	zmm13{k6}, zmm13, zmm28		;; apply fudge multiplier for I6		; 58-61
	vmulpd	zmm4{k1}, zmm4, zmm28		;; apply fudge multiplier for I8		; 58-61
	zstore	[srcreg+d4+d2+64], zmm2		;; Save I7					; 57+1

	zstore	[srcreg+d1+64], zmm0		;; Save I2					; 58+1
	zstore	[srcreg+d2+d1], zmm7		;; Save R4					; 58+2
	zstore	[srcreg+d4+d1], zmm1		;; Save R6					; 59+2
	zstore	[srcreg+d4+d2+d1], zmm9		;; Save R8					; 60+2
	zstore	[srcreg+d2+d1+64], zmm10	;; Save I4					; 61+2
	zstore	[srcreg+d4+d1+64], zmm13	;; Save I6					; 62+2
	zstore	[srcreg+d4+d2+d1+64], zmm4	;; Save I8					; 62+3
	bump	srcreg, srcinc
	ENDM


;;
;; ********************************* reduced sin/cos eight-complex-fft8 variants **************************************
;;
;; These macros are used in the last levels of pass 1.  These macros differ from the standard eight-complex macros
;; in that the sin/cos values are computed on the fly to reduce memory requirements.  To do this we cannot
;; store data as (cos/sin,sin), these macros work on (cos,sin) values. These macros also apply a column weights multiplier.

zr8_rsc_wpn_eight_complex_calc_sincos MACRO weights, src1, src2, clm
	zr8_rsc_wpn_eight_complex_calc_sincos1 weights+0*64, src1+0*ZMM_SCD1, src2+0*ZMM_SCD4, ZMM_TMPS[0*ZMM_SCD8]
	IF clm GE 2
	zr8_rsc_wpn_eight_complex_calc_sincos1 weights+1*64, src1+1*ZMM_SCD1, src2+1*ZMM_SCD4, ZMM_TMPS[1*ZMM_SCD8]
	ENDIF
	IF clm GE 4
	zr8_rsc_wpn_eight_complex_calc_sincos1 weights+2*64, src1+2*ZMM_SCD1, src2+2*ZMM_SCD4, ZMM_TMPS[2*ZMM_SCD8]
	zr8_rsc_wpn_eight_complex_calc_sincos1 weights+3*64, src1+3*ZMM_SCD1, src2+3*ZMM_SCD4, ZMM_TMPS[3*ZMM_SCD8]
	ENDIF
	ENDM

zr8_rsc_wpn_eight_complex_calc_sincos1 MACRO weights, src1, src2, dest
	vmovapd	zmm0, [src1+64]		;; cos
	vmovapd	zmm1, [src1]		;; sin
	vmulpd	zmm0, zmm0, [weights]	;; Apply the column weight		; 1-4
	vmulpd	zmm1, zmm1, [weights]	;; Apply the column weight		; 1-4
;; BUG - interleave the clm=2 and clm=4 cases here to hide this stall
	vmovapd	zmm2, [src2+0*128+64]	;; cos1
	vmulpd	zmm3, zmm0, zmm2	;; ac1 = cos * cos1			; 5-8
	vmulpd	zmm2, zmm1, zmm2	;; bc1 = sin * cos1			; 5-8
	vmovapd	zmm4, [src2+1*128+64]	;; cos2
	vmulpd	zmm5, zmm0, zmm4	;; ac2 = cos * cos2			; 6-9
	vmulpd	zmm4, zmm1, zmm4	;; bc2 = sin * cos2			; 6-9
	vmovapd	zmm6, [src2+2*128+64]	;; cos3
	vmulpd	zmm7, zmm0, zmm6	;; ac3 = cos * cos3			; 7-10
	vmulpd	zmm6, zmm1, zmm6	;; bc3 = sin * cos3			; 7-10
	vmovapd	zmm8, [src2+3*128+64]	;; cos4
	vmulpd	zmm9, zmm0, zmm8	;; ac4 = cos * cos4			; 8-11
	vmulpd	zmm8, zmm1, zmm8	;; bc4 = sin * cos4			; 8-11
	zstore	dest[0*128+64], zmm0
	zstore	dest[0*128], zmm1
	vmovapd	zmm15, [src2+0*128]	;; sin1
	zfnmaddpd zmm10, zmm1, zmm15, zmm3 ;; new cos1 = ac1 - sin * sin1	; 9-12
	zfmaddpd zmm3, zmm1, zmm15, zmm3 ;; new cos7 = ac1 + sin * sin1		; 9-12
	zfmaddpd zmm11, zmm0, zmm15, zmm2 ;; new sin1 = cos * sin1 + bc1	; 10-13
	zfnmaddpd zmm2, zmm0, zmm15, zmm2 ;; new sin7 = bc1 - cos * sin1	; 10-13
	vmovapd	zmm15, [src2+1*128]	;; sin2
	zfnmaddpd zmm12, zmm1, zmm15, zmm5 ;; new cos2 = ac2 - sin * sin2	; 11-14
	zfmaddpd zmm5, zmm1, zmm15, zmm5 ;; new cos6 = ac2 + sin * sin2		; 11-14
	zfmaddpd zmm13, zmm0, zmm15, zmm4 ;; new sin2 = cos * sin2 + bc2	; 12-15
	zfnmaddpd zmm4, zmm0, zmm15, zmm4 ;; new sin6 = bc2 - cos * sin2	; 12-15
	vmovapd	zmm15, [src2+2*128]	;; sin3
	zfnmaddpd zmm14, zmm1, zmm15, zmm7 ;; new cos3 = ac3 - sin * sin3	; 13-16
	zfmaddpd zmm7, zmm1, zmm15, zmm7 ;; new cos5 = ac3 + sin * sin3		; 13-16
	zstore	dest[1*128+64], zmm10	;; cos1
	zfmaddpd zmm10, zmm0, zmm15, zmm6 ;; new sin3 = cos * sin3 + bc3	; 14-17
	zfnmaddpd zmm6, zmm0, zmm15, zmm6 ;; new sin5 = bc3 - cos * sin3	; 14-17
	vmovapd	zmm15, [src2+3*128]	;; sin4
	zstore	dest[7*128+64], zmm3	;; cos7
	zfnmaddpd zmm9, zmm1, zmm15, zmm9 ;; new cos4 = ac4 - sin * sin4	; 15-18
	zfmaddpd zmm8, zmm0, zmm15, zmm8 ;; new sin4 = cos * sin4 + bc4		; 15-18
	zstore	dest[1*128], zmm11	;; sin1
	zstore	dest[7*128], zmm2	;; sin7
	zstore	dest[2*128+64], zmm12	;; cos2
	zstore	dest[6*128+64], zmm5	;; cos6
	zstore	dest[2*128], zmm13	;; sin2
	zstore	dest[6*128], zmm4	;; sin6
	zstore	dest[3*128], zmm10	;; sin3
	zstore	dest[3*128+64], zmm14	;; cos3
	zstore	dest[4*128], zmm8	;; sin4
	zstore	dest[4*128+64], zmm9	;; cos4
	zstore	dest[5*128], zmm6	;; sin5
	zstore	dest[5*128+64], zmm7	;; cos5
	ENDM

zr8_rsc_wpn_eight_complex_calc_sincos_simple MACRO weights, src2, clm
	zr8_rsc_wpn_eight_complex_calc_sincos_simple1 weights+0*64, src2+0*ZMM_SCD4, ZMM_TMPS[0*ZMM_SCD8]
	IF clm GE 2
	zr8_rsc_wpn_eight_complex_calc_sincos_simple1 weights+1*64, src2+1*ZMM_SCD4, ZMM_TMPS[1*ZMM_SCD8]
	ENDIF
	IF clm GE 4
	zr8_rsc_wpn_eight_complex_calc_sincos_simple1 weights+2*64, src2+2*ZMM_SCD4, ZMM_TMPS[2*ZMM_SCD8]
	zr8_rsc_wpn_eight_complex_calc_sincos_simple1 weights+3*64, src2+3*ZMM_SCD4, ZMM_TMPS[3*ZMM_SCD8]
	ENDIF
	ENDM

zr8_rsc_wpn_eight_complex_calc_sincos_simple1 MACRO weights, src2, dest
	vmovapd	zmm0, [weights]			;; cos (1) * column weight
	vpxorq	zmm1, zmm1, zmm1		;; sin (0) * column weight
	zstore	dest[0*128+64], zmm0
	zstore	dest[0*128], zmm1

	vmulpd	zmm2, zmm0, [src2+0*128+64]	;; cos * column weight
	vmulpd	zmm3, zmm0, [src2+0*128]	;; sin * column weight
	zstore	dest[1*128+64], zmm2	
	zstore	dest[1*128], zmm3
	zstore	dest[7*128+64], zmm2
	vsubpd	zmm3, zmm1, zmm3		;; - sin * column weight
	zstore	dest[7*128], zmm3

	vmulpd	zmm2, zmm0, [src2+1*128+64]	;; cos * column weight
	vmulpd	zmm3, zmm0, [src2+1*128]	;; sin * column weight
	zstore	dest[2*128+64], zmm2
	zstore	dest[2*128], zmm3
	zstore	dest[6*128+64], zmm2
	vsubpd	zmm3, zmm1, zmm3		;; - sin * column weight
	zstore	dest[6*128], zmm3

	vmulpd	zmm2, zmm0, [src2+2*128+64]	;; cos * column weight
	vmulpd	zmm3, zmm0, [src2+2*128]	;; sin * column weight
	zstore	dest[3*128+64], zmm2
	zstore	dest[3*128], zmm3
	zstore	dest[5*128+64], zmm2
	vsubpd	zmm3, zmm1, zmm3		;; - sin * column weight
	zstore	dest[5*128], zmm3

	vmulpd	zmm2, zmm0, [src2+3*128+64]	;; cos * column weight
	vmulpd	zmm3, zmm0, [src2+3*128]	;; sin * column weight
	zstore	dest[4*128+64], zmm2
	zstore	dest[4*128], zmm3
	ENDM


;; Use registers for distances between output blocks.  This lets us share pass1 code.
;; Faster, but less readable version, that integrates shuffles with computation

zr8_rsc_complex_only_preload MACRO
	mov	eax, 00110011b			;; For vblendmpd during swizzle
	kmovw	k7, eax
	vpmovzxbq zmm31, ZMM_PERMUTE1		;; zmm31 = 8+4 0+4 8+6 0+6 8+0 0+0 8+2 0+2 [msw at left]
	vpmovzxbq zmm30, ZMM_PERMUTE2		;; zmm30 = 8+5 0+5 8+7 0+7 8+1 0+1 8+3 0+3
	vbroadcastsd zmm29, ZMM_SQRTHALF
	ENDM

zr8_rsc_wpn_sgreg_eight_complex_fft8_preload MACRO
	use zr8_rsc_complex_only_preload or zr8_rsc_complex_and_real_preload
	ENDM

zr8_rsc_wpn_sgreg_eight_complex_fft8 MACRO srcreg,srcinc,d1,d2,d4,dstreg,dstinc,e1reg,e3reg,dst4reg,screg,scinc,maxrpt,L1pt,L1preg,L1p4reg
	vmovapd	zmm3, [srcreg+d2+64]		;; i3_7	i3_6 i3_5 i3_4 i3_3 i3_2 i3_1 i3_0
	vmovapd	zmm28, [srcreg+d2+d1+64]	;; i4_7	i4_6 i4_5 i4_4 i4_3 i4_2 i4_1 i4_0
	vmovapd	zmm2, zmm3
	vpermt2pd zmm2, zmm31, zmm28		;; i4_4 i3_4 i4_6 i3_6 i4_0 i3_0 i4_2 i3_2		; 1-3		n 5
	vpermt2pd zmm3, zmm30, zmm28		;; i4_5 i3_5 i4_7 i3_7 i4_1 i3_1 i4_3 i3_3		; 2-4		n 7

	vmovapd	zmm0, [srcreg+64]		;; i1_7	i1_6 i1_5 i1_4 i1_3 i1_2 i1_1 i1_0
	vmovapd	zmm28, [srcreg+d1+64]		;; i2_7	i2_6 i2_5 i2_4 i2_3 i2_2 i2_1 i2_0
	vshufpd	zmm1, zmm0, zmm28, 11111111b	;; i2_7 i1_7 i2_5 i1_5 i2_3 i1_3 i2_1 i1_1		; 3		n 7
	vshufpd	zmm0, zmm0, zmm28, 00000000b	;; i2_6 i1_6 i2_4 i1_4 i2_2 i1_2 i2_0 i1_0		; 4		n 5

	vmovapd	zmm7, [srcreg+d4+d2+64]		;; i7_7	i7_6 i7_5 i7_4 i7_3 i7_2 i7_1 i7_0
	vmovapd	zmm28, [srcreg+d4+d2+d1+64]	;; i8_7	i8_6 i8_5 i8_4 i8_3 i8_2 i8_1 i8_0
	vmovapd zmm6, zmm7
	vpermt2pd zmm6, zmm31, zmm28		;; i8_4 i7_4 i8_6 i7_6 i8_0 i7_0 i8_2 i7_2		; 5-7		n 9
	vblendmpd zmm16{k7}, zmm2, zmm0		;; i4_4 i3_4 i2_4 i1_4 i4_0 i3_0 i2_0 i1_0		;  5		n 17

	vpermt2pd zmm7, zmm30, zmm28		;; i8_5 i7_5 i8_7 i7_7 i8_1 i7_1 i8_3 i7_3		; 6-8		n 11
	vblendmpd zmm0{k7}, zmm0, zmm2		;; i2_6 i1_6 i4_6 i3_6 i2_2 i1_2 i4_2 i3_2		;  6		n 19

	vmovapd	zmm5, [srcreg+d4+64]		;; i5_7	i5_6 i5_5 i5_4 i5_3 i5_2 i5_1 i5_0
	vmovapd	zmm28, [srcreg+d4+d1+64]	;; i6_7	i6_6 i6_5 i6_4 i6_3 i6_2 i6_1 i6_0
	vshufpd	zmm4, zmm5, zmm28, 00000000b	;; i6_6 i5_6 i6_4 i5_4 i6_2 i5_2 i6_0 i5_0		; 7		n 9
	vblendmpd zmm2{k7}, zmm3, zmm1		;; i4_5 i3_5 i2_5 i1_5 i4_1 i3_1 i2_1 i1_1		;  7		n 21

	vshufpd	zmm5, zmm5, zmm28, 11111111b	;; i6_7 i5_7 i6_5 i5_5 i6_3 i5_3 i6_1 i5_1		; 8		n 11
	vblendmpd zmm1{k7}, zmm1, zmm3		;; i2_7 i1_7 i4_7 i3_7 i2_3 i1_3 i4_3 i3_3		;  8		n 23

	vmovapd	zmm15, [srcreg+d4+d2]		;; r7_7	r7_6 r7_5 r7_4 r7_3 r7_2 r7_1 r7_0
	vmovapd	zmm28, [srcreg+d4+d2+d1]	;; r8_7	r8_6 r8_5 r8_4 r8_3 r8_2 r8_1 r8_0
	vmovapd zmm14, zmm15
	vpermt2pd zmm14, zmm31, zmm28		;; r8_4 r7_4 r8_6 r7_6 r8_0 r7_0 r8_2 r7_2		; 9-11		n 13
	vblendmpd zmm3{k7}, zmm6, zmm4		;; i8_4 i7_4 i6_4 i5_4 i8_0 i7_0 i6_0 i5_0		;  9		n 17

	vpermt2pd zmm15, zmm30, zmm28		;; r8_5 r7_5 r8_7 r7_7 r8_1 r7_1 r8_3 r7_3		; 10-12		n 15
	vblendmpd zmm4{k7}, zmm4, zmm6		;; i6_6 i5_6 i8_6 i7_6 i6_2 i5_2 i8_2 i7_2		;  10		n 19

	vmovapd	zmm13, [srcreg+d4]		;; r5_7	r5_6 r5_5 r5_4 r5_3 r5_2 r5_1 r5_0
	vmovapd	zmm28, [srcreg+d4+d1]		;; r6_7	r6_6 r6_5 r6_4 r6_3 r6_2 r6_1 r6_0
	vshufpd	zmm12, zmm13, zmm28, 00000000b	;; r6_6 r5_6 r6_4 r5_4 r6_2 r5_2 r6_0 r5_0		; 11		n 13
	vblendmpd zmm6{k7}, zmm7, zmm5		;; i8_5 i7_5 i6_5 i5_5 i8_1 i7_1 i6_1 i5_1		;  11		n 21

	vshufpd	zmm13, zmm13, zmm28, 11111111b	;; r6_7 r5_7 r6_5 r5_5 r6_3 r5_3 r6_1 r5_1		; 12		n 15
	vblendmpd zmm5{k7}, zmm5, zmm7		;; i6_7 i5_7 i8_7 i7_7 i6_3 i5_3 i8_3 i7_3		;  12		n 23

	vmovapd	zmm11, [srcreg+d2]		;; r3_7	r3_6 r3_5 r3_4 r3_3 r3_2 r3_1 r3_0
	vmovapd	zmm28, [srcreg+d2+d1]		;; r4_7	r4_6 r4_5 r4_4 r4_3 r4_2 r4_1 r4_0
	vmovapd	zmm10, zmm11
	vpermt2pd zmm10, zmm31, zmm28		;; r4_4 r3_4 r4_6 r3_6 r4_0 r3_0 r4_2 r3_2		; 13-15		n 17
	vblendmpd zmm7{k7}, zmm14, zmm12	;; r8_4 r7_4 r6_4 r5_4 r8_0 r7_0 r6_0 r5_0		;  13		n 29

	vpermt2pd zmm11, zmm30, zmm28		;; r4_5 r3_5 r4_7 r3_7 r4_1 r3_1 r4_3 r3_3		; 14-16		n 19
	vblendmpd zmm12{k7}, zmm12, zmm14	;; r6_6 r5_6 r8_6 r7_6 r6_2 r5_2 r8_2 r7_2		;  14		n 31

	vmovapd	zmm9, [srcreg]			;; r1_7	r1_6 r1_5 r1_4 r1_3 r1_2 r1_1 r1_0
	vmovapd	zmm28, [srcreg+d1]		;; r2_7	r2_6 r2_5 r2_4 r2_3 r2_2 r2_1 r2_0
	vshufpd	zmm8, zmm9, zmm28, 00000000b	;; r2_6 r1_6 r2_4 r1_4 r2_2 r1_2 r2_0 r1_0		; 15		n 17
	vblendmpd zmm14{k7}, zmm15, zmm13	;; r8_5 r7_5 r6_5 r6_5 r8_1 r7_1 r6_1 r5_1		;  15		n 27

	vshufpd	zmm9, zmm9, zmm28, 11111111b	;; r2_7 r1_7 r2_5 r1_5 r2_3 r1_3 r2_1 r1_1		; 16		n 19
	vblendmpd zmm13{k7}, zmm13, zmm15	;; r6_7 r5_7 r8_7 r7_7 r6_3 r5_3 r8_3 r7_3		;  16		n 25
	vmovapd	zmm25, [screg+0*128+64]		;; cosine for R1/I1
	bump	srcreg, srcinc

	vshuff64x2 zmm17, zmm16, zmm3, 01000100b;; i8_0 i7_0 i6_0 i5_0 i4_0 i3_0 i2_0 i1_0		; 17-19		n 21
	vblendmpd zmm15{k7}, zmm10, zmm8	;; r4_4 r3_4 r2_4 r1_4 r4_0 r3_0 r2_0 r1_0		;  17		n 29
	vmovapd	zmm28, [screg+4*128+64]		;; cosine for R2/I2

	vshuff64x2 zmm16, zmm16, zmm3, 11101110b;; i8_4 i7_4 i6_4 i5_4 i4_4 i3_4 i2_4 i1_4		; 18-20		n 21
	vblendmpd zmm8{k7}, zmm8, zmm10		;; r2_6 r1_6 r4_6 r3_6 r2_2 r1_2 r4_2 r3_2		;  18		n 31
	vmovapd	zmm27, [screg+2*128+64]		;; cosine for R3/I3

	vshuff64x2 zmm3, zmm0, zmm4, 00010001b	;; i8_2 i7_2 i6_2 i5_2 i4_2 i3_2 i2_2 i1_2		; 19-21		n 23
	vblendmpd zmm10{k7}, zmm11, zmm9	;; r4_5 r3_5 r2_5 r1_5 r4_1 r3_1 r2_1 r1_1		;  19		n 27
	vmovapd	zmm26, [screg+6*128+64]		;; cosine for R4/I4

	vshuff64x2 zmm0, zmm0, zmm4, 10111011b	;; i8_6 i7_6 i6_6 i5_6 i4_6 i3_6 i2_6 i1_6		; 20-22		n 23
	vblendmpd zmm9{k7}, zmm9, zmm11		;; r2_7 r1_7 r4_7 r3_7 r2_3 r1_3 r4_3 r3_3		;  20		n 25
	vmovapd	zmm24, [screg+0*128]		;; sine for R1/I1

	vshuff64x2 zmm4, zmm2, zmm6, 01000100b	;; i8_1 i7_1 i6_1 i5_1 i4_1 i3_1 i2_1 i1_1		; 21-23		n 25
	vshuff64x2 zmm2, zmm2, zmm6, 11101110b	;; i8_5 i7_5 i6_5 i5_5 i4_5 i3_5 i2_5 i1_5		; 22-24		n 25
	vmovapd	zmm23, [screg+4*128]		;; sine for R2/I2

	vaddpd	zmm11, zmm17, zmm16		;; I1 + I5 (new I1)					; 21-24		n 38
	vsubpd	zmm17, zmm17, zmm16		;; I1 - I5 (new I5)					; 22-25		n 46
	vmovapd	zmm22, [screg+2*128]		;; sine for R3/I3

	vshuff64x2 zmm6, zmm1, zmm5, 00010001b	;; i8_3 i7_3 i6_3 i5_3 i4_3 i3_3 i2_3 i1_3		; 23-25		n 27
	vshuff64x2 zmm1, zmm1, zmm5, 10111011b	;; i8_7 i7_7 i6_7 i5_7 i4_7 i3_7 i2_7 i1_7		; 24-26		n 27
	vmovapd	zmm21, [screg+6*128]		;; sine for R4/I4

	vaddpd	zmm16, zmm3, zmm0		;; I3 + I7 (new I3)					; 23-26		n 38
	vsubpd	zmm3, zmm3, zmm0		;; I3 - I7 (new I7)					; 24-27		n 42
	L1prefetchw dstreg, L1pt - L1PREFETCH_DEST_NONE

	vshuff64x2 zmm5, zmm9, zmm13, 00010001b	;; r8_3 r7_3 r6_3 r5_3 r4_3 r3_3 r2_3 r1_3		; 25-27		n 29
	vshuff64x2 zmm9, zmm9, zmm13, 10111011b ;; r8_7 r7_7 r6_7 r5_7 r4_7 r3_7 r2_7 r1_7		; 26-28		n 29
	L1prefetchw dstreg+64, L1pt - L1PREFETCH_DEST_NONE

	vsubpd	zmm0, zmm4, zmm2		;; I2 - I6 (new I6)					; 25-28		n 34
	vaddpd	zmm4, zmm4, zmm2		;; I2 + I6 (new I2)					; 26-29		n 37
	L1prefetchw dstreg+e1reg, L1pt - L1PREFETCH_DEST_NONE

	vshuff64x2 zmm13, zmm10, zmm14, 01000100b ;; r8_1 r7_1 r6_1 r5_1 r4_1 r3_1 r2_1 r1_1		; 27-29		n 31
	vshuff64x2 zmm10, zmm10, zmm14, 11101110b ;; r8_5 r7_5 r6_5 r5_5 r4_5 r3_5 r2_5 r1_5		; 28-30		n 31
	L1prefetchw dstreg+e1reg+64, L1pt - L1PREFETCH_DEST_NONE

	vsubpd	zmm2, zmm6, zmm1		;; I4 - I8 (new I8)					; 27-30		n 35
	vaddpd	zmm6, zmm6, zmm1		;; I4 + I8 (new I4)					; 28-31		n 37
	L1prefetchw dstreg+2*e1reg, L1pt - L1PREFETCH_DEST_NONE

	vshuff64x2 zmm14, zmm15, zmm7, 01000100b;; r8_0 r7_0 r6_0 r5_0 r4_0 r3_0 r2_0 r1_0		; 29-31		n 33
	vshuff64x2 zmm15, zmm15, zmm7, 11101110b;; r8_4 r7_4 r6_4 r5_4 r4_4 r3_4 r2_4 r1_4		; 30-32		n 33
	L1prefetchw dstreg+2*e1reg+64, L1pt - L1PREFETCH_DEST_NONE

	vsubpd	zmm1, zmm5, zmm9		;; R4 - R8 (new R8)					; 29-32		n 34
	vaddpd	zmm5, zmm5, zmm9		;; R4 + R8 (new R4)					; 30-33		n 39
	L1prefetchw dstreg+e3reg, L1pt - L1PREFETCH_DEST_NONE

	vshuff64x2 zmm7, zmm8, zmm12, 00010001b ;; r8_2 r7_2 r6_2 r5_2 r4_2 r3_2 r2_2 r1_2		; 31-33		n 36
	vshuff64x2 zmm8, zmm8, zmm12, 10111011b ;; r8_6 r7_6 r6_6 r5_6 r4_6 r3_6 r2_6 r1_6		; 32-34		n 36
	L1prefetchw dstreg+e3reg+64, L1pt - L1PREFETCH_DEST_NONE

	vsubpd	zmm9, zmm13, zmm10		;; R2 - R6 (new R6)					; 31-34		n 35
	vaddpd	zmm13, zmm13, zmm10		;; R2 + R6 (new R2)					; 32-35		n 39
	L1prefetchw dst4reg, L1pt - L1PREFETCH_DEST_NONE

	vsubpd	zmm10, zmm14, zmm15		;; R1 - R5 (new R5)					; 33-36		n 42
	vaddpd	zmm14, zmm14, zmm15		;; R1 + R5 (new R1)					; 33-36		n 45
	L1prefetchw dst4reg+64, L1pt - L1PREFETCH_DEST_NONE

	vaddpd	zmm15, zmm0, zmm1		;; I6 + R8 (new2 I6)					; 34-37		n 41
	vsubpd	zmm0, zmm0, zmm1		;; I6 - R8 (new2 I8)					; 34-37		n 40
	L1prefetchw dst4reg+e1reg, L1pt - L1PREFETCH_DEST_NONE

	vaddpd	zmm1, zmm9, zmm2		;; R6 + I8 (new2 R8)					; 35-38		n 40
	vsubpd	zmm9, zmm9, zmm2		;; R6 - I8 (new2 R6)					; 35-38		n 41
	L1prefetchw dst4reg+e1reg+64, L1pt - L1PREFETCH_DEST_NONE

	vaddpd	zmm2, zmm7, zmm8		;; R3 + R7 (new R3)					; 36-39		n 45
	vsubpd	zmm7, zmm7, zmm8		;; R3 - R7 (new R7)					; 36-39		n 46
	L1prefetchw dst4reg+2*e1reg, L1pt - L1PREFETCH_DEST_NONE

	vaddpd	zmm8, zmm4, zmm6		;; I2 + I4 (newer I2)					; 37-40		n 43
	vsubpd	zmm4, zmm4, zmm6		;; I2 - I4 (newer I4)					; 37-40		n 51
	L1prefetchw dst4reg+2*e1reg+64, L1pt - L1PREFETCH_DEST_NONE

	vaddpd	zmm6, zmm11, zmm16		;; I1 + I3 (newer I1)					; 38-41		n 43
	vsubpd	zmm11, zmm11, zmm16		;; I1 - I3 (newer I3)					; 38-41		n 44
	L1prefetchw dst4reg+e3reg, L1pt - L1PREFETCH_DEST_NONE

	vsubpd	zmm16, zmm13, zmm5		;; R2 - R4 (newer R4)					; 39-42		n 44
	vaddpd	zmm13, zmm13, zmm5		;; R2 + R4 (newer R2)					; 39-42		n 50
	L1prefetchw dst4reg+e3reg+64, L1pt - L1PREFETCH_DEST_NONE

	vaddpd	zmm5, zmm1, zmm0		;; R8 + I8 (newer I8/SQRTHALF)				; 40-43		n 48
	vsubpd	zmm1, zmm1, zmm0		;; R8 - I8 (newer R8/SQRTHALF)				; 40-43		n 53

	vsubpd	zmm0, zmm9, zmm15		;; R6 - I6 (newer R6/SQRTHALF)				; 41-44		n 47
	vaddpd	zmm9, zmm9, zmm15		;; R6 + I6 (newer I6/SQRTHALF)				; 41-44		n 52

	vsubpd	zmm15, zmm10, zmm3		;; R5 - I7 (newer R5)					; 42-45		n 47
	vaddpd	zmm10, zmm10, zmm3		;; R5 + I7 (newer R7)					; 42-45		n 48

	vaddpd	zmm3, zmm6, zmm8		;; I1 + I2 (last I1)					; 43-46		n 49
	vsubpd	zmm6, zmm6, zmm8		;; I1 - I2 (last I2)					; 43-46		n 49

	vaddpd	zmm8, zmm11, zmm16		;; I3 + R4 (last I3)					; 44-47		n 54
	vsubpd	zmm11, zmm11, zmm16		;; I3 - R4 (last I4)					; 44-47		n 54

	vaddpd	zmm16, zmm14, zmm2		;; R1 + R3 (newer R1)					; 45-48		n 50
	vsubpd	zmm14, zmm14, zmm2		;; R1 - R3 (newer R3)					; 45-48		n 51

 	vaddpd	zmm2, zmm17, zmm7		;; I5 + R7 (newer I5)					; 46-49		n 52
	vsubpd	zmm17, zmm17, zmm7		;; I5 - R7 (newer I7)					; 46-49		n 53

	zfmaddpd zmm7, zmm0, zmm29, zmm15	;; R5 + R6 * SQRTHALF (last R5)				; 47-50		n 61
	zfnmaddpd zmm0, zmm0, zmm29, zmm15	;; R5 - R6 * SQRTHALF (last R6)				; 47-50		n 62

	zfnmaddpd zmm15, zmm5, zmm29, zmm10	;; R7 - I8 * SQRTHALF (last R7)				; 48-51		n 63
	zfmaddpd zmm5, zmm5, zmm29, zmm10	;; R7 + I8 * SQRTHALF (last R8)				; 48-51		n 64

	vmulpd	zmm12, zmm3, zmm25		;; B1 = I1 * cosine					; 49-52		n 57
	vmulpd	zmm18, zmm6, zmm28		;; B2 = I2 * cosine					; 49-52		n 57

	vaddpd	zmm10, zmm16, zmm13		;; R1 + R2 (last R1)					; 50-53		n 55
	vsubpd	zmm16, zmm16, zmm13		;; R1 - R2 (last R2)					; 50-53		n 55

	vsubpd	zmm13, zmm14, zmm4		;; R3 - I4 (last R3)					; 51-54		n 56
	vaddpd	zmm14, zmm14, zmm4		;; R3 + I4 (last R4)					; 51-54		n 56

	zfmaddpd zmm4, zmm9, zmm29, zmm2	;; I5 + I6 * SQRTHALF (last I5)				; 52-55		n 61
	zfnmaddpd zmm9, zmm9, zmm29, zmm2	;; I5 - I6 * SQRTHALF (last I6)				; 52-55		n 62

	zfmaddpd zmm2, zmm1, zmm29, zmm17	;; I7 + R8 * SQRTHALF (last I7)				; 53-56		n 63
	zfnmaddpd zmm1, zmm1, zmm29, zmm17	;; I7 - R8 * SQRTHALF (last I8)				; 53-56		n 64
	vmovapd	zmm17, [screg+1*128+64]		;; cosine for R5/I5

	vmulpd	zmm19, zmm8, zmm27		;; B3 = I3 * cosine					; 54-57		n 58
	vmulpd	zmm20, zmm11, zmm26		;; B4 = I4 * cosine					; 54-57		n 58

	vmulpd	zmm25, zmm10, zmm25		;; A1 = R1 * cosine					; 55-58		n 59
	vmulpd	zmm28, zmm16, zmm28		;; A2 = R2 * cosine					; 55-58		n 59

	vmulpd	zmm27, zmm13, zmm27		;; A3 = R3 * cosine					; 56-59		n 60
	vmulpd	zmm26, zmm14, zmm26		;; A4 = R4 * cosine					; 56-59		n 60

	zfmaddpd zmm10, zmm10, zmm24, zmm12	;; B1 + R1 * sine (final I1)				; 57-60
	zfmaddpd zmm16, zmm16, zmm23, zmm18	;; B2 + R2 * sine (final I2)				; 57-60
	vmovapd	zmm12, [screg+5*128+64]		;; cosine for R6/I6

	zfmaddpd zmm13, zmm13, zmm22, zmm19	;; B3 + R3 * sine (final I3)				; 58-61
	zfmaddpd zmm14, zmm14, zmm21, zmm20	;; B4 + R4 * sine (final I4)				; 58-61
	vmovapd	zmm18, [screg+3*128+64]		;; cosine for R7/I7

	zfnmaddpd zmm3, zmm3, zmm24, zmm25	;; A1 - I1 * sine (final R1)				; 59-62
	zfnmaddpd zmm6, zmm6, zmm23, zmm28	;; A2 - I2 * sine (final R2)				; 59-62
	vmovapd	zmm19, [screg+7*128+64]		;; cosine for R8/I8

	zfnmaddpd zmm8, zmm8, zmm22, zmm27	;; A3 - I3 * sine (final R3)				; 60-63
	zfnmaddpd zmm11, zmm11, zmm21, zmm26	;; A4 - I4 * sine (final R4)				; 60-63
	vmovapd	zmm20, [screg+1*128]		;; sine for R5/I5

	vmulpd	zmm22, zmm7, zmm17		;; A5 = R5 * cosine					; 61-64		n 65
	vmulpd	zmm17, zmm4, zmm17		;; B5 = I5 * cosine					; 61-64		n 65
	vmovapd	zmm25, [screg+5*128]		;; sine for R6/I6
	zstore	[dstreg+64], zmm10		;; Save I1						; 61

	vmulpd	zmm27, zmm0, zmm12		;; A6 = R6 * cosine					; 62-65		n 66
	vmulpd	zmm12, zmm9, zmm12		;; B6 = I6 * cosine					; 62-65		n 66
	vmovapd	zmm24, [screg+3*128]		;; sine for R7/I7
	zstore	[dstreg+e1reg+64], zmm16	;; Save I2						; 61+1

	vmulpd	zmm21, zmm15, zmm18		;; A7 = R7 * cosine					; 63-66		n 67
	vmulpd	zmm18, zmm2, zmm18		;; B7 = I7 * cosine					; 63-66		n 67
	vmovapd	zmm28, [screg+7*128]		;; sine for R8/I8
	zstore	[dstreg+2*e1reg+64], zmm13	;; Save I3						; 62+1

	vmulpd	zmm26, zmm5, zmm19		;; A8 = R8 * cosine					; 64-67		n 68
	vmulpd	zmm19, zmm1, zmm19		;; B8 = I8 * cosine					; 64-67		n 68
	bump	screg, scinc
	zstore	[dstreg+e3reg+64], zmm14	;; Save I4						; 62+2

	zfnmaddpd zmm4, zmm4, zmm20, zmm22	;; A5 - I5 * sine (final R5)				; 65-68
	zfmaddpd zmm7, zmm7, zmm20, zmm17	;; B5 + R5 * sine (final I5)				; 65-68
	zstore	[dstreg], zmm3			;; Save R1						; 63+2

	zfnmaddpd zmm9, zmm9, zmm25, zmm27	;; A6 - I6 * sine (final R6)				; 66-69
	zfmaddpd zmm0, zmm0, zmm25, zmm12	;; B6 + R6 * sine (final I6)				; 66-69
	zstore	[dstreg+e1reg], zmm6		;; Save R2						; 63+3

	zfnmaddpd zmm2, zmm2, zmm24, zmm21	;; A7 - I7 * sine (final R7)				; 67-70
	zfmaddpd zmm15, zmm15, zmm24, zmm18	;; B7 + R7 * sine (final I7)				; 67-70
	zstore	[dstreg+2*e1reg], zmm8		;; Save R3						; 64+3

	zfnmaddpd zmm1, zmm1, zmm28, zmm26	;; A8 - I8 * sine (final R8)				; 68-71
	zfmaddpd zmm5, zmm5, zmm28, zmm19	;; B8 + R8 * sine (final I8)				; 68-71
	zstore	[dstreg+e3reg], zmm11		;; Save R4						; 64+4

	zstore	[dst4reg], zmm4			;; Save R5						; 69
	zstore	[dst4reg+64], zmm7		;; Save I5						; 69+1
	zstore	[dst4reg+e1reg], zmm9		;; Save R6						; 70+1
	zstore	[dst4reg+e1reg+64], zmm0	;; Save I6						; 70+2
	zstore	[dst4reg+2*e1reg], zmm2		;; Save R7						; 71+2
	zstore	[dst4reg+2*e1reg+64], zmm15	;; Save I7						; 71+3
	zstore	[dst4reg+e3reg], zmm1		;; Save R8						; 72+3
	zstore	[dst4reg+e3reg+64], zmm5	;; Save I8						; 72+4
	bump	dstreg, dstinc
	bump	dst4reg, dstinc
	ENDM

; Use registers for distances between input blocks.  This lets us share pass1 code.

zr8_rsc_wpn_sgreg_eight_complex_unfft8_preload MACRO
	use zr8_rsc_complex_only_preload or zr8_rsc_complex_and_real_preload
	ENDM

zr8_rsc_wpn_sgreg_eight_complex_unfft8 MACRO srcreg,srcinc,d1reg,d3reg,src4reg,dstreg,dstinc,e1,e2,e4,screg,scinc,maxrpt,L1pt,L1preg,L1p4reg
	vmovapd	zmm28, [screg+1*128+64]		;; cosine for R5/I5
	vmovapd zmm4, [src4reg]			;; Load R5
	vmulpd	zmm16, zmm4, zmm28		;; A5 = R5 * cosine					; 1-4		n 6
	vmovapd zmm12, [src4reg+64]		;; Load I5
	vmulpd	zmm17, zmm12, zmm28		;; B5 = I5 * cosine					; 2-5		n 7

	vmovapd	zmm28, [screg+5*128+64]		;; cosine for R6/I6
	vmovapd zmm5, [src4reg+d1reg]		;; Load R6
	vmulpd	zmm18, zmm5, zmm28		;; A6 = R6 * cosine					; 2-5		n 7
	vmovapd zmm13, [src4reg+d1reg+64]	;; Load I6
	vmulpd	zmm19, zmm13, zmm28		;; B6 = I6 * cosine					; 3-6		n 8

	vmovapd	zmm28, [screg+3*128+64]		;; cosine for R7/I7
	vmovapd zmm14, [src4reg+2*d1reg+64]	;; Load I7
	vmulpd	zmm21, zmm14, zmm28		;; B7 = I7 * cosine					; 3-6		n 8
	vmovapd zmm6, [src4reg+2*d1reg]		;; Load R7
	vmulpd	zmm20, zmm6, zmm28		;; A7 = R7 * cosine					; 4-7		n 9

	vmovapd	zmm28, [screg+7*128+64]		;; cosine for R8/I8
	vmovapd zmm15, [src4reg+d3reg+64]	;; Load I8
	vmulpd	zmm23, zmm15, zmm28		;; B8 = I8 * cosine					; 4-7		n 9
	vmovapd zmm7, [src4reg+d3reg]		;; Load R8	
	vmulpd	zmm22, zmm7, zmm28		;; A8 = R8 * cosine					; 5-8		n 10

	vmovapd	zmm28, [screg+0*128+64]		;; cosine for R1/I1
	vmovapd zmm0, [srcreg]			;; Load R1
	vmulpd	zmm24, zmm0, zmm28		;; A1 = R1 * cosine					; 5-8		n 13
	vmovapd zmm8, [srcreg+64]		;; Load I1
	vmulpd	zmm25, zmm8, zmm28		;; B1 = I1 * cosine					; 6-9		n 14

	vmovapd	zmm28, [screg+1*128]		;; sine for R5/I5
	zfmaddpd zmm16, zmm12, zmm28, zmm16	;; A5 = A5 + I5 * sine (first R5)			; 6-9		n 17
	zfnmaddpd zmm17, zmm4, zmm28, zmm17	;; B5 = B5 - R5 * sine (first I5)			; 7-10		n 18

	vmovapd	zmm28, [screg+5*128]		;; sine for R6/I6
	zfmaddpd zmm18, zmm13, zmm28, zmm18	;; A6 = A6 + I6 * sine (first R6)			; 7-10		n 17
	zfnmaddpd zmm19, zmm5, zmm28, zmm19	;; B6 = B6 - R6 * sine (first I6)			; 8-11		n 18

	vmovapd	zmm28, [screg+3*128]		;; sine for R7/I7
	zfnmaddpd zmm21, zmm6, zmm28, zmm21	;; B7 = B7 - R7 * sine (first I7)			; 8-11		n 19
	zfmaddpd zmm20, zmm14, zmm28, zmm20	;; A7 = A7 + I7 * sine (first R7)			; 9-12		n 20

	vmovapd	zmm28, [screg+7*128]		;; sine for R8/I8
	zfnmaddpd zmm23, zmm7, zmm28, zmm23	;; B8 = B8 - R8 * sine (first I8)			; 9-12		n 19
	zfmaddpd zmm22, zmm15, zmm28, zmm22	;; A8 = A8 + I8 * sine (first R8)			; 10-13		n 20

	vmovapd	zmm28, [screg+4*128+64]		;; cosine for R2/I2
	vmovapd zmm1, [srcreg+d1reg]		;; Load R2
	vmulpd	zmm13, zmm1, zmm28		;; A2 = R2 * cosine					; 10-13		n 14
	vmovapd zmm9, [srcreg+d1reg+64]		;; Load I2
	vmulpd	zmm5, zmm9, zmm28		;; B2 = I2 * cosine					; 11-14		n 15

	vmovapd	zmm28, [screg+2*128+64]		;; cosine for R3/I3
	vmovapd zmm2, [srcreg+2*d1reg]		;; Load R3
	vmulpd	zmm14, zmm2, zmm28		;; A3 = R3 * cosine					; 11-14		n 15
	vmovapd zmm10, [srcreg+2*d1reg+64]	;; Load I3
	vmulpd	zmm6, zmm10, zmm28		;; B3 = I3 * cosine					; 12-15		n 16

	vmovapd	zmm28, [screg+6*128+64]		;; cosine for R4/I4
	vmovapd zmm3, [srcreg+d3reg]		;; Load R4
	vmulpd	zmm15, zmm3, zmm28		;; A4 = R4 * cosine					; 12-15		n 16
	vmovapd zmm11, [srcreg+d3reg+64]	;; Load I4
	vmulpd	zmm7, zmm11, zmm28		;; B4 = I4 * cosine					; 13-16		n 17

	vmovapd	zmm28, [screg+0*128]		;; sine for R1/I1
	zfmaddpd zmm24, zmm8, zmm28, zmm24	;; A1 = A1 + I1 * sine (first R1)			; 13-16		n 21
	zfnmaddpd zmm25, zmm0, zmm28, zmm25	;; B1 = B1 - R1 * sine (first I1)			; 14-17		n 26

	vmovapd	zmm28, [screg+4*128]		;; sine for R2/I2
	zfmaddpd zmm13, zmm9, zmm28, zmm13	;; A2 = A2 + I2 * sine (first R2)			; 14-17		n 21
	zfnmaddpd zmm5, zmm1, zmm28, zmm5	;; B2 = B2 - R2 * sine (first I2)			; 15-18		n 26

	vmovapd	zmm28, [screg+2*128]		;; sine for R3/I3
	zfmaddpd zmm14, zmm10, zmm28, zmm14	;; A3 = A3 + I3 * sine (first R3)			; 15-18		n 22
	zfnmaddpd zmm6, zmm2, zmm28, zmm6	;; B3 = B3 - R3 * sine (first I3)			; 16-19		n 23

	vmovapd	zmm28, [screg+6*128]		;; sine for R4/I4
	zfmaddpd zmm15, zmm11, zmm28, zmm15	;; A4 = A4 + I4 * sine (first R4)			; 16-19		n 22
	zfnmaddpd zmm7, zmm3, zmm28, zmm7	;; B4 = B4 - R4 * sine (first I4)			; 17-20		n 23

	vsubpd	zmm4, zmm16, zmm18		;; R5 - R6 (new R6)					; 17-20		n 24
	vaddpd	zmm16, zmm16, zmm18		;; R5 + R6 (new R5)					; 18-21		n 31
	L1prefetch L1preg, L1pt

	vsubpd	zmm18, zmm17, zmm19		;; I5 - I6 (new I6)					; 18-21		n 24
	vaddpd	zmm17, zmm17, zmm19		;; I5 + I6 (new I5)					; 19-22		n 28
	L1prefetch L1preg+64, L1pt

	vsubpd	zmm19, zmm21, zmm23		;; I7 - I8 (new R8)					; 19-22		n 25
	vaddpd	zmm21, zmm21, zmm23		;; I7 + I8 (new I7)					; 20-23		n 28
	L1prefetch L1preg+d1reg, L1pt

	vsubpd	zmm23, zmm22, zmm20		;; R8 - R7 (new I8)					; 20-23		n 25
	vaddpd	zmm22, zmm22, zmm20		;; R8 + R7 (new R7)					; 21-24		n 31
	L1prefetch L1preg+d1reg+64, L1pt

	vaddpd	zmm20, zmm24, zmm13		;; R1 + R2 (new R1)					; 21-24		n 27
	vsubpd	zmm24, zmm24, zmm13		;; R1 - R2 (new R2)					; 22-25		n 29
	L1prefetch L1preg+2*d1reg, L1pt

	vaddpd	zmm13, zmm15, zmm14		;; R4 + R3 (new R3)					; 22-25		n 27
	vsubpd	zmm15, zmm15, zmm14		;; R4 - R3 (new I4)					; 23-26		n 39
	L1prefetch L1preg+2*d1reg+64, L1pt

	vsubpd	zmm14, zmm6, zmm7		;; I3 - I4 (new R4)					; 23-26		n 29
	vaddpd	zmm6, zmm6, zmm7		;; I3 + I4 (new I3)					; 24-27		n 37
	L1prefetch L1preg+d3reg, L1pt

	vsubpd	zmm7, zmm18, zmm4		;; I6 - R6 (new2 I6)					; 24-27		n 30
	vaddpd	zmm4, zmm4, zmm18		;; R6 + I6 (new2 R6)					; 25-28		n 32
	L1prefetch L1preg+d3reg+64, L1pt

	vsubpd	zmm18, zmm23, zmm19		;; I8 - R8 (new2 I8)					; 25-28		n 30
	vaddpd	zmm19, zmm19, zmm23		;; R8 + I8 (new2 R8)					; 26-29		n 32
	L1prefetch L1p4reg, L1pt

	vaddpd	zmm23, zmm25, zmm5		;; I1 + I2 (new I1)					; 26-29		n 37
	vsubpd	zmm25, zmm25, zmm5		;; I1 - I2 (new I2)					; 27-30		n 39
	L1prefetch L1p4reg+64, L1pt

	vsubpd	zmm5, zmm20, zmm13		;; R1 - R3 (newer R3)					; 27-30		n 33
	vaddpd	zmm20, zmm20, zmm13		;; R1 + R3 (newer R1)					; 28-31		n 35
	L1prefetch L1p4reg+d1reg, L1pt

	vsubpd	zmm13, zmm17, zmm21		;; I5 - I7 (newer R7)					; 28-31		n 33
	vaddpd	zmm17, zmm17, zmm21		;; I5 + I7 (newer I5)					; 29-32		n 51
	L1prefetch L1p4reg+d1reg+64, L1pt

	vsubpd	zmm21, zmm24, zmm14		;; R2 - R4 (newer R4)					; 29-32		n 34
	vaddpd	zmm24, zmm24, zmm14		;; R2 + R4 (newer R2)					; 30-33		n 36
	L1prefetch L1p4reg+2*d1reg, L1pt

	vsubpd	zmm14, zmm7, zmm18		;; I6 - I8 (newer R8/SQRTHALF)				; 30-33		n 34
	vaddpd	zmm7, zmm7, zmm18		;; I6 + I8 (newer I6/SQRTHALF)				; 31-34		n 52
	L1prefetch L1p4reg+2*d1reg+64, L1pt

	vaddpd	zmm18, zmm22, zmm16		;; R7 + R5 (newer R5)					; 31-34		n 35
	vsubpd	zmm22, zmm22, zmm16		;; R7 - R5 (newer I7)					; 32-35		n 49
	L1prefetch L1p4reg+d3reg, L1pt

	vaddpd	zmm16, zmm19, zmm4		;; R8 + R6 (newer R6/SQRTHALF)				; 32-35		n 36
	vsubpd	zmm19, zmm19, zmm4		;; R8 - R6 (newer I8/SQRTHALF)				; 33-36		n 50
	L1prefetch L1p4reg+d3reg+64, L1pt

	vaddpd	zmm4, zmm5, zmm13		;; R3 + R7 (final R3)					; 33-36		n 38
	vsubpd	zmm5, zmm5, zmm13		;; R3 - R7 (final R7)					; 34-37		n 42
	bump	srcreg, srcinc

	zfmaddpd zmm13, zmm14, zmm29, zmm21	;; R4 + R8 * SQRTHALF (final R4)			; 34-37		n 38
	zfnmaddpd zmm14, zmm14, zmm29, zmm21	;; R4 - R8 * SQRTHALF (final R8)			; 35-38		n 42
	bump	src4reg, srcinc

	vaddpd	zmm21, zmm20, zmm18		;; R1 + R5 (final R1)					; 35-38		n 40
	vsubpd	zmm20, zmm20, zmm18		;; R1 - R5 (final R5)					; 36-39		n 44
	bump	screg, scinc

	zfmaddpd zmm18, zmm16, zmm29, zmm24	;; R2 + R6 * SQRTHALF (final R2)			; 36-39		n 40
	zfnmaddpd zmm16, zmm16, zmm29, zmm24	;; R2 - R6 * SQRTHALF (final R6)			; 37-40		n 44
	bump	L1preg, srcinc
	vaddpd	zmm24, zmm23, zmm6		;; I1 + I3 (newer I1)					; 37-40		n 51

	vmovapd	zmm8, zmm4
	vpermt2pd zmm8, zmm31, zmm13		;; r4_4 r3_4 r4_6 r3_6 r4_0 r3_0 r4_2 r3_2		; 38-40		n 41
	vpermt2pd zmm4, zmm30, zmm13		;; r4_5 r3_5 r4_7 r3_7 r4_1 r3_1 r4_3 r3_3		; 39-41		n 43

	vsubpd	zmm23, zmm23, zmm6		;; I1 - I3 (newer I3)					;  38-41	n 49
	vaddpd	zmm6, zmm25, zmm15		;; I2 + I4 (newer I2)					;  39-42	n 52
	bump	L1p4reg, srcinc

	vshufpd	zmm13, zmm21, zmm18, 00000000b	;; r2_6 r1_6 r2_4 r1_4 r2_2 r1_2 r2_0 r1_0		; 40		n 41
	vshufpd	zmm21, zmm21, zmm18, 11111111b	;; r2_7 r1_7 r2_5 r1_5 r2_3 r1_3 r2_1 r1_1		; 41		n 43

	vsubpd	zmm25, zmm25, zmm15		;; I2 - I4 (newer I4)					;  40-43	n 50
	vblendmpd zmm9{k7}, zmm8, zmm13		;; r4_4 r3_4 r2_4 r1_4 r4_0 r3_0 r2_0 r1_0		;  41		n 46

	vmovapd zmm18, zmm5
	vpermt2pd zmm18, zmm31, zmm14		;; r8_4 r7_4 r8_6 r7_6 r8_0 r7_0 r8_2 r7_2		; 42-44		n 45
	vpermt2pd zmm5, zmm30, zmm14		;; r8_5 r7_5 r8_7 r7_7 r8_1 r7_1 r8_3 r7_3		; 43-45		n 47

	vblendmpd zmm13{k7}, zmm13, zmm8	;; r2_6 r1_6 r4_6 r3_6 r2_2 r1_2 r4_2 r3_2		;  42		n 48
	vblendmpd zmm8{k7}, zmm4, zmm21		;; r4_5 r3_5 r2_5 r1_5 r4_1 r3_1 r2_1 r1_1		;  43		n 50

	vshufpd	zmm14, zmm20, zmm16, 00000000b	;; r6_6 r5_6 r6_4 r5_4 r6_2 r5_2 r6_0 r5_0		; 44		n 45
	vshufpd	zmm20, zmm20, zmm16, 11111111b	;; r6_7 r5_7 r6_5 r5_5 r6_3 r5_3 r6_1 r5_1		; 45		n 47

	vblendmpd zmm21{k7}, zmm21, zmm4	;; r2_7 r1_7 r4_7 r3_7 r2_3 r1_3 r4_3 r3_3		;  44		n 52
	vblendmpd zmm4{k7}, zmm18, zmm14	;; r8_4 r7_4 r6_4 r5_4 r8_0 r7_0 r6_0 r5_0		;  45		n 46

	vshuff64x2 zmm2, zmm9, zmm4, 01000100b	;; r8_0 r7_0 r6_0 r5_0 r4_0 r3_0 r2_0 r1_0		; 46-48
	vshuff64x2 zmm9, zmm9, zmm4, 11101110b	;; r8_4 r7_4 r6_4 r5_4 r4_4 r3_4 r2_4 r1_4		; 47-49

	vblendmpd zmm14{k7}, zmm14, zmm18	;; r6_6 r5_6 r8_6 r7_6 r6_2 r5_2 r8_2 r7_2		;  46		n 48
	vblendmpd zmm18{k7}, zmm5, zmm20	;; r8_5 r7_5 r6_5 r6_5 r8_1 r7_1 r6_1 r5_1		;  47		n 50

	vshuff64x2 zmm4, zmm13, zmm14, 00010001b;; r8_2 r7_2 r6_2 r5_2 r4_2 r3_2 r2_2 r1_2		; 48-50
	vshuff64x2 zmm13, zmm13, zmm14, 10111011b;;r8_6 r7_6 r6_6 r5_6 r4_6 r3_6 r2_6 r1_6		; 49-51

	vblendmpd zmm20{k7}, zmm20, zmm5	;; r6_7 r5_7 r8_7 r7_7 r6_3 r5_3 r8_3 r7_3		;  48		n 52
	vaddpd	zmm15, zmm23, zmm22		;; I3 + I7 (final I3)					;  49-52	n 54
	zstore	[dstreg], zmm2										; 49

	vshuff64x2 zmm14, zmm8, zmm18, 01000100b;; r8_1 r7_1 r6_1 r5_1 r4_1 r3_1 r2_1 r1_1		; 50-52
	vshuff64x2 zmm8, zmm8, zmm18, 11101110b ;; r8_5 r7_5 r6_5 r5_5 r4_5 r3_5 r2_5 r1_5		; 51-53

	zfmaddpd zmm10, zmm19, zmm29, zmm25	;; I4 + I8 * SQRTHALF (final I4)			;  50-53	n 54
	vaddpd	zmm12, zmm24, zmm17		;; I1 + I5 (final I1)					;  51-54	n 56
	zstore	[dstreg+e4], zmm9									; 50
	zstore	[dstreg+e2], zmm4									; 51

	vshuff64x2 zmm18, zmm21, zmm20, 00010001b;;r8_3 r7_3 r6_3 r5_3 r4_3 r3_3 r2_3 r1_3		; 52-54
	vshuff64x2 zmm21, zmm21, zmm20, 10111011b;;r8_7 r7_7 r6_7 r5_7 r4_7 r3_7 r2_7 r1_7		; 53-55

	zfmaddpd zmm0, zmm7, zmm29, zmm6	;; I2 + I6 * SQRTHALF (final I2)			;  52-55	n 56
	vsubpd	zmm23, zmm23, zmm22		;; I3 - I7 (final I7)					;  53-56	n 58
	zstore	[dstreg+e4+e2], zmm13									; 52
	zstore	[dstreg+e1], zmm14									; 53

	vmovapd	zmm20, zmm15
	vpermt2pd zmm20, zmm31, zmm10		;; i4_4 i3_4 i4_6 i3_6 i4_0 i3_0 i4_2 i3_2		; 54-56		n 57
	vpermt2pd zmm15, zmm30, zmm10		;; i4_5 i3_5 i4_7 i3_7 i4_1 i3_1 i4_3 i3_3		; 55-57		n 59

	zfnmaddpd zmm19, zmm19, zmm29, zmm25	;; I4 - I8 * SQRTHALF (final I8)			;  54-57	n 58
	vsubpd	zmm24, zmm24, zmm17		;; I1 - I5 (final I5)					;  55-58	n 60
	zstore	[dstreg+e4+e1], zmm8									; 54
	zstore	[dstreg+e2+e1], zmm18									; 55

	vshufpd	zmm10, zmm12, zmm0, 00000000b	;; i2_6 i1_6 i2_4 i1_4 i2_2 i1_2 i2_0 i1_0		; 56		n 57
	vshufpd	zmm12, zmm12, zmm0, 11111111b	;; i2_7 i1_7 i2_5 i1_5 i2_3 i1_3 i2_1 i1_1		; 57		n 59

	zfnmaddpd zmm7, zmm7, zmm29, zmm6	;; I2 - I6 * SQRTHALF (final I6)			;  56-59	n 60
	vblendmpd zmm11{k7}, zmm20, zmm10	;; i4_4 i3_4 i2_4 i1_4 i4_0 i3_0 i2_0 i1_0		;  57		n 62
	zstore	[dstreg+e4+e2+e1], zmm21								; 56

	vmovapd zmm0, zmm23
	vpermt2pd zmm0, zmm31, zmm19		;; i8_4 i7_4 i8_6 i7_6 i8_0 i7_0 i8_2 i7_2		; 58-60		n 61
	vpermt2pd zmm23, zmm30, zmm19		;; i8_5 i7_5 i8_7 i7_7 i8_1 i7_1 i8_3 i7_3		; 59-61		n 62

	vblendmpd zmm10{k7}, zmm10, zmm20	;; i2_6 i1_6 i4_6 i3_6 i2_2 i1_2 i4_2 i3_2		;  58		n 64
	vblendmpd zmm20{k7}, zmm15, zmm12	;; i4_5 i3_5 i2_5 i1_5 i4_1 i3_1 i2_1 i1_1		;  59		n 66

	vshufpd	zmm19, zmm24, zmm7, 00000000b	;; i6_6 i5_6 i6_4 i5_4 i6_2 i5_2 i6_0 i5_0		; 60		n 61
	vshufpd	zmm24, zmm24, zmm7, 11111111b	;; i6_7 i5_7 i6_5 i5_5 i6_3 i5_3 i6_1 i5_1		; 61		n 63

	vblendmpd zmm12{k7}, zmm12, zmm15	;; i2_7 i1_7 i4_7 i3_7 i2_3 i1_3 i4_3 i3_3		;  60		n 68
	vblendmpd zmm15{k7}, zmm0, zmm19	;; i8_4 i7_4 i6_4 i5_4 i8_0 i7_0 i6_0 i5_0		;  61		n 62

	vshuff64x2 zmm25, zmm11, zmm15, 01000100b ;; i8_0 i7_0 i6_0 i5_0 i4_0 i3_0 i2_0 i1_0		; 62-64
	vshuff64x2 zmm11, zmm11, zmm15, 11101110b ;; i8_4 i7_4 i6_4 i5_4 i4_4 i3_4 i2_4 i1_4		; 63-65

	vblendmpd zmm19{k7}, zmm19, zmm0	;; i6_6 i5_6 i8_6 i7_6 i6_2 i5_2 i8_2 i7_2		;  62		n 64
	vblendmpd zmm0{k7}, zmm23, zmm24	;; i8_5 i7_5 i6_5 i5_5 i8_1 i7_1 i6_1 i5_1		;  63		n 66

	vshuff64x2 zmm15, zmm10, zmm19, 00010001b ;; i8_2 i7_2 i6_2 i5_2 i4_2 i3_2 i2_2 i1_2		; 64-66
	vshuff64x2 zmm10, zmm10, zmm19, 10111011b;; i8_6 i7_6 i6_6 i5_6 i4_6 i3_6 i2_6 i1_6		; 65-67

	vblendmpd zmm24{k7}, zmm24, zmm23	;; i6_7 i5_7 i8_7 i7_7 i6_3 i5_3 i8_3 i7_3		;  64		n 68
	zstore	[dstreg+64], zmm25									; 65

	vshuff64x2 zmm19, zmm20, zmm0, 01000100b ;; i8_1 i7_1 i6_1 i5_1 i4_1 i3_1 i2_1 i1_1		; 66-68
	zstore	[dstreg+e4+64], zmm11									; 66

	vshuff64x2 zmm20, zmm20, zmm0, 11101110b ;; i8_5 i7_5 i6_5 i5_5 i4_5 i3_5 i2_5 i1_5		; 67-69
	zstore	[dstreg+e2+64], zmm15									; 67

	vshuff64x2 zmm0, zmm12, zmm24, 00010001b ;; i8_3 i7_3 i6_3 i5_3 i4_3 i3_3 i2_3 i1_3		; 68-70
	zstore	[dstreg+e4+e2+64], zmm10								; 68

	vshuff64x2 zmm12, zmm12, zmm24, 10111011b;; i8_7 i7_7 i6_7 i5_7 i4_7 i3_7 i2_7 i1_7		; 69-71
	zstore	[dstreg+e1+64], zmm19									; 69

	zstore	[dstreg+e4+e1+64], zmm20								; 70
	zstore	[dstreg+e2+e1+64], zmm0									; 71
	zstore	[dstreg+e4+e2+e1+64], zmm12								; 72
	bump	dstreg, dstinc
	ENDM


;;
;; ************************************* sixteen-reals-fft variants ******************************************
;;

;; These macros operate on 16 reals doing 4 levels of the FFT, applying
;; the sin/cos multipliers afterwards.  The output is 2 reals and 7 complex numbers.

;; To calculate a 16-reals FFT, we calculate 16 complex values in a brute force way (using a shorthand notation):
;; r1 + r2 + ... + r16	*  w^0000000000...
;; r1 + r2 + ... + r16	*  w^0123456789A...
;; r1 + r2 + ... + r16	*  w^02468ACE....
;;    ...
;; r1 + r2 + ... + r16	*  w^...A987654321
;; Note that Hermetian symmetry means we won't need to calculate the last 8 complex values.
;;
;; The sin/cos values (w = 16th root of unity) are:
;; w^1 = .924 + .383i
;; w^2 = .707 + .707i
;; w^3 = .383 + .924i
;; w^4 = 0 + 1i
;; w^5 = -.383 + .924i
;; w^6 = -.707 + .707i
;; w^7 = -.924 + .383i
;; w^8 = -1
;;
;; Applying the sin/cos values above (and noting that combining r2 and r10, r3 and r11, etc. will simplify calculations):
;;R1 = (r1+r9)     +(r3+r11)     +(r7+r15) + (r5+r13)     +(r4+r12)     +(r6+r14)     +(r2+r10)     +(r8+r16)
;;R2 = (r1-r9) +.707(r3-r11) -.707(r7-r15)            +.383(r4-r12) -.383(r6-r14) +.924(r2-r10) -.924(r8-r16)
;;R3 = (r1+r9)                             - (r5+r13) -.707(r4+r12) -.707(r6+r14) +.707(r2+r10) +.707(r8+r16)
;;R4 = (r1-r9) -.707(r3-r11) +.707(r7-r15)            -.924(r4-r12) +.924(r6-r14) +.383(r2-r10) -.383(r8-r16)
;;R5 = (r1+r9)     -(r3+r11)     -(r7+r15) + (r5+r13)
;;R6 = (r1-r9) -.707(r3-r11) +.707(r7-r15)            +.924(r4-r12) -.924(r6-r14) -.383(r2-r10) +.383(r8-r16)
;;R7 = (r1+r9)                             - (r5+r13) +.707(r4+r12) +.707(r6+r14) -.707(r2+r10) -.707(r8+r16)
;;R8 = (r1-r9) +.707(r3-r11) -.707(r7-r15)            -.383(r4-r12) +.383(r6-r14) -.924(r2-r10) +.924(r8-r16)
;;R9 = (r1+r9)     +(r3+r11)     +(r7+r15) + (r5+r13)     -(r4+r12)     -(r6+r14)     -(r2+r10)     -(r8+r16)
;;I2 =         +.707(r3-r11) +.707(r7-r15) + (r5-r13) +.924(r4-r12) +.924(r6-r14) +.383(r2-r10) +.383(r8-r16)
;;I3 =             +(r3+r11)     -(r7+r15)            +.707(r4+r12) -.707(r6+r14) +.707(r2+r10) -.707(r8+r16)
;;I4 =         +.707(r3-r11) +.707(r7-r15) - (r5-r13) -.383(r4-r12) -.383(r6-r14) +.924(r2-r10) +.924(r8-r16)
;;I5 =                                                    -(r4+r12)     +(r6+r14)     +(r2+r10)     -(r8+r16)
;;I6 =         -.707(r3-r11) -.707(r7-r15) + (r5-r13) -.383(r4-r12) -.383(r6-r14) +.924(r2-r10) +.924(r8-r16)
;;I7 =             -(r3+r11)     +(r7+r15)            +.707(r4+r12) -.707(r6+r14) +.707(r2+r10) -.707(r8+r16)
;;I8 =         -.707(r3-r11) -.707(r7-r15) - (r5-r13) +.924(r4-r12) +.924(r6-r14) +.383(r2-r10) +.383(r8-r16)

;; Further simplification:
;;R1 = (r1+r9) + (r5+r13)     +((r3+r11)+(r7+r15))     +(((r2+r10)+(r8+r16))+((r4+r12)+(r6+r14)))
;;R9 = (r1+r9) + (r5+r13)     +((r3+r11)+(r7+r15))     -(((r2+r10)+(r8+r16))+((r4+r12)+(r6+r14)))
;;R5 = (r1+r9) + (r5+r13)     -((r3+r11)+(r7+r15))
;;R3 = (r1+r9) - (r5+r13)                          +.707(((r2+r10)+(r8+r16))-((r4+r12)+(r6+r14)))
;;R7 = (r1+r9) - (r5+r13)                          -.707(((r2+r10)+(r8+r16))-((r4+r12)+(r6+r14)))
;;R2 = (r1-r9) +.707((r3-r11)-(r7-r15))            +.924((r2-r10)-(r8-r16)) +.383((r4-r12)-(r6-r14))
;;R8 = (r1-r9) +.707((r3-r11)-(r7-r15))            -.924((r2-r10)-(r8-r16)) -.383((r4-r12)-(r6-r14))
;;R4 = (r1-r9) -.707((r3-r11)-(r7-r15))            +.383((r2-r10)-(r8-r16)) -.924((r4-r12)-(r6-r14))
;;R6 = (r1-r9) -.707((r3-r11)-(r7-r15))            -.383((r2-r10)-(r8-r16)) +.924((r4-r12)-(r6-r14))
;;I5 =                                                 +(((r2+r10)-(r8+r16))-((r4+r12)-(r6+r14)))
;;I3 =             +((r3+r11)-(r7+r15))            +.707(((r2+r10)-(r8+r16))+((r4+r12)-(r6+r14)))
;;I7 =             -((r3+r11)-(r7+r15))            +.707(((r2+r10)-(r8+r16))+((r4+r12)-(r6+r14)))
;;I2 =         +.707((r3-r11)+(r7-r15)) + (r5-r13) +.383((r2-r10)+(r8-r16)) +.924((r4-r12)+(r6-r14))
;;I8 =         -.707((r3-r11)+(r7-r15)) - (r5-r13) +.383((r2-r10)+(r8-r16)) +.924((r4-r12)+(r6-r14))
;;I4 =         +.707((r3-r11)+(r7-r15)) - (r5-r13) +.924((r2-r10)+(r8-r16)) -.383((r4-r12)+(r6-r14))
;;I6 =         -.707((r3-r11)+(r7-r15)) + (r5-r13) +.924((r2-r10)+(r8-r16)) -.383((r4-r12)+(r6-r14))

;; NOTE: unlike the AVX versions of this macro, we do not "back up" the last 2 reals by one level.
;; Thus, the next level 16-reals will get its inputs with r1+/-r9, r2+/-r10 already calculated.
;; ALSO NOTE: this macro does not output its results in bit-reversed order.


; Uses two sin-cos pointers, one for real table (w^1,w^3,w^5,w^7), one for complex table (w^2,w^4,w^6)
; Used in middle levels of second pass in a two-pass FFT
zr8_2sc_sixteen_reals_fft_preload MACRO
	zr8_16r_fft_cmn_preload
	ENDM

zr8_2sc_sixteen_reals_fft MACRO srcreg,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	zr8_16r_fft_cmn srcreg,0,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

; Offsets source by rbx
zr8f_2sc_sixteen_reals_fft_preload MACRO
	zr8_16r_fft_cmn_preload
	ENDM

zr8f_2sc_sixteen_reals_fft MACRO srcreg,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	zr8_16r_fft_cmn srcreg,rbx,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

; Combined sin/cos data
; The real sin/cos values are after the complex sin/cos values.
; Used in middle levels of first pass in a two-pass FFT
zr8_csc_sixteen_reals_fft_preload MACRO
	zr8_16r_fft_cmn_preload
	ENDM

zr8_csc_sixteen_reals_fft MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	zr8_16r_fft_cmn srcreg,0,srcinc,d1,d2,d4,screg+4*128,0,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

zr8_16r_fft_cmn_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRTHALF
	vbroadcastsd zmm30, ZMM_P924_P383
	vbroadcastsd zmm29, ZMM_P383
	ENDM

zr8_16r_fft_cmn MACRO srcreg,srcoff,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	vmovapd	zmm1, [srcreg+srcoff+d1]	;; r2+r10
	vmovapd	zmm3, [srcreg+srcoff+d4+d2+d1]	;; r8+r16
	vaddpd	zmm0, zmm1, zmm3		;; r2++ = (r2+r10)+(r8+r16)				; 1-4		n 6
	vsubpd	zmm1, zmm1, zmm3		;; r2+- = (r2+r10)-(r8+r16)				; 1-4		n 7

	vmovapd	zmm5, [srcreg+srcoff+d2+d1]	;; r4+r12
	vmovapd	zmm7, [srcreg+srcoff+d4+d1]	;; r6+r14
	vaddpd	zmm3, zmm5, zmm7		;; r4++ = (r4+r12)+(r6+r14)				; 2-5		n 6
	vsubpd	zmm5, zmm5, zmm7		;; r4+- = (r4+r12)-(r6+r14)				; 2-5		n 7

	vmovapd	zmm13, [srcreg+srcoff]		;; r1+r9
	vmovapd	zmm15, [srcreg+srcoff+d4]	;; r5+r13
	vaddpd	zmm7, zmm13, zmm15		;; r1++ = (r1+r9)+(r5+r13)				; 3-6		n 8
	vsubpd	zmm13, zmm13, zmm15		;; r1+- = (r1+r9)-(r5+r13)				; 3-6		n 11

	vmovapd	zmm9, [srcreg+srcoff+d2]	;; r3+r11
	vmovapd	zmm11, [srcreg+srcoff+d4+d2]	;; r7+r15
	vaddpd	zmm15, zmm9, zmm11		;; r3++ = (r3+r11)+(r7+r15)				; 4-7		n 8
	vsubpd	zmm9, zmm9, zmm11		;; r3+- = (r3+r11)-(r7+r15)				; 4-7		n 12

	vmovapd	zmm10, [srcreg+srcoff+d2+64]	;; r3-r11
	vmovapd	zmm12, [srcreg+srcoff+d4+d2+64]	;; r7-r15
	vaddpd	zmm11, zmm10, zmm12		;; r3-+ = (r3-r11)+(r7-r15)				; 5-8		n 16
	vsubpd	zmm10, zmm10, zmm12		;; r3-- = (r3-r11)-(r7-r15)				; 5-8		n 14

	vmovapd	zmm2, [srcreg+srcoff+d1+64]	;; r2-r10
	vaddpd	zmm12, zmm0, zmm3		;; r2+++ = (r2++) + (r4++)				; 6-9		n 13
	vsubpd	zmm0, zmm0, zmm3		;; r2++- = (r2++) - (r4++)				; 6-9		n 11

	vmovapd	zmm4, [srcreg+srcoff+d4+d2+d1+64] ;; r8-r16
	vaddpd	zmm3, zmm1, zmm5		;; r2+-+ = (r2+-) + (r4+-)				; 7-10		n 12
	vsubpd	zmm1, zmm1, zmm5		;; r2+-- = (r2+-) - (r4+-)	(final I5)		; 7-10		n 18

	vmovapd	zmm6, [srcreg+srcoff+d2+d1+64]	;; r4-r12
	vmovapd	zmm8, [srcreg+srcoff+d4+d1+64]	;; r6-r14
	vaddpd	zmm5, zmm7, zmm15		;; r1+++ = (r1++) + (r3++)				; 8-11		n 13
	vsubpd	zmm7, zmm7, zmm15		;; r1++- = (r1++) - (r3++)	(final R5)		; 8-11		n 18

	vmovapd	zmm14, [srcreg+srcoff+64]	;; r1-r9
	vmovapd	zmm16, [srcreg+srcoff+d4+64]	;; r5-r13
	vaddpd	zmm15, zmm2, zmm4		;; r2-+ = (r2-r10)+(r8-r16)				; 9-12		n 17
	vsubpd	zmm2, zmm2, zmm4		;; r2-- = (r2-r10)-(r8-r16)				; 9-12		n 15

	vmovapd	zmm28, [screg2+1*128+64]	;; cosine/sine for R5/I5 (w^4 = complex w^2)
	vaddpd	zmm4, zmm6, zmm8		;; r4-+ = (r4-r12)+(r6-r14)				; 10-13		n 17
	vsubpd	zmm6, zmm6, zmm8		;; r4-- = (r4-r12)-(r6-r14)				; 10-13		n 15

	vmovapd	zmm27, [screg2+0*128+64]	;; cosine/sine for R3/I3 (w^2 = complex w^1)
	vmovapd	zmm26, [screg2+2*128+64]	;; cosine/sine for R7/I7 (w^6 = complex w^3)
	zfmaddpd zmm8, zmm0, zmm31, zmm13	;; R3 = (r1+-) + .707(r2++-)				; 11-14		n 19
	zfnmaddpd zmm0, zmm0, zmm31, zmm13	;; R7 = (r1+-) - .707(r2++-)				; 11-14		n 20

	vmovapd	zmm25, [screg2+1*128]		;; sine for R5/I5
	zfmaddpd zmm13, zmm3, zmm31, zmm9	;; I3 = .707*(r2+-+) + (r3+-)				; 12-15		n 19
	zfmsubpd zmm3, zmm3, zmm31, zmm9	;; I7 = .707*(r2+-+) - (r3+-)				; 12-15		n 20

	vmovapd	zmm24, [screg2+0*128]		;; sine for R3/I3
	vaddpd	zmm9, zmm5, zmm12		;; R1 = (r1+++) + (r2+++)				; 13-16
	vsubpd	zmm5, zmm5, zmm12		;; R9 = (r1+++) - (r2+++)				; 13-16

	vmovapd	zmm23, [screg2+2*128]		;; sine for R7/I7
	vmovapd	zmm22, [screg1+0*128+64]	;; cosine/sine for R2/I2 (w^1)
	zfmaddpd zmm12, zmm10, zmm31, zmm14	;; r2o = (r1-r9) + .707*(r3--)				; 14-17		n 21
	zfnmaddpd zmm10, zmm10, zmm31, zmm14	;; r4o = (r1-r9) - .707*(r3--)				; 14-17		n 22

	vmovapd	zmm21, [screg1+3*128+64]	;; cosine/sine for R8/I8 (w^7)
	zfmaddpd zmm14, zmm2, zmm30, zmm6	;; r2e/.383 = .924/.383(r2--) + (r4--)			; 15-18		n 21
	zfnmaddpd zmm6, zmm6, zmm30, zmm2	;; r4e/.383 = (r2--) - .924/.383(r4--)			; 15-18		n 22

	vmovapd	zmm20, [screg1+1*128+64]	;; cosine/sine for R4/I4 (w^3)
	vmovapd	zmm19, [screg1+2*128+64]	;; cosine/sine for R6/I6 (w^5)
	zfmaddpd zmm2, zmm11, zmm31, zmm16	;; i2o = .707(r3-+) + (r5-r13)				; 16-19		n 23
	zfmsubpd zmm11, zmm11, zmm31, zmm16	;; i4o = .707(r3-+) - (r5-r13)				; 16-19		n 24

	vmovapd	zmm18, [screg1+0*128]		;; sine for R2/I2
	vmovapd	zmm17, [screg1+3*128]		;; sine for R8/I8
	zfmaddpd zmm16, zmm4, zmm30, zmm15	;; i2e/.383 = (r2-+) + .924/.383(r4-+)			; 17-20		n 23
	zfmsubpd zmm15, zmm15, zmm30, zmm4	;; i4e/.383 = .924/.383(r2-+) - (r4-+)			; 17-20		n 24
	zstore	[srcreg], zmm9			;; Store R1						; 17

	vmovapd	zmm9, [screg1+1*128]		;; sine for R4/I4
	zfmsubpd zmm4, zmm7, zmm28, zmm1	;; A5 = R5 * cosine/sine - I5				; 18-21		n 25
	zfmaddpd zmm1, zmm1, zmm28, zmm7	;; B5 = I5 * cosine/sine + R5				; 18-21		n 25
	zstore	[srcreg+64], zmm5		;; Store R9						; 17+1

	vmovapd zmm5, [screg1+2*128]		;; sine for R6/I6
	zfmsubpd zmm7, zmm8, zmm27, zmm13	;; A3 = R3 * cosine/sine - I3				; 19-22		n 26
	zfmaddpd zmm13, zmm13, zmm27, zmm8	;; B3 = I3 * cosine/sine + R3				; 19-22		n 26
	bump	screg1, scinc1

	L1prefetchw srcreg+L1pd, L1pt
	zfmsubpd zmm8, zmm0, zmm26, zmm3	;; A7 = R7 * cosine/sine - I7				; 20-23		n 27
	zfmaddpd zmm3, zmm3, zmm26, zmm0	;; B7 = I7 * cosine/sine + R7				; 20-23		n 27
	bump	screg2, scinc2

	L1prefetchw srcreg+64+L1pd, L1pt
	zfmaddpd zmm0, zmm14, zmm29, zmm12	;; R2 = r2o + .383*r2e					; 21-24		n 
	zfnmaddpd zmm14, zmm14, zmm29, zmm12	;; R8 = r2o - .383*r2e					; 21-24		n 

	L1prefetchw srcreg+d1+L1pd, L1pt
	zfmaddpd zmm12, zmm6, zmm29, zmm10	;; R4 = r4o + .383*r4e					; 22-25		n 
	zfnmaddpd zmm6, zmm6, zmm29, zmm10	;; R6 = r4o - .383*r4e					; 22-25		n 

	L1prefetchw srcreg+d1+64+L1pd, L1pt
	zfmaddpd zmm10, zmm16, zmm29, zmm2	;; I2 = .383*i2e + i2o					; 23-26		n 34
	zfmsubpd zmm16, zmm16, zmm29, zmm2	;; I8 = .383*i2e - i2o					; 23-26		n 33

	L1prefetchw srcreg+d2+L1pd, L1pt
	zfmaddpd zmm2, zmm15, zmm29, zmm11	;; I4 = .383*i4e + i4o					; 24-27		n 38
	zfmsubpd zmm15, zmm15, zmm29, zmm11	;; I6 = .383*i4e - i4o					; 24-27		n 36

	L1prefetchw srcreg+d2+64+L1pd, L1pt
	vmulpd	zmm4, zmm4, zmm25		;; A5 = A5 * sine (final R5)				; 25-28
	vmulpd	zmm1, zmm1, zmm25		;; B5 = B5 * sine (final I5)				; 25-28

	L1prefetchw srcreg+d2+d1+L1pd, L1pt
	vmulpd	zmm7, zmm7, zmm24		;; A3 = A3 * sine (final R3)				; 26-29
	vmulpd	zmm13, zmm13, zmm24		;; B3 = B3 * sine (final I3)				; 26-29

	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt
	vmulpd	zmm8, zmm8, zmm23		;; A7 = A7 * sine (final R7)				; 27-30
	vmulpd	zmm3, zmm3, zmm23		;; B7 = B7 * sine (final I7)				; 27-30

	L1prefetchw srcreg+d4+L1pd, L1pt
	zfmsubpd zmm11, zmm0, zmm22, zmm10	;; A2 = R2 * cosine/sine - I2				; 28-31		n 32
	zfmaddpd zmm10, zmm10, zmm22, zmm0	;; B2 = I2 * cosine/sine + R2				; 28-31

	L1prefetchw srcreg+d4+64+L1pd, L1pt
	zfmsubpd zmm0, zmm14, zmm21, zmm16	;; A8 = R8 * cosine/sine - I8				; 29-32
	zfmaddpd zmm16, zmm16, zmm21, zmm14	;; B8 = I8 * cosine/sine + R8				; 29-32
	zstore	[srcreg+d4], zmm4		;; Store R5						; 29

	L1prefetchw srcreg+d4+d1+L1pd, L1pt
	zfmsubpd zmm14, zmm12, zmm20, zmm2	;; A4 = R4 * cosine/sine - I4				; 30-33
	zfmaddpd zmm2, zmm2, zmm20, zmm12	;; B4 = I4 * cosine/sine + R4				; 30-33
	zstore	[srcreg+d4+64], zmm1		;; Store I5						; 29+1

	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt
	zfmsubpd zmm12, zmm6, zmm19, zmm15	;; A6 = R6 * cosine/sine - I6				; 31-34
	zfmaddpd zmm15, zmm15, zmm19, zmm6	;; B6 = I6 * cosine/sine + R6				; 31-34
	zstore	[srcreg+d2], zmm7		;; Store R3						; 30+1

	L1prefetchw srcreg+d4+d2+L1pd, L1pt
	vmulpd	zmm11, zmm11, zmm18		;; A2 = A2 * sine (final R2)				; 32-35
	vmulpd	zmm10, zmm10, zmm18		;; B2 = B2 * sine (final I2)				; 32-35
	zstore	[srcreg+d2+64], zmm13		;; Store I3						; 30+2

	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt
	vmulpd	zmm0, zmm0, zmm17		;; A8 = A8 * sine (final R8)				; 33-36
	vmulpd	zmm16, zmm16, zmm17		;; B8 = B8 * sine (final I8)				; 33-36
	zstore	[srcreg+d4+d2], zmm8		;; Store R7						; 31+2

	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt
	vmulpd	zmm14, zmm14, zmm9		;; A4 = A4 * sine (final R4)				; 34-37
	vmulpd	zmm2, zmm2, zmm9		;; B4 = B4 * sine (final I4)				; 34-37
	zstore	[srcreg+d4+d2+64], zmm3		;; Store I7						; 31+3

	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt
	vmulpd	zmm12, zmm12, zmm5		;; A6 = A6 * sine (final R6)				; 35-38
	vmulpd	zmm15, zmm15, zmm5		;; B6 = B6 * sine (final I6)				; 35-38

	zstore	[srcreg+d1], zmm11		;; Store R2						; 36
	zstore	[srcreg+d1+64], zmm10		;; Store I2						; 36+1
	zstore	[srcreg+d4+d2+d1], zmm0		;; Store R8						; 37+1
	zstore	[srcreg+d4+d2+d1+64], zmm16	;; Store I8						; 37+2
	zstore	[srcreg+d2+d1], zmm14		;; Store R4						; 38+2
	zstore	[srcreg+d2+d1+64], zmm2		;; Store I4						; 38+3
	zstore	[srcreg+d4+d1], zmm12		;; Store R6						; 39+3
	zstore	[srcreg+d4+d1+64], zmm15	;; Store I6						; 39+4
	bump	srcreg, srcinc
	ENDM




;; Used in first levels of pass 1 in a two-pass FFT
;; This is the only 16-reals macro that must do the initial plus/minus on distance 8 input arguments
zr8_wpn_sixteen_reals_first_fft_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRTHALF
	vbroadcastsd zmm30, ZMM_P924_P383
	vbroadcastsd zmm29, ZMM_P383
	vbroadcastsd zmm28, ZMM_ONE_OVER_B
	ENDM

zr8_wpn_sixteen_reals_first_fft MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maskreg,maskinc,grpreg,grpinc,maxrpt,L1pt,L1pd
	mov	bl, [maskreg+0*1]		;; Load index into compressed fudges
	mov	r14, [r13+0*8]			;; Load the xor mask
	xor	r14, [r12+rbx*8]		;; Apply the xor mask to 64 bits of fudge flags

	kmovw	k1, r14d			;; Load R1 and R9 fudge factor mask			; 1
	kshiftrw k2, k1, 8			;; R9's fudge						; 2
	vmovapd zmm14, [grpreg+0*64]		;; group multiplier for R1
	vmulpd	zmm14{k1}, zmm14, zmm28		;; fudged group multiplier for R1			; 2-5
	vmovapd zmm20, [grpreg+1*64]		;; group multiplier for R9
	vmulpd	zmm20{k2}, zmm20, zmm28		;; fudged group multiplier for R9			; 3-6

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R2 and R10 fudge factor mask			; 3
	kshiftrw k2, k1, 8			;; R10's fudge						; 4
	vmovapd zmm2, [grpreg+2*64]		;; group multiplier for R2
	vmulpd	zmm2{k1}, zmm2, zmm28		;; fudged group multiplier for R2			; 4-7
	vmovapd zmm21, [grpreg+3*64]		;; group multiplier for R10
	vmulpd	zmm21{k2}, zmm21, zmm28		;; fudged group multiplier for R10			; 5-8

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R3 and R11 fudge factor mask			; 5
	kshiftrw k2, k1, 8			;; R11's fudge						; 6
	vmovapd zmm10, [grpreg+4*64]		;; group multiplier for R3
	vmulpd	zmm10{k1}, zmm10, zmm28		;; fudged group multiplier for R3			; 6-9
	vmovapd zmm22, [grpreg+5*64]		;; group multiplier for R11
	vmulpd	zmm22{k2}, zmm22, zmm28		;; fudged group multiplier for R11			; 7-10

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R4 and R12 fudge factor mask			; 7
	kshiftrw k2, k1, 8			;; R12's fudge						; 8
	vmovapd zmm6, [grpreg+6*64]		;; group multiplier for R4
	vmulpd	zmm6{k1}, zmm6, zmm28		;; fudged group multiplier for R4			; 8-11
	vmovapd zmm23, [grpreg+7*64]		;; group multiplier for R12
	vmulpd	zmm23{k2}, zmm23, zmm28		;; fudged group multiplier for R12			; 9-12


	mov	bl, [maskreg+1*1]		;; Load index into compressed fudges
	mov	r14, [r13+1*8]			;; Load the xor mask
	xor	r14, [r12+rbx*8]		;; Apply the xor mask to 64 bits of fudge flags

	kmovw	k1, r14d			;; Load R5 and R13 fudge factor mask			; 9
	kshiftrw k2, k1, 8			;; R13's fudge						; 10
	vmovapd zmm16, [grpreg+8*64]		;; group multiplier for R5
	vmulpd	zmm16{k1}, zmm16, zmm28		;; fudged group multiplier for R5			; 10-13
	vmovapd zmm24, [grpreg+9*64]		;; group multiplier for R13
	vmulpd	zmm24{k2}, zmm24, zmm28		;; fudged group multiplier for R13			; 11-14

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R6 and R14 fudge factor mask			; 11
	kshiftrw k2, k1, 8			;; R14's fudge						; 12
	vmovapd zmm8, [grpreg+10*64]		;; group multiplier for R6
	vmulpd	zmm8{k1}, zmm8, zmm28		;; fudged group multiplier for R6			; 12-15
	vmovapd zmm25, [grpreg+11*64]		;; group multiplier for R14
	vmulpd	zmm25{k2}, zmm25, zmm28		;; fudged group multiplier for R14			; 13-16

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R7 and R15 fudge factor mask			; 13
	kshiftrw k2, k1, 8			;; R15's fudge						; 14
	vmovapd zmm12, [grpreg+12*64]		;; group multiplier for R7
	vmulpd	zmm12{k1}, zmm12, zmm28		;; fudged group multiplier for R7			; 14-17
	vmovapd zmm26, [grpreg+13*64]		;; group multiplier for R15
	vmulpd	zmm26{k2}, zmm26, zmm28		;; fudged group multiplier for R15			; 15-18

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R8 and R16 fudge factor mask			; 15
	kshiftrw k2, k1, 8			;; R16's fudge						; 16
	vmovapd zmm4, [grpreg+14*64]		;; group multiplier for R8
	vmulpd	zmm4{k1}, zmm4, zmm28		;; fudged group multiplier for R8			; 16-19
	vmovapd zmm27, [grpreg+15*64]		;; group multiplier for R16
	vmulpd	zmm27{k2}, zmm27, zmm28		;; fudged group multiplier for R16			; 17-20

	vmulpd	zmm2, zmm2, [srcreg+d1]		;; apply the fudged group multiplier to R2		; 17-20		n 21
	vmulpd	zmm14, zmm14, [srcreg]		;; apply the fudged group multiplier to R1		; 18-21
	vmulpd	zmm6, zmm6, [srcreg+d2+d1]	;; apply the fudged group multiplier to R4		; 18-21		n 22
	vmulpd	zmm10, zmm10, [srcreg+d2]	;; apply the fudged group multiplier to R3		; 19-22
	vmulpd	zmm8, zmm8, [srcreg+d4+d1]	;; apply the fudged group multiplier to R6		; 19-22		n 23
	vmulpd	zmm16, zmm16, [srcreg+d4]	;; apply the fudged group multiplier to R5		; 20-23
	vmulpd	zmm4, zmm4, [srcreg+d4+d2+d1]	;; apply the fudged group multiplier to R8		; 20-23		n 24
	vmulpd	zmm12, zmm12, [srcreg+d4+d2]	;; apply the fudged group multiplier to R7		; 21-24

	bump	maskreg, maskinc
	bump	grpreg, grpinc

	vmovapd	zmm0, [srcreg+d1+64]		;; r10
	zfmaddpd zmm1, zmm0, zmm21, zmm2	;; r2+r10*fudged_group_mult				; 21-24		n 29
	zfnmaddpd zmm2, zmm0, zmm21, zmm2	;; r2-r10*fudged_group_mult				; 22-25		n 37

	vmovapd	zmm0, [srcreg+d2+d1+64]		;; r12
	zfmaddpd zmm5, zmm0, zmm23, zmm6	;; r4+r12*fudged_group_mult				; 22-25		n 30
	zfnmaddpd zmm6, zmm0, zmm23, zmm6	;; r4-r12*fudged_group_mult				; 23-26		n 38

	vmovapd	zmm0, [srcreg+d4+d1+64]		;; r14
	zfmaddpd zmm7, zmm0, zmm25, zmm8	;; r6+r14*fudged_group_mult				; 23-26		n 30
	zfnmaddpd zmm8, zmm0, zmm25, zmm8	;; r6-r14*fudged_group_mult				; 24-27		n 38

	vmovapd	zmm0, [srcreg+d4+d2+d1+64]	;; r16
	zfmaddpd zmm3, zmm0, zmm27, zmm4	;; r8+r16*fudged_group_mult				; 24-27		n 29
	zfnmaddpd zmm4, zmm0, zmm27, zmm4	;; r8-r16*fudged_group_mult				; 25-28		n 37

	vmovapd	zmm0, [srcreg+64]		;; r9
	zfmaddpd zmm13, zmm0, zmm20, zmm14	;; r1+r9*fudged_group_mult				; 25-28		n 31
	zfnmaddpd zmm14, zmm0, zmm20, zmm14	;; r1-r9*fudged_group_mult				; 26-29		n 42

	vmovapd	zmm0, [srcreg+d4+64]		;; r13
	zfmaddpd zmm15, zmm0, zmm24, zmm16	;; r5+r13*fudged_group_mult				; 26-29		n 31
	zfnmaddpd zmm16, zmm0, zmm24, zmm16	;; r5-r13*fudged_group_mult				; 27-30		n 44

	vmovapd	zmm0, [srcreg+d2+64]		;; r11
	zfmaddpd zmm9, zmm0, zmm22, zmm10	;; r3+r11*fudged_group_mult				; 27-30		n 32
	zfnmaddpd zmm10, zmm0, zmm22, zmm10	;; r3-r11*fudged_group_mult				; 28-31		n 33

	vmovapd	zmm0, [srcreg+d4+d2+64]		;; r15
	zfmaddpd zmm11, zmm0, zmm26, zmm12	;; r7+r15*fudged_group_mult				; 28-31		n 32
	zfnmaddpd zmm12, zmm0, zmm26, zmm12	;; r7-r15*fudged_group_mult				; 29-32		n 33

	L1prefetchw srcreg+L1pd, L1pt
	vaddpd	zmm0, zmm1, zmm3		;; r2++ = (r2+r10)+(r8+r16)				; 29-32		n 34
	vsubpd	zmm1, zmm1, zmm3		;; r2+- = (r2+r10)-(r8+r16)				; 30-33		n 35
	L1prefetchw srcreg+64+L1pd, L1pt
	vaddpd	zmm3, zmm5, zmm7		;; r4++ = (r4+r12)+(r6+r14)				; 30-33		n 34
	vsubpd	zmm5, zmm5, zmm7		;; r4+- = (r4+r12)-(r6+r14)				; 31-34		n 35
	L1prefetchw srcreg+d1+L1pd, L1pt
	vaddpd	zmm7, zmm13, zmm15		;; r1++ = (r1+r9)+(r5+r13)				; 31-34		n 36
	vsubpd	zmm13, zmm13, zmm15		;; r1+- = (r1+r9)-(r5+r13)				; 32-35		n 39
	L1prefetchw srcreg+d1+64+L1pd, L1pt
	vaddpd	zmm15, zmm9, zmm11		;; r3++ = (r3+r11)+(r7+r15)				; 32-35		n 36
	vsubpd	zmm9, zmm9, zmm11		;; r3+- = (r3+r11)-(r7+r15)				; 33-36		n 40
	L1prefetchw srcreg+d2+L1pd, L1pt
	vaddpd	zmm11, zmm10, zmm12		;; r3-+ = (r3-r11)+(r7-r15)				; 33-36		n 44
	vsubpd	zmm10, zmm10, zmm12		;; r3-- = (r3-r11)-(r7-r15)				; 34-37		n 42

	L1prefetchw srcreg+d2+64+L1pd, L1pt
	vaddpd	zmm12, zmm0, zmm3		;; r2+++ = (r2++) + (r4++)				; 34-37		n 41
	vsubpd	zmm0, zmm0, zmm3		;; r2++- = (r2++) - (r4++)				; 35-38		n 39
	L1prefetchw srcreg+d2+d1+L1pd, L1pt
	vaddpd	zmm3, zmm1, zmm5		;; r2+-+ = (r2+-) + (r4+-)				; 35-38		n 40
	vsubpd	zmm1, zmm1, zmm5		;; r2+-- = (r2+-) - (r4+-)	(final I5)		; 36-39		n 46
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt
	vaddpd	zmm5, zmm7, zmm15		;; r1+++ = (r1++) + (r3++)				; 36-39		n 41
	vsubpd	zmm7, zmm7, zmm15		;; r1++- = (r1++) - (r3++)	(final R5)		; 37-40		n 46

	L1prefetchw srcreg+d4+L1pd, L1pt
	vaddpd	zmm15, zmm2, zmm4		;; r2-+ = (r2-r10)+(r8-r16)				; 37-40		n 45
	vsubpd	zmm2, zmm2, zmm4		;; r2-- = (r2-r10)-(r8-r16)				; 38-41		n 43
	L1prefetchw srcreg+d4+64+L1pd, L1pt
	vaddpd	zmm4, zmm6, zmm8		;; r4-+ = (r4-r12)+(r6-r14)				; 38-41		n 45
	vsubpd	zmm6, zmm6, zmm8		;; r4-- = (r4-r12)-(r6-r14)				; 39-42		n 43

	L1prefetchw srcreg+d4+d1+L1pd, L1pt
	zfmaddpd zmm8, zmm0, zmm31, zmm13	;; R3 = (r1+-) + .707(r2++-)				; 39-42		n 47
	zfnmaddpd zmm0, zmm0, zmm31, zmm13	;; R7 = (r1+-) - .707(r2++-)				; 40-43		n 48

	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt
	zfmaddpd zmm13, zmm3, zmm31, zmm9	;; I3 = .707*(r2+-+) + (r3+-)				; 40-43		n 47
	zfmsubpd zmm3, zmm3, zmm31, zmm9	;; I7 = .707*(r2+-+) - (r3+-)				; 41-44		n 48

	L1prefetchw srcreg+d4+d2+L1pd, L1pt
	vaddpd	zmm9, zmm5, zmm12		;; R1 = (r1+++) + (r2+++)				; 41-44
	vsubpd	zmm5, zmm5, zmm12		;; R9 = (r1+++) - (r2+++)				; 42-45

	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt
	zfmaddpd zmm12, zmm10, zmm31, zmm14	;; r2o = (r1-r9) + .707*(r3--)				; 42-45		n 49
	zfnmaddpd zmm10, zmm10, zmm31, zmm14	;; r4o = (r1-r9) - .707*(r3--)				; 43-46		n 50

	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt
	zfmaddpd zmm14, zmm2, zmm30, zmm6	;; r2e/.383 = .924/.383(r2--) + (r4--)			; 43-46		n 49
	zfnmaddpd zmm6, zmm6, zmm30, zmm2	;; r4e/.383 = (r2--) - .924/.383(r4--)			; 44-47		n 50

	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt
	zfmaddpd zmm2, zmm11, zmm31, zmm16	;; i2o = .707(r3-+) + (r5-r13)				; 44-47		n 51
	zfmsubpd zmm11, zmm11, zmm31, zmm16	;; i4o = .707(r3-+) - (r5-r13)				; 45-48		n 52

	zfmaddpd zmm16, zmm4, zmm30, zmm15	;; i2e/.383 = (r2-+) + .924/.383(r4-+)			; 45-48		n 51
	zfmsubpd zmm15, zmm15, zmm30, zmm4	;; i4e/.383 = .924/.383(r2-+) - (r4-+)			; 46-49		n 52
	zstore	[srcreg], zmm9			;; Store R1						; 45

	vmovapd	zmm27, [screg+3*128+64]		;; cosine/sine for R5/I5 (w^4)
	zfmsubpd zmm4, zmm7, zmm27, zmm1	;; A5 = R5 * cosine/sine - I5				; 46-49		n 53
	zfmaddpd zmm1, zmm1, zmm27, zmm7	;; B5 = I5 * cosine/sine + R5				; 47-50		n 54
	zstore	[srcreg+64], zmm5		;; Store R9						; 46

	vmovapd	zmm27, [screg+1*128+64]		;; cosine/sine for R3/I3 (w^2)
	zfmsubpd zmm7, zmm8, zmm27, zmm13	;; A3 = R3 * cosine/sine - I3				; 47-50		n 54
	zfmaddpd zmm13, zmm13, zmm27, zmm8	;; B3 = I3 * cosine/sine + R3				; 48-51		n 55

	vmovapd	zmm27, [screg+5*128+64]		;; cosine/sine for R7/I7 (w^6)
	zfmsubpd zmm8, zmm0, zmm27, zmm3	;; A7 = R7 * cosine/sine - I7				; 48-51		n 55
	zfmaddpd zmm3, zmm3, zmm27, zmm0	;; B7 = I7 * cosine/sine + R7				; 49-52		n 56

	zfmaddpd zmm0, zmm14, zmm29, zmm12	;; R2 = r2o + .383*r2e					; 49-52		n 56
	zfnmaddpd zmm14, zmm14, zmm29, zmm12	;; R8 = r2o - .383*r2e					; 50-53		n 57
	zfmaddpd zmm12, zmm6, zmm29, zmm10	;; R4 = r4o + .383*r4e					; 50-53		n 58
	zfnmaddpd zmm6, zmm6, zmm29, zmm10	;; R6 = r4o - .383*r4e					; 51-54		n 59

	zfmaddpd zmm10, zmm16, zmm29, zmm2	;; I2 = .383*i2e + i2o					; 51-54		n 56
	zfmsubpd zmm16, zmm16, zmm29, zmm2	;; I8 = .383*i2e - i2o					; 52-55		n 57
	zfmaddpd zmm2, zmm15, zmm29, zmm11	;; I4 = .383*i4e + i4o					; 52-55		n 58
	zfmsubpd zmm15, zmm15, zmm29, zmm11	;; I6 = .383*i4e - i4o					; 53-56		n 59

	vmovapd	zmm27, [screg+3*128]		;; sine for R5/I5
	vmulpd	zmm4, zmm4, zmm27		;; A5 = A5 * sine (final R5)				; 53-56
	vmulpd	zmm1, zmm1, zmm27		;; B5 = B5 * sine (final I5)				; 54-57

	vmovapd	zmm27, [screg+1*128]		;; sine for R3/I3
	vmulpd	zmm7, zmm7, zmm27		;; A3 = A3 * sine (final R3)				; 54-57
	vmulpd	zmm13, zmm13, zmm27		;; B3 = B3 * sine (final I3)				; 55-58

	vmovapd	zmm27, [screg+5*128]		;; sine for R7/I7
	vmulpd	zmm8, zmm8, zmm27		;; A7 = A7 * sine (final R7)				; 55-58
	vmulpd	zmm3, zmm3, zmm27		;; B7 = B7 * sine (final I7)				; 56-59

	vmovapd	zmm27, [screg+0*128+64]		;; cosine/sine for R2/I2 (w^1)
	zfmsubpd zmm11, zmm0, zmm27, zmm10	;; A2 = R2 * cosine/sine - I2				; 56-59		n 60
	zfmaddpd zmm10, zmm10, zmm27, zmm0	;; B2 = I2 * cosine/sine + R2				; 57-60		n 61

	vmovapd	zmm27, [screg+6*128+64]		;; cosine/sine for R8/I8 (w^7)
	zfmsubpd zmm0, zmm14, zmm27, zmm16	;; A8 = R8 * cosine/sine - I8				; 57-60		n 61
	zfmaddpd zmm16, zmm16, zmm27, zmm14	;; B8 = I8 * cosine/sine + R8				; 58-61		n 62
	zstore	[srcreg+d4], zmm4		;; Store R5						; 57

	vmovapd	zmm27, [screg+2*128+64]		;; cosine/sine for R4/I4 (w^3)
	zfmsubpd zmm14, zmm12, zmm27, zmm2	;; A4 = R4 * cosine/sine - I4				; 58-61		n 62
	zfmaddpd zmm2, zmm2, zmm27, zmm12	;; B4 = I4 * cosine/sine + R4				; 59-62		n 63
	zstore	[srcreg+d4+64], zmm1		;; Store I5						; 58

	vmovapd	zmm27, [screg+4*128+64]		;; cosine/sine for R6/I6 (w^5)
	zfmsubpd zmm12, zmm6, zmm27, zmm15	;; A6 = R6 * cosine/sine - I6				; 59-62		n 63
	zfmaddpd zmm15, zmm15, zmm27, zmm6	;; B6 = I6 * cosine/sine + R6				; 60-63		n 64
	zstore	[srcreg+d2], zmm7		;; Store R3						; 58+1

	vmovapd	zmm27, [screg+0*128]		;; sine for R2/I2
	vmulpd	zmm11, zmm11, zmm27		;; A2 = A2 * sine (final R2)				; 60-63
	vmulpd	zmm10, zmm10, zmm27		;; B2 = B2 * sine (final I2)				; 61-64
	zstore	[srcreg+d2+64], zmm13		;; Store I3						; 59+1

	vmovapd	zmm27, [screg+6*128]		;; sine for R8/I8
	vmulpd	zmm0, zmm0, zmm27		;; A8 = A8 * sine (final R8)				; 61-64
	vmulpd	zmm16, zmm16, zmm27		;; B8 = B8 * sine (final I8)				; 62-65
	zstore	[srcreg+d4+d2], zmm8		;; Store R7						; 59+2

	vmovapd	zmm27, [screg+2*128]		;; sine for R4/I4
	vmulpd	zmm14, zmm14, zmm27		;; A4 = A4 * sine (final R4)				; 62-65
	vmulpd	zmm2, zmm2, zmm27		;; B4 = B4 * sine (final I4)				; 63-66
	zstore	[srcreg+d4+d2+64], zmm3		;; Store I7						; 60+2

	vmovapd zmm27, [screg+4*128]		;; sine for R6/I6
	vmulpd	zmm12, zmm12, zmm27		;; A6 = A6 * sine (final R6)				; 63-66
	vmulpd	zmm15, zmm15, zmm27		;; B6 = B6 * sine (final I6)				; 64-67

	bump	screg, scinc
	zstore	[srcreg+d1], zmm11		;; Store R2						; 64
	zstore	[srcreg+d1+64], zmm10		;; Store I2						; 65
	zstore	[srcreg+d4+d2+d1], zmm0		;; Store R8						; 65+1
	zstore	[srcreg+d4+d2+d1+64], zmm16	;; Store I8						; 66+1
	zstore	[srcreg+d2+d1], zmm14		;; Store R4						; 66+2
	zstore	[srcreg+d2+d1+64], zmm2		;; Store I4						; 67+2
	zstore	[srcreg+d4+d1], zmm12		;; Store R6						; 67+3
	zstore	[srcreg+d4+d1+64], zmm15	;; Store I6						; 68+3
	bump	srcreg, srcinc
	ENDM


;;
;; ************************************* 16-reals-unfft variants ******************************************
;;

;; These macros produce 16 reals after doing 4 levels of the inverse FFT applying
;; the sin/cos multipliers beforehand.  The input is 2 real and 7 complex numbers.

;; To calculate a 16-reals inverse FFT, we calculate 16 real values from 16 complex inputs in a brute force way.
;; First we note that the 16 complex values are computed from the 7 complex and 2 real inputs using Hermetian symmetry, thusly:
;; c1 = r1A + 0*i
;; c2 = r2 + i2*i
;; ...
;; c8 = r8 + i8*i
;; c9 = r1B + 0*i
;; c10 = r8 - i8*i
;; ...
;; c16 = r2 - i2*i 
;;
;; The brute force calculations are:
;;
;; c1 + c2 + ... + c16	*  w^-0000000000...
;; c1 + c2 + ... + c16	*  w^-0123456789A...
;; c1 + c2 + ... + c16	*  w^-02468ACE....
;;    ...
;; c1 + c2 + ... + c16	*  w^-...A987654321
;;
;; The sin/cos values (w = 16th root of unity) are:
;; w^-1 = .924 - .383i
;; w^-2 = .707 - .707i
;; w^-3 = .383 - .924i
;; w^-4 = 0 - 1i
;; w^-5 = -.383 - .924i
;; w^-6 = -.707 - .707i
;; w^-7 = -.924 - .383i
;; w^-8 = -1

;; Applying the sin/cos values above, taking advantage of symmetry, and ignoring a lot of multiplies by 2:
;; r1     +(r2+r8)     +(r3+r7)     +(r4+r6) + r5 + r9
;; r1 +.924(r2-r8) +.707(r3-r7) +.383(r4-r6)      - r9 +.383(i2+i8) +.707(i3+i7) +.924(i4+i6) + i5
;; r1 +.707(r2+r8)              -.707(r4+r6) - r5 + r9 +.707(i2-i8)     +(i3-i7) +.707(i4-i6)
;; r1 +.383(r2-r8) -.707(r3-r7) -.924(r4-r6)      - r9 +.924(i2+i8) +.707(i3+i7) -.383(i4+i6) - i5
;; r1                  -(r3+r7)              + r5 + r9     +(i2-i8)                  -(i4-i6)
;; r1 -.383(r2-r8) -.707(r3-r7) +.924(r4-r6)      - r9 +.924(i2+i8) -.707(i3+i7) -.383(i4+i6) + i5
;; r1 -.707(r2+r8)              +.707(r4+r6) - r5 + r9 +.707(i2-i8)     -(i3-i7) +.707(i4-i6)
;; r1 -.924(r2-r8) +.707(r3-r7) -.383(r4-r6)      - r9 +.383(i2+i8) -.707(i3+i7) +.924(i4+i6) - i5
;; r1     -(r2+r8)     +(r3+r7)     -(r4+r6) + r5 + r9
;; ... r10 thru r16 are the same as r8 through r2 but with the sign of the imaginary component changed.
;;
;; Also remember that due to the funny way we do things reals input r1A = r1+r9 and r1B = r1-r9

;; Simplifying yields:
;;R1 = (r1+r9) + r5 +(r3+r7)     +((r2+r8)+(r4+r6))
;;R9 = (r1+r9) + r5 +(r3+r7)     -((r2+r8)+(r4+r6))
;;R5 = (r1+r9) + r5 -(r3+r7)                        +((i2-i8)-(i4-i6))
;;R13= (r1+r9) + r5 -(r3+r7)                        -((i2-i8)-(i4-i6))
;;R3 = (r1+r9) - r5 +(i3-i7) +.707(((r2+r8)-(r4+r6))+((i2-i8)+(i4-i6)))
;;R11= (r1+r9) - r5 +(i3-i7) -.707(((r2+r8)-(r4+r6))+((i2-i8)+(i4-i6)))
;;R7 = (r1+r9) - r5 -(i3-i7) -.707(((r2+r8)-(r4+r6))-((i2-i8)+(i4-i6)))
;;R15= (r1+r9) - r5 -(i3-i7) +.707(((r2+r8)-(r4+r6))-((i2-i8)+(i4-i6)))

;;R2 = (r1-r9) + i5 +.707((r3-r7)+(i3+i7)) +.924((r2-r8)+(i4+i6)) +.383((i2+i8)+(r4-r6))
;;R10= (r1-r9) + i5 +.707((r3-r7)+(i3+i7)) -.924((r2-r8)+(i4+i6)) -.383((i2+i8)+(r4-r6))
;;R6 = (r1-r9) + i5 -.707((r3-r7)+(i3+i7)) -.383((r2-r8)+(i4+i6)) +.924((i2+i8)+(r4-r6))
;;R14= (r1-r9) + i5 -.707((r3-r7)+(i3+i7)) +.383((r2-r8)+(i4+i6)) -.924((i2+i8)+(r4-r6))
;;R4 = (r1-r9) - i5 -.707((r3-r7)-(i3+i7)) +.383((r2-r8)-(i4+i6)) +.924((i2+i8)-(r4-r6))
;;R12= (r1-r9) - i5 -.707((r3-r7)-(i3+i7)) -.383((r2-r8)-(i4+i6)) -.924((i2+i8)-(r4-r6))
;;R8 = (r1-r9) - i5 +.707((r3-r7)-(i3+i7)) -.924((r2-r8)-(i4+i6)) +.383((i2+i8)-(r4-r6))
;;R16= (r1-r9) - i5 +.707((r3-r7)-(i3+i7)) +.924((r2-r8)-(i4+i6)) -.383((i2+i8)-(r4-r6))

;; Uses two sin/cos ptrs.  Used in last levels of 2nd pass in a two-pass FFT
zr8_2sc_sixteen_reals_unfft_preload MACRO
	zr8_16r_unfft_cmn_preload
	ENDM

zr8_2sc_sixteen_reals_unfft MACRO srcreg,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	zr8_16r_unfft_cmn srcreg,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

;; Combined sin/cos data.  Used in mid-levels of 1st pass in a two-pass FFT
zr8_csc_sixteen_reals_unfft_preload MACRO
	zr8_16r_unfft_cmn_preload
	ENDM

zr8_csc_sixteen_reals_unfft MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maxrpt,L1pt,L1pd
	zr8_16r_unfft_cmn srcreg,srcinc,d1,d2,d4,screg+4*128,0,screg,scinc,maxrpt,L1pt,L1pd
	ENDM


zr8_16r_unfft_cmn_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRTHALF
	vbroadcastsd zmm30, ZMM_P924_P383
	vbroadcastsd zmm29, ZMM_P383
	ENDM

zr8_16r_unfft_cmn MACRO srcreg,srcinc,d1,d2,d4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	vmovapd	zmm28, [screg2+0*128+64]	;; cosine/sine for R3/I3 (w^2 = complex w^1)
	vmovapd	zmm2, [srcreg+d2]		;; R3
	vmovapd	zmm10, [srcreg+d2+64]		;; I3
	zfmaddpd zmm16, zmm2, zmm28, zmm10	;; A3 = R3 * cosine/sine + I3				; 1-4		n 5
	zfmsubpd zmm10, zmm10, zmm28, zmm2	;; B3 = I3 * cosine/sine - R3				; 1-4		n 5

	vmovapd	zmm28, [screg1+0*128+64]	;; cosine/sine for R2/I2 (w^1)
	vmovapd	zmm1, [srcreg+d1]		;; R2
	vmovapd	zmm9, [srcreg+d1+64]		;; I2
	zfmaddpd zmm2, zmm1, zmm28, zmm9	;; A2 = R2 * cosine/sine + I2				; 2-5		n 7
	zfmsubpd zmm9, zmm9, zmm28, zmm1	;; B2 = I2 * cosine/sine - R2				; 2-5		n 7

	vmovapd	zmm28, [screg1+1*128+64]	;; cosine/sine for R4/I4 (w^3)
	vmovapd	zmm3, [srcreg+d2+d1]		;; R4
	vmovapd	zmm11, [srcreg+d2+d1+64]	;; I4
	zfmaddpd zmm1, zmm3, zmm28, zmm11	;; A4 = R4 * cosine/sine + I4				; 3-6		n 9
	zfmsubpd zmm11, zmm11, zmm28, zmm3	;; B4 = I4 * cosine/sine - R4				; 3-6		n 9

	vmovapd	zmm28, [screg2+2*128+64]	;; cosine/sine for R7/I7 (w^6 = complex w^3)
	vmovapd	zmm6, [srcreg+d4+d2]		;; R7
	vmovapd	zmm14, [srcreg+d4+d2+64]	;; I7
	zfmaddpd zmm3, zmm6, zmm28, zmm14	;; A7 = R7 * cosine/sine + I7				; 4-7		n 11
	zfmsubpd zmm14, zmm14, zmm28, zmm6	;; B7 = I7 * cosine/sine - R7				; 4-7		n 12

	vmovapd	zmm28, [screg2+0*128]		;; sine for R3/I3
	vmulpd	zmm16, zmm16, zmm28		;; R3 = A3 * sine					; 5-8		n 11
	vmulpd	zmm10, zmm10, zmm28		;; I3 = B3 * sine					; 5-8		n 12

	vmovapd	zmm28, [screg1+3*128+64]	;; cosine/sine for R8/I8 (w^7)
	vmovapd	zmm7, [srcreg+d4+d2+d1]		;; R8
	vmovapd	zmm15, [srcreg+d4+d2+d1+64]	;; I8
	zfmaddpd zmm6, zmm7, zmm28, zmm15	;; A8 = R8 * cosine/sine + I8				; 6-9		n 13
	zfmsubpd zmm15, zmm15, zmm28, zmm7	;; B8 = I8 * cosine/sine - R8				; 6-9		n 14

	vmovapd	zmm28, [screg1+0*128]		;; sine for R2/I2
	vmulpd	zmm2, zmm2, zmm28		;; R2 = A2 * sine					; 7-10		n 13
	vmulpd	zmm9, zmm9, zmm28		;; I2 = B2 * sine					; 7-10		n 14

	vmovapd	zmm28, [screg1+2*128+64]	;; cosine/sine for R6/I6 (w^5)
	vmovapd	zmm5, [srcreg+d4+d1]		;; R6
	vmovapd	zmm13, [srcreg+d4+d1+64]	;; I6
	zfmaddpd zmm7, zmm5, zmm28, zmm13	;; A6 = R6 * cosine/sine + I6				; 8-11		n 15
	zfmsubpd zmm13, zmm13, zmm28, zmm5	;; B6 = I6 * cosine/sine - R6				; 8-11		n 16

	vmovapd	zmm28, [screg1+1*128]		;; sine for R4/I4
	vmulpd	zmm1, zmm1, zmm28		;; R4 = A4 * sine					; 9-12		n 15
	vmulpd	zmm11, zmm11, zmm28		;; I4 = B4 * sine					; 9-12		n 16

	vmovapd	zmm28, [screg2+1*128+64]	;; cosine/sine for R5/I5 (w^4 = complex w^2)
	vmovapd	zmm4, [srcreg+d4]		;; R5
	vmovapd	zmm12, [srcreg+d4+64]		;; I5
	zfmaddpd zmm5, zmm4, zmm28, zmm12	;; A5 = R5 * cosine/sine + I5				; 10-13		n 17
	zfmsubpd zmm12, zmm12, zmm28, zmm4	;; B5 = I5 * cosine/sine - R5				; 10-13		n 18

	vmovapd	zmm28, [screg2+2*128]		;; sine for R7/I7
	zfnmaddpd zmm4, zmm3, zmm28, zmm16	;; R3-(R7 = A7 * sine)					; 11-14		n 19
	zfmaddpd zmm3, zmm3, zmm28, zmm16	;; R3+(R7 = A7 * sine)					; 11-14		n 22

	vmovapd	zmm27, [screg1+3*128]		;; sine for R8/I8
	zfmaddpd zmm16, zmm14, zmm28, zmm10	;; I3+(I7 = B7 * sine)					; 12-15		n 19
	zfnmaddpd zmm14, zmm14, zmm28, zmm10	;; I3-(I7 = B7 * sine)					; 12-15		n 23

	vmovapd	zmm26, [screg1+2*128]		;; sine for R6/I6
	zfmaddpd zmm10, zmm6, zmm27, zmm2	;; R2+(R8 = A8 * sine)					; 13-16		n 20
	zfnmaddpd zmm6, zmm6, zmm27, zmm2	;; R2-(R8 = A8 * sine)					; 13-16		n 24

	vmovapd	zmm25, [screg2+1*128]		;; sine for R5/I5
	zfnmaddpd zmm2, zmm15, zmm27, zmm9	;; I2-(I8 = B8 * sine)					; 14-17		n 21
	zfmaddpd zmm15, zmm15, zmm27, zmm9	;; I2+(I8 = B8 * sine)					; 14-17		n 25

	vmovapd	zmm0, [srcreg]			;; R1+R9
	zfmaddpd zmm9, zmm7, zmm26, zmm1	;; R4+(R6 = A6 * sine)					; 15-18		n 20
	zfnmaddpd zmm7, zmm7, zmm26, zmm1	;; R4-(R6 = A6 * sine)					; 15-18		n 25

	vmovapd	zmm8, [srcreg+64]		;; R1-R9
	zfnmaddpd zmm1, zmm13, zmm26, zmm11	;; I4-(I6 = B6 * sine)					; 16-19		n 21
	zfmaddpd zmm13, zmm13, zmm26, zmm11	;; I4+(I6 = B6 * sine)					; 16-19		n 24

	L1prefetchw srcreg+L1pd, L1pt
	zfmaddpd zmm11, zmm5, zmm25, zmm0	;; r1++ = (r1+r9) + r5*sine				; 17-20		n 22
	zfnmaddpd zmm5, zmm5, zmm25, zmm0	;; r1+- = (r1+r9) - r5*sine				; 17-20		n 23

	L1prefetchw srcreg+d1+L1pd, L1pt
	zfmaddpd zmm0, zmm12, zmm25, zmm8	;; r1-+ = (r1-r9) + i5*sine				; 18-21		n 29
	zfnmaddpd zmm12, zmm12, zmm25, zmm8	;; r1-- = (r1-r9) - i5*sine				; 18-21		n 31

	L1prefetchw srcreg+d2+L1pd, L1pt
	vaddpd	zmm8, zmm4, zmm16		;; r3-+ = (r3-r7) + (i3+i7)				; 19-22		n 29
	vsubpd	zmm4, zmm4, zmm16		;; r3-- = (r3-r7) - (i3+i7)				; 19-22		n 31

	L1prefetchw srcreg+d4+L1pd, L1pt
	vaddpd	zmm16, zmm10, zmm9		;; r2++ = (r2+r8) + (r4+r6)				; 20-23		n 26
	vsubpd	zmm10, zmm10, zmm9		;; r2+- = (r2+r8) - (r4+r6)				; 20-23		n 28

	L1prefetchw srcreg+d2+d1+L1pd, L1pt
	vsubpd	zmm9, zmm2, zmm1		;; i2-- = (i2-i8) - (i4-i6)				; 21-24		n 27
	vaddpd	zmm2, zmm2, zmm1		;; i2-+ = (i2-i8) + (i4-i6)				; 21-24		n 28

	L1prefetchw srcreg+d4+d1+L1pd, L1pt
	vaddpd	zmm1, zmm11, zmm3		;; r1+++ = (r1++) + (r3+r7)				; 22-25		n 26
	vsubpd	zmm11, zmm11, zmm3		;; r1++- = (r1++) - (r3+r7)				; 22-25		n 27

	L1prefetchw srcreg+d4+d2+L1pd, L1pt
	vaddpd	zmm3, zmm5, zmm14		;; r1+-+ = (r1+-) + (i3-i7)				; 23-26		n 33
	vsubpd	zmm5, zmm5, zmm14		;; r1+-- = (r1+-) - (i3-i7)				; 23-26		n 34

	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt
	vaddpd	zmm14, zmm6, zmm13		;; r2-+ = (r2-r8) + (i4+i6)				; 24-27		n 30
	vsubpd	zmm6, zmm6, zmm13		;; r2-- = (r2-r8) - (i4+i6)				; 24-27		n 32

	L1prefetchw srcreg+64+L1pd, L1pt
	vaddpd	zmm13, zmm15, zmm7		;; i2++ = (i2+i8) + (r4-r6)				; 25-28		n 30
	vsubpd	zmm15, zmm15, zmm7		;; i2+- = (i2+i8) - (r4-r6)				; 25-28		n 32

	L1prefetchw srcreg+d1+64+L1pd, L1pt
	vaddpd	zmm7, zmm1, zmm16		;; R1 = (r1+++) + (r2++)				; 26-29
	vsubpd	zmm1, zmm1, zmm16		;; R9 = (r1+++) - (r2++)				; 26-29

	L1prefetchw srcreg+d2+64+L1pd, L1pt
	vaddpd	zmm16, zmm11, zmm9		;; R5  = (r1++-) + (i2--)				; 27-30
	vsubpd	zmm11, zmm11, zmm9		;; R13 = (r1++-) - (i2--)				; 27-30

	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt
	vaddpd	zmm9, zmm10, zmm2		;; r2+-+ = (r2+-) + (i2-+)				; 28-31		n 33
	vsubpd	zmm10, zmm10, zmm2		;; r2+-- = (r2+-) - (i2-+)				; 28-31		n 34

	L1prefetchw srcreg+d4+64+L1pd, L1pt
	zfmaddpd zmm2, zmm8, zmm31, zmm0	;; r2_10o = (r1-+) + .707(r3-+)				; 29-32		n 35
	zfnmaddpd zmm8, zmm8, zmm31, zmm0	;; r6_14o = (r1-+) - .707(r3-+)				; 29-32		n 36

	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt
	zfmaddpd zmm0, zmm14, zmm30, zmm13	;; r2_10e/.383 = .924/.383(r2-+) + (i2++)		; 30-33		n 35
	zfmsubpd zmm13, zmm13, zmm30, zmm14	;; r6_14e/.383 = .924/.383(i2++) - (r2-+)		; 30-33		n 36
	zstore	[srcreg], zmm7			;; Save R1						; 30

	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt
	zfnmaddpd zmm14, zmm4, zmm31, zmm12	;; r4_12o = (r1--) - .707(r3--)				; 31-34		n 37
	zfmaddpd zmm4, zmm4, zmm31, zmm12	;; r8_16o = (r1--) + .707(r3--)				; 31-34		n 38
	zstore	[srcreg+64], zmm1		;; Save R9						; 30+1

	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt
	zfmaddpd zmm12, zmm15, zmm30, zmm6	;; r4_12e/.383 = .924/.383(i2+-) + (r2--)		; 32-35		n 37
	zfnmaddpd zmm6, zmm6, zmm30, zmm15	;; r8_16e/.383 = (i2+-) - .924/.383(r2--)		; 32-35		n 38
	zstore	[srcreg+d4], zmm16		;; Save R5						; 31+1

	zfmaddpd zmm15, zmm9, zmm31, zmm3	;; R3  = (r1+-+) + .707(r2+-+)				; 33-36
	zfnmaddpd zmm9, zmm9, zmm31, zmm3	;; R11 = (r1+-+) - .707(r2+-+)				; 33-36
	zstore	[srcreg+d4+64], zmm11		;; Save R13						; 31+2

	zfnmaddpd zmm3, zmm10, zmm31, zmm5	;; R7  = (r1+--) - .707(r2+--)				; 34-37
	zfmaddpd zmm10, zmm10, zmm31, zmm5	;; R15 = (r1+--) + .707(r2+--)				; 34-37
	bump	screg1, scinc1

	zfmaddpd zmm5, zmm0, zmm29, zmm2	;; R2  = r2_10o + .383*r2_10e				; 35-38
	zfnmaddpd zmm0, zmm0, zmm29, zmm2	;; R10 = r2_10o - .383*r2_10e				; 35-38
	bump	screg2, scinc2

	zfmaddpd zmm2, zmm13, zmm29, zmm8	;; R6  = r6_14o + .383*r6_14e				; 36-39
	zfnmaddpd zmm13, zmm13, zmm29, zmm8	;; R14 = r6_14o - .383*r6_14e				; 36-39

	zfmaddpd zmm8, zmm12, zmm29, zmm14	;; R4  = r4_12o + .383*r4_12e				; 37-40
	zfnmaddpd zmm12, zmm12, zmm29, zmm14	;; R12 = r4_12o - .383*r4_12e				; 37-40
	zstore	[srcreg+d2], zmm15		;; Save R3						; 37

	zfmaddpd zmm14, zmm6, zmm29, zmm4	;; R8  = r8_16o + .383*r8_16e				; 38-41
	zfnmaddpd zmm6, zmm6, zmm29, zmm4	;; R16 = r8_16o - .383*r8_16e				; 38-41

	zstore	[srcreg+d2+64], zmm9		;; Save R11						; 37+1
	zstore	[srcreg+d4+d2], zmm3		;; Save R7						; 38+1
	zstore	[srcreg+d4+d2+64], zmm10	;; Save R15						; 38+2
	zstore	[srcreg+d1], zmm5		;; Save R2						; 39+2
	zstore	[srcreg+d1+64], zmm0		;; Save R10						; 39+3
	zstore	[srcreg+d4+d1], zmm2		;; Save R6						; 40+3
	zstore	[srcreg+d4+d1+64], zmm13	;; Save R14						; 40+4
	zstore	[srcreg+d2+d1], zmm8		;; Save R4						; 41+4
	zstore	[srcreg+d2+d1+64], zmm12	;; Save R12						; 41+5
	zstore	[srcreg+d4+d2+d1], zmm14	;; Save R8						; 42+5
	zstore	[srcreg+d4+d2+d1+64], zmm6	;; Save R16						; 42+6
	bump	srcreg, srcinc
	ENDM


;; Used in last levels of pass 1 in a two-pass FFT
zr8_wpn_sixteen_reals_last_unfft_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRTHALF
	vbroadcastsd zmm30, ZMM_P924_P383
	vbroadcastsd zmm29, ZMM_P383
	vbroadcastsd zmm28, ZMM_B
	ENDM

zr8_wpn_sixteen_reals_last_unfft MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maskreg,maskinc,maxrpt,L1pt,L1pd
	vmovapd	zmm2, [srcreg+d2]		;; R3
	vmovapd	zmm10, [srcreg+d2+64]		;; I3
	vmovapd	zmm27, [screg+1*128+64]		;; cosine/sine for R3/I3 (w^2)
	zfmaddpd zmm16, zmm2, zmm27, zmm10	;; A3 = R3 * cosine/sine + I3				; 1-4		n 5
	zfmsubpd zmm10, zmm10, zmm27, zmm2	;; B3 = I3 * cosine/sine - R3				; 1-4		n 5

	vmovapd	zmm1, [srcreg+d1]		;; R2
	vmovapd	zmm9, [srcreg+d1+64]		;; I2
	vmovapd	zmm27, [screg+0*128+64]		;; cosine/sine for R2/I2 (w^1)
	zfmaddpd zmm2, zmm1, zmm27, zmm9	;; A2 = R2 * cosine/sine + I2				; 2-5		n 7
	zfmsubpd zmm9, zmm9, zmm27, zmm1	;; B2 = I2 * cosine/sine - R2				; 2-5		n 7

	vmovapd	zmm3, [srcreg+d2+d1]		;; R4
	vmovapd	zmm11, [srcreg+d2+d1+64]	;; I4
	vmovapd	zmm27, [screg+2*128+64]		;; cosine/sine for R4/I4 (w^3)
	zfmaddpd zmm1, zmm3, zmm27, zmm11	;; A4 = R4 * cosine/sine + I4				; 3-6		n 9
	zfmsubpd zmm11, zmm11, zmm27, zmm3	;; B4 = I4 * cosine/sine - R4				; 3-6		n 9

	vmovapd	zmm6, [srcreg+d4+d2]		;; R7
	vmovapd	zmm14, [srcreg+d4+d2+64]	;; I7
	vmovapd	zmm27, [screg+5*128+64]		;; cosine/sine for R7/I7 (w^6)
	zfmaddpd zmm3, zmm6, zmm27, zmm14	;; A7 = R7 * cosine/sine + I7				; 4-7		n 11
	zfmsubpd zmm14, zmm14, zmm27, zmm6	;; B7 = I7 * cosine/sine - R7				; 4-7		n 12

	vmovapd	zmm27, [screg+1*128]		;; sine for R3/I3
	vmulpd	zmm16, zmm16, zmm27		;; R3 = A3 * sine					; 5-8		n 11
	vmulpd	zmm10, zmm10, zmm27		;; I3 = B3 * sine					; 5-8		n 12

	vmovapd	zmm7, [srcreg+d4+d2+d1]		;; R8
	vmovapd	zmm15, [srcreg+d4+d2+d1+64]	;; I8
	vmovapd	zmm27, [screg+6*128+64]		;; cosine/sine for R8/I8 (w^7)
	zfmaddpd zmm6, zmm7, zmm27, zmm15	;; A8 = R8 * cosine/sine + I8				; 6-9		n 13
	zfmsubpd zmm15, zmm15, zmm27, zmm7	;; B8 = I8 * cosine/sine - R8				; 6-9		n 14

	vmovapd	zmm27, [screg+0*128]		;; sine for R2/I2
	vmulpd	zmm2, zmm2, zmm27		;; R2 = A2 * sine					; 7-10		n 13
	vmulpd	zmm9, zmm9, zmm27		;; I2 = B2 * sine					; 7-10		n 14

	vmovapd	zmm5, [srcreg+d4+d1]		;; R6
	vmovapd	zmm13, [srcreg+d4+d1+64]	;; I6
	vmovapd	zmm27, [screg+4*128+64]		;; cosine/sine for R6/I6 (w^5)
	zfmaddpd zmm7, zmm5, zmm27, zmm13	;; A6 = R6 * cosine/sine + I6				; 8-11		n 15
	zfmsubpd zmm13, zmm13, zmm27, zmm5	;; B6 = I6 * cosine/sine - R6				; 8-11		n 16

	vmovapd	zmm27, [screg+2*128]		;; sine for R4/I4
	vmulpd	zmm1, zmm1, zmm27		;; R4 = A4 * sine					; 9-12		n 15
	vmulpd	zmm11, zmm11, zmm27		;; I4 = B4 * sine					; 9-12		n 16

	vmovapd	zmm4, [srcreg+d4]		;; R5
	vmovapd	zmm12, [srcreg+d4+64]		;; I5
	vmovapd	zmm27, [screg+3*128+64]		;; cosine/sine for R5/I5 (w^4)
	zfmaddpd zmm5, zmm4, zmm27, zmm12	;; A5 = R5 * cosine/sine + I5				; 10-13		n 17
	zfmsubpd zmm12, zmm12, zmm27, zmm4	;; B5 = I5 * cosine/sine - R5				; 10-13		n 18

	vmovapd	zmm27, [screg+5*128]		;; sine for R7/I7
	zfnmaddpd zmm4, zmm3, zmm27, zmm16	;; R3-(R7 = A7 * sine)					; 11-14		n 19
	zfmaddpd zmm3, zmm3, zmm27, zmm16	;; R3+(R7 = A7 * sine)					; 11-14		n 22

	zfmaddpd zmm16, zmm14, zmm27, zmm10	;; I3+(I7 = B7 * sine)					; 12-15		n 19
	zfnmaddpd zmm14, zmm14, zmm27, zmm10	;; I3-(I7 = B7 * sine)					; 12-15		n 23

	vmovapd	zmm27, [screg+6*128]		;; sine for R8/I8
	zfmaddpd zmm10, zmm6, zmm27, zmm2	;; R2+(R8 = A8 * sine)					; 13-16		n 20
	zfnmaddpd zmm6, zmm6, zmm27, zmm2	;; R2-(R8 = A8 * sine)					; 13-16		n 25

	zfnmaddpd zmm2, zmm15, zmm27, zmm9	;; I2-(I8 = B8 * sine)					; 14-17		n 21
	zfmaddpd zmm15, zmm15, zmm27, zmm9	;; I2+(I8 = B8 * sine)					; 14-17		n 27

	vmovapd	zmm27, [screg+4*128]		;; sine for R6/I6
	zfmaddpd zmm9, zmm7, zmm27, zmm1	;; R4+(R6 = A6 * sine)					; 15-18		n 20
	zfnmaddpd zmm7, zmm7, zmm27, zmm1	;; R4-(R6 = A6 * sine)					; 15-18		n 27

	zfnmaddpd zmm1, zmm13, zmm27, zmm11	;; I4-(I6 = B6 * sine)					; 16-19		n 21
	zfmaddpd zmm13, zmm13, zmm27, zmm11	;; I4+(I6 = B6 * sine)					; 16-19		n 25

	vmovapd	zmm0, [srcreg]			;; R1+R9
	vmovapd	zmm27, [screg+3*128]		;; sine for R5/I5
	zfmaddpd zmm11, zmm5, zmm27, zmm0	;; r1++ = (r1+r9) + r5*sine				; 17-20		n 22
	zfnmaddpd zmm5, zmm5, zmm27, zmm0	;; r1+- = (r1+r9) - r5*sine				; 17-20		n 23

	vmovapd	zmm8, [srcreg+64]		;; R1-R9
	zfmaddpd zmm0, zmm12, zmm27, zmm8	;; r1-+ = (r1-r9) + i5*sine				; 18-21		n 33
	zfnmaddpd zmm12, zmm12, zmm27, zmm8	;; r1-- = (r1-r9) - i5*sine				; 18-21		n 37
	bump	screg, scinc

	vaddpd	zmm8, zmm4, zmm16		;; r3-+ = (r3-r7) + (i3+i7)				; 19-22		n 33
	vsubpd	zmm4, zmm4, zmm16		;; r3-- = (r3-r7) - (i3+i7)				; 19-22		n 37
	mov	bl, [maskreg+0*1]		;; Load index into compressed fudges

	vaddpd	zmm16, zmm10, zmm9		;; r2++ = (r2+r8) + (r4+r6)				; 20-23		n 28
	vsubpd	zmm10, zmm10, zmm9		;; r2+- = (r2+r8) - (r4+r6)				; 20-23		n 31
	mov	r14, [r13+0*8]			;; Load the xor mask

	vsubpd	zmm9, zmm2, zmm1		;; i2-- = (i2-i8) - (i4-i6)				; 21-24		n 30
	vaddpd	zmm2, zmm2, zmm1		;; i2-+ = (i2-i8) + (i4-i6)				; 21-24		n 31
	xor	r14, [r12+rbx*8]		;; Apply the xor mask to 64 bits of fudge flags

	vaddpd	zmm1, zmm11, zmm3		;; r1+++ = (r1++) + (r3+r7)				; 22-25		n 28
	vsubpd	zmm11, zmm11, zmm3		;; r1++- = (r1++) - (r3+r7)				; 22-25		n 30
	mov	bl, [maskreg+1*1]		;; Load index into compressed fudges
	bump	maskreg, maskinc

	kmovw	k1, r14d			;; Load R1 and R9 fudge factor mask			; 23		n 32
	vaddpd	zmm3, zmm5, zmm14		;; r1+-+ = (r1+-) + (i3-i7)				; 23-26		n 40
	shr	r14, 16				;; Next 16 bits of fudge flags

	kmovw	k2, r14d			;; Load R2 and R10 fudge factor mask			; 24		n 47
	vsubpd	zmm5, zmm5, zmm14		;; r1+-- = (r1+-) - (i3-i7)				; 24-27		n 41
	shr	r14, 16				;; Next 16 bits of fudge flags

	kmovw	k3, r14d			;; Load R3 and R11 fudge factor mask			; 25		n 44
	vaddpd	zmm14, zmm6, zmm13		;; r2-+ = (r2-r8) + (i4+i6)				; 25-28		n 35
	shr	r14, 16				;; Next 16 bits of fudge flags

	kmovw	k4, r14d			;; Load R4 and R12 fudge factor mask			; 26		n 51
	vsubpd	zmm6, zmm6, zmm13		;; r2-- = (r2-r8) - (i4+i6)				; 26-29		n 39
	mov	r14, [r13+1*8]			;; Load the xor mask

	vaddpd	zmm13, zmm15, zmm7		;; i2++ = (i2+i8) + (r4-r6)				; 27-30		n 35
	vsubpd	zmm15, zmm15, zmm7		;; i2+- = (i2+i8) - (r4-r6)				; 27-30		n 39
	xor	r14, [r12+rbx*8]		;; Apply the xor mask to 64 bits of fudge flags

	kmovw	k5, r14d			;; Load R5 and R13 fudge factor mask			; 28		n 36
	vaddpd	zmm7, zmm1, zmm16		;; R1 = (r1+++) + (r2++)				; 28-31		n 32
	shr	r14, 16				;; Next 16 bits of fudge flags
	L1prefetchw srcreg+L1pd, L1pt

	kmovw	k6, r14d			;; Load R6 and R14 fudge factor mask			; 29		n 50
	vsubpd	zmm1, zmm1, zmm16		;; R9 = (r1+++) - (r2++)				; 29-32		n 34
	shr	r14, 16				;; Next 16 bits of fudge flags
	L1prefetchw srcreg+64+L1pd, L1pt

	kmovw	k7, r14d			;; Load R7 and R15 fudge factor mask			; 30		n 45
	vaddpd	zmm16, zmm11, zmm9		;; R5  = (r1++-) + (i2--)				; 30-33		n 36
	shr	r14, 16				;; Next 16 bits of fudge flags
	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	zmm11, zmm11, zmm9		;; R13 = (r1++-) - (i2--)				; 31-34		n 38
	vaddpd	zmm9, zmm10, zmm2		;; r2+-+ = (r2+-) + (i2-+)				; 31-34		n 40
	L1prefetchw srcreg+d1+64+L1pd, L1pt

	vmulpd	zmm7{k1}, zmm7, zmm28		;; apply fudge multiplier for R1			; 32-35
	vsubpd	zmm10, zmm10, zmm2		;; r2+-- = (r2+-) - (i2-+)				; 32-35		n 41
	L1prefetchw srcreg+d2+L1pd, L1pt

	kshiftrw k1, k1, 8			;; R9's fudge						; 33		n 34
	zfmaddpd zmm2, zmm8, zmm31, zmm0	;; r2_10o = (r1-+) + .707(r3-+)				; 33-36		n 42
	L1prefetchw srcreg+d2+64+L1pd, L1pt

	vmulpd	zmm1{k1}, zmm1, zmm28		;; apply fudge multiplier for R9			; 34-37
	zfnmaddpd zmm8, zmm8, zmm31, zmm0	;; r6_14o = (r1-+) - .707(r3-+)				; 34-37		n 43
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	kmovw	k1, r14d			;; Load R8 and R16 fudge factor mask			; 35		n 52
	zfmaddpd zmm0, zmm14, zmm30, zmm13	;; r2_10e/.383 = .924/.383(r2-+) + (i2++)		; 35-38		n 42
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt

	vmulpd	zmm16{k5}, zmm16, zmm28		;; apply fudge multiplier for R5			; 36-39
	zfmsubpd zmm13, zmm13, zmm30, zmm14	;; r6_14e/.383 = .924/.383(i2++) - (r2-+)		; 36-39		n 43
	zstore	[srcreg], zmm7			;; Save R1						; 36
	L1prefetchw srcreg+d4+L1pd, L1pt

	kshiftrw k5, k5, 8			;; R13's fudge						; 37		n 38
	zfnmaddpd zmm14, zmm4, zmm31, zmm12	;; r4_12o = (r1--) - .707(r3--)				; 37-40		n 44
	L1prefetchw srcreg+d4+64+L1pd, L1pt

	vmulpd	zmm11{k5}, zmm11, zmm28		;; apply fudge multiplier for R13			; 38-41
	zfmaddpd zmm4, zmm4, zmm31, zmm12	;; r8_16o = (r1--) + .707(r3--)				; 38-41		n 49
	zstore	[srcreg+64], zmm1		;; Save R9						; 38
	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	zfmaddpd zmm12, zmm15, zmm30, zmm6	;; r4_12e/.383 = .924/.383(i2+-) + (r2--)		; 39-42		n 44
	zfnmaddpd zmm6, zmm6, zmm30, zmm15	;; r8_16e/.383 = (i2+-) - .924/.383(r2--)		; 39-42		n 49
	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt

	zfmaddpd zmm15, zmm9, zmm31, zmm3	;; R3  = (r1+-+) + .707(r2+-+)				; 40-43		n 44
	zfnmaddpd zmm9, zmm9, zmm31, zmm3	;; R11 = (r1+-+) - .707(r2+-+)				; 40-43		n 46
	zstore	[srcreg+d4], zmm16		;; Save R5						; 40
	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	zfnmaddpd zmm3, zmm10, zmm31, zmm5	;; R7  = (r1+--) - .707(r2+--)				; 41-44		n 45
	zfmaddpd zmm10, zmm10, zmm31, zmm5	;; R15 = (r1+--) + .707(r2+--)				; 41-44		n 49
	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt

	zfmaddpd zmm5, zmm0, zmm29, zmm2	;; R2  = r2_10o + .383*r2_10e				; 42-45		n 47
	zfnmaddpd zmm0, zmm0, zmm29, zmm2	;; R10 = r2_10o - .383*r2_10e				; 42-45		n 49
	zstore	[srcreg+d4+64], zmm11		;; Save R13						; 42
	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	zfmaddpd zmm2, zmm13, zmm29, zmm8	;; R6  = r6_14o + .383*r6_14e				; 43-46		n 50
	zfnmaddpd zmm13, zmm13, zmm29, zmm8	;; R14 = r6_14o - .383*r6_14e				; 43-46		n 53
	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt

	vmulpd	zmm15{k3}, zmm15, zmm28		;; apply fudge multiplier for R3			; 44-47
	zfmaddpd zmm8, zmm12, zmm29, zmm14	;; R4  = r4_12o + .383*r4_12e				; 44-47		n 51

	kshiftrw k3, k3, 8			;; R11's fudge						; 45		n 46
	vmulpd	zmm3{k7}, zmm3, zmm28		;; apply fudge multiplier for R7			; 45-48

	kshiftrw k7, k7, 8			;; R15's fudge						; 46		n 49
	vmulpd	zmm9{k3}, zmm9, zmm28		;; apply fudge multiplier for R11			; 46-49

	zfnmaddpd zmm12, zmm12, zmm29, zmm14	;; R12 = r4_12o - .383*r4_12e				; 47-50		n 54
	vmulpd	zmm5{k2}, zmm5, zmm28		;; apply fudge multiplier for R2			; 47-50

	kshiftrw k2, k2, 8			;; R10's fudge						; 48		n 49
	zfmaddpd zmm14, zmm6, zmm29, zmm4	;; R8  = r8_16o + .383*r8_16e				; 48-51		n 52
	zstore	[srcreg+d2], zmm15		;; Save R3						; 48

	vmulpd	zmm10{k7}, zmm10, zmm28		;; apply fudge multiplier for R15			; 49-52
	vmulpd	zmm0{k2}, zmm0, zmm28		;; apply fudge multiplier for R10			; 49-52
	zstore	[srcreg+d4+d2], zmm3		;; Save R7						; 49

	zfnmaddpd zmm6, zmm6, zmm29, zmm4	;; R16 = r8_16o - .383*r8_16e				; 50-53		n 54
	vmulpd	zmm2{k6}, zmm2, zmm28		;; apply fudge multiplier for R6			; 50-53
 	zstore	[srcreg+d2+64], zmm9		;; Save R11						; 50

	kshiftrw k6, k6, 8			;; R14's fudge						; 51		n 53
	vmulpd	zmm8{k4}, zmm8, zmm28		;; apply fudge multiplier for R4			; 51-54
	zstore	[srcreg+d1], zmm5		;; Save R2						; 51

	kshiftrw k4, k4, 8			;; R12's fudge						; 52		n 54
	vmulpd	zmm14{k1}, zmm14, zmm28		;; apply fudge multiplier for R8			; 52-55

	kshiftrw k1, k1, 8			;; R16's fudge						; 53		n 54
	vmulpd	zmm13{k6}, zmm13, zmm28		;; apply fudge multiplier for R14			; 53-56
	zstore	[srcreg+d4+d2+64], zmm10	;; Save R15						; 53

	vmulpd	zmm12{k4}, zmm12, zmm28		;; apply fudge multiplier for R12			; 54-57
	vmulpd	zmm6{k1}, zmm6, zmm28		;; apply fudge multiplier for R16			; 54-57
	zstore	[srcreg+d1+64], zmm0		;; Save R10						; 53+1

	zstore	[srcreg+d4+d1], zmm2		;; Save R6						; 54+1
	zstore	[srcreg+d2+d1], zmm8		;; Save R4						; 55+1
	zstore	[srcreg+d4+d2+d1], zmm14	;; Save R8						; 56+1
	zstore	[srcreg+d2+d1+64], zmm12	;; Save R12						; 58+1
	zstore	[srcreg+d4+d1+64], zmm13	;; Save R14						; 57+1
	zstore	[srcreg+d4+d2+d1+64], zmm6	;; Save R16						; 58+2
	bump	srcreg, srcinc
	ENDM


;;
;; ********************************* reduced sin/cos sixteen-reals-fft8 and sixteen-reals-unfft8 variants **************************************
;;

;; These versions use registers for distances between blocks.  This lets us share pass1 code.
;; Weights must be applied.  The complex s/c data referenced by screg1 has already been weighted.
;; The real s/c data referenced by screg2 has not been weighted.

zr8_rsc_complex_and_real_preload MACRO
	mov	eax, 00110011b			;; For vblendmpd during swizzle
	kmovw	k7, eax
	vpmovzxbq zmm31, ZMM_PERMUTE1		;; zmm31 = 8+4 0+4 8+6 0+6 8+0 0+0 8+2 0+2 [msw at left]
	vpmovzxbq zmm30, ZMM_PERMUTE2		;; zmm30 = 8+5 0+5 8+7 0+7 8+1 0+1 8+3 0+3
	vbroadcastsd zmm29, ZMM_SQRTHALF
	vbroadcastsd zmm28, ZMM_P924_P383
	vbroadcastsd zmm27, ZMM_P383
	ENDM

zr8_rsc_wpn_sgreg_2sc_sixteen_reals_fft8_preload MACRO
	use zr8_rsc_complex_and_real_preload
	ENDM

zr8_rsc_wpn_sgreg_2sc_sixteen_reals_fft8 MACRO srcreg,srcinc,d1,d2,d4,dstreg,dstinc,e1reg,e3reg,dst4reg,screg1,scinc1,wgtreg,wgtinc,screg2,scinc2,maxrpt,L1pt,L1preg,L1p4reg
	vmovapd	zmm15, [srcreg+d4+d2]		;; r7_7	r7_6 r7_5 r7_4 r7_3 r7_2 r7_1 r7_0
	vmovapd	zmm26, [srcreg+d4+d2+d1]	;; r8_7	r8_6 r8_5 r8_4 r8_3 r8_2 r8_1 r8_0
	vmovapd zmm14, zmm15
	vpermt2pd zmm14, zmm31, zmm26		;; r8_4 r7_4 r8_6 r7_6 r8_0 r7_0 r8_2 r7_2		; 1-3		n 5

	vpermt2pd zmm15, zmm30, zmm26		;; r8_5 r7_5 r8_7 r7_7 r8_1 r7_1 r8_3 r7_3		; 2-4		n 7

	vmovapd	zmm13, [srcreg+d4]		;; r5_7	r5_6 r5_5 r5_4 r5_3 r5_2 r5_1 r5_0
	vmovapd	zmm26, [srcreg+d4+d1]		;; r6_7	r6_6 r6_5 r6_4 r6_3 r6_2 r6_1 r6_0
	vshufpd	zmm12, zmm13, zmm26, 00000000b	;; r6_6 r5_6 r6_4 r5_4 r6_2 r5_2 r6_0 r5_0		; 3		n 5

	vshufpd	zmm13, zmm13, zmm26, 11111111b	;; r6_7 r5_7 r6_5 r5_5 r6_3 r5_3 r6_1 r5_1		; 4		n 7

	vmovapd	zmm11, [srcreg+d2]		;; r3_7	r3_6 r3_5 r3_4 r3_3 r3_2 r3_1 r3_0
	vmovapd	zmm26, [srcreg+d2+d1]		;; r4_7	r4_6 r4_5 r4_4 r4_3 r4_2 r4_1 r4_0
	vmovapd	zmm10, zmm11
	vpermt2pd zmm10, zmm31, zmm26		;; r4_4 r3_4 r4_6 r3_6 r4_0 r3_0 r4_2 r3_2		; 5-7		n 9
	vblendmpd zmm17{k7}, zmm14, zmm12	;; r8_4 r7_4 r6_4 r5_4 r8_0 r7_0 r6_0 r5_0		;  5		n 21

	vpermt2pd zmm11, zmm30, zmm26		;; r4_5 r3_5 r4_7 r3_7 r4_1 r3_1 r4_3 r3_3		; 6-8		n 11
	vblendmpd zmm12{k7}, zmm12, zmm14	;; r6_6 r5_6 r8_6 r7_6 r6_2 r5_2 r8_2 r7_2		;  6		n 23

	vmovapd	zmm9, [srcreg]			;; r1_7	r1_6 r1_5 r1_4 r1_3 r1_2 r1_1 r1_0
	vmovapd	zmm26, [srcreg+d1]		;; r2_7	r2_6 r2_5 r2_4 r2_3 r2_2 r2_1 r2_0
	vshufpd	zmm8, zmm9, zmm26, 00000000b	;; r2_6 r1_6 r2_4 r1_4 r2_2 r1_2 r2_0 r1_0		; 7		n 9
	vblendmpd zmm14{k7}, zmm15, zmm13	;; r8_5 r7_5 r6_5 r6_5 r8_1 r7_1 r6_1 r5_1		;  7		n 19

	vshufpd	zmm9, zmm9, zmm26, 11111111b	;; r2_7 r1_7 r2_5 r1_5 r2_3 r1_3 r2_1 r1_1		; 8		n 11
	vblendmpd zmm13{k7}, zmm13, zmm15	;; r6_7 r5_7 r8_7 r7_7 r6_3 r5_3 r8_3 r7_3		;  8		n 17

	vmovapd	zmm3, [srcreg+d2+64]		;; r11_7 r11_6 r11_5 r11_4 r11_3 r11_2 r11_1 r11_0
	vmovapd	zmm26, [srcreg+d2+d1+64]	;; r12_7 r12_6 r12_5 r12_4 r12_3 r12_2 r12_1 r12_0
	vmovapd	zmm2, zmm3
	vpermt2pd zmm2, zmm31, zmm26		;; r12_4 r11_4 r12_6 r11_6 r12_0 r11_0 r12_2 r11_2	; 9-11		n 13
	vblendmpd zmm15{k7}, zmm10, zmm8	;; r4_4 r3_4 r2_4 r1_4 r4_0 r3_0 r2_0 r1_0		;  9		n 21

	vpermt2pd zmm3, zmm30, zmm26		;; r12_5 r11_5 r12_7 r11_7 r12_1 r11_1 r12_3 r11_3	; 10-12		n 15
	vblendmpd zmm8{k7}, zmm8, zmm10		;; r2_6 r1_6 r4_6 r3_6 r2_2 r1_2 r4_2 r3_2		;  10		n 23

	vmovapd	zmm0, [srcreg+64]		;; r9_7	r9_6 r9_5 r9_4 r9_3 r9_2 r9_1 r9_0
	vmovapd	zmm26, [srcreg+d1+64]		;; r10_7 r10_6 r10_5 r10_4 r10_3 r10_2 r10_1 r10_0
	vshufpd	zmm1, zmm0, zmm26, 11111111b	;; r10_7 r9_7 r10_5 r9_5 r10_3 r9_3 r10_1 r9_1		; 11		n 15
	vblendmpd zmm10{k7}, zmm11, zmm9	;; r4_5 r3_5 r2_5 r1_5 r4_1 r3_1 r2_1 r1_1		;  11		n 19

	vshufpd	zmm0, zmm0, zmm26, 00000000b	;; r10_6 r9_6 r10_4 r9_4 r10_2 r9_2 r10_0 r9_0		; 12		n 13
	vblendmpd zmm9{k7}, zmm9, zmm11		;; r2_7 r1_7 r4_7 r3_7 r2_3 r1_3 r4_3 r3_3		;  12		n 17

	vmovapd	zmm7, [srcreg+d4+d2+64]		;; r15_7 r15_6 r15_5 r15_4 r15_3 r15_2 r15_1 r15_0
	vmovapd	zmm26, [srcreg+d4+d2+d1+64]	;; r16_7 r16_6 r16_5 r16_4 r16_3 r16_2 r16_1 r16_0
	vmovapd zmm6, zmm7
	vpermt2pd zmm6, zmm31, zmm26		;; r16_4 r15_4 r16_6 r15_6 r16_0 r15_0 r16_2 r15_2	; 13-15		n 17
	vblendmpd zmm16{k7}, zmm2, zmm0		;; r12_4 r11_4 r10_4 r9_4 r12_0 r11_0 r10_0 r9_0	;  13		n 29

	vpermt2pd zmm7, zmm30, zmm26		;; r16_5 r15_5 r16_7 r15_7 r16_1 r15_1 r16_3 r15_3	; 14-16		n 19
	vblendmpd zmm0{k7}, zmm0, zmm2		;; r10_6 r9_6 r12_6 r11_6 r10_2 r9_2 r12_2 r11_2	;  14		n 31

	vmovapd	zmm5, [srcreg+d4+64]		;; r13_7 r13_6 r13_5 r13_4 r13_3 r13_2 r13_1 r13_0
	vmovapd	zmm26, [srcreg+d4+d1+64]	;; r14_7 r14_6 r14_5 r14_4 r14_3 r14_2 r14_1 r14_0
	vshufpd	zmm4, zmm5, zmm26, 00000000b	;; r14_6 r13_6 r14_4 r13_4 r14_2 r13_2 r14_0 r13_0	; 15		n 17
	vblendmpd zmm2{k7}, zmm3, zmm1		;; r12_5 r11_5 r10_5 r9_5 r12_1 r11_1 r10_1 r9_1	;  15		n 25

	vshufpd	zmm5, zmm5, zmm26, 11111111b	;; r14_7 r13_7 r14_5 r13_5 r14_3 r13_3 r14_1 r13_1	; 16		n 19
	vblendmpd zmm1{k7}, zmm1, zmm3		;; r10_7 r9_7 r12_7 r11_7 r10_3 r9_3 r12_3 r11_3	;  16		n 27
	vmovapd	zmm26, [wgtreg]			;; Load the weights to apply to screg2 data and R1A/R1B

	vshuff64x2 zmm11, zmm9, zmm13, 00010001b;; r8_3 r7_3 r6_3 r5_3 r4_3 r3_3 r2_3 r1_3 (R4)		; 17-19		n 21
	vblendmpd zmm3{k7}, zmm6, zmm4		;; r16_4 r15_4 r14_4 r13_4 r16_0 r15_0 r14_0 r13_0	;  17		n 29
	vmovapd	zmm25, [screg1+2*128+64]	;; cosine for w^4 (8-complex w^2)

	vshuff64x2 zmm9, zmm9, zmm13, 10111011b ;; r8_7 r7_7 r6_7 r5_7 r4_7 r3_7 r2_7 r1_7 (R8)		; 18-20		n 21
	vblendmpd zmm4{k7}, zmm4, zmm6		;; r14_6 r13_6 r16_6 r15_6 r14_2 r13_2 r16_2 r15_2	;  18		n 31
	vmovapd	zmm24, [screg1+1*128+64]	;; cosine for w^2 (8-complex w^1)

	vshuff64x2 zmm13, zmm10, zmm14, 01000100b ;; r8_1 r7_1 r6_1 r5_1 r4_1 r3_1 r2_1 r1_1 (R2)	; 19-21		n 23
	vblendmpd zmm6{k7}, zmm7, zmm5		;; r16_5 r15_5 r14_5 r13_5 r16_1 r15_1 r14_1 r13_1	;  19		n 25
	vmovapd	zmm23, [screg1+5*128+64]	;; cosine for w^10 (8-complex w^5)

	vshuff64x2 zmm10, zmm10, zmm14, 11101110b ;; r8_5 r7_5 r6_5 r5_5 r4_5 r3_5 r2_5 r1_5 (R6)	; 20-22		n 23
	vblendmpd zmm5{k7}, zmm5, zmm7		;; r14_7 r13_7 r16_7 r15_7 r14_3 r13_3 r16_3 r15_3	;  20		n 27
	vmovapd	zmm22, [screg1+2*128]		;; sine for w^4 (8-complex w^2)

	vshuff64x2 zmm14, zmm15, zmm17, 01000100b;; r8_0 r7_0 r6_0 r5_0 r4_0 r3_0 r2_0 r1_0 (R1)	; 21-23		n 25
	vshuff64x2 zmm15, zmm15, zmm17, 11101110b;; r8_4 r7_4 r6_4 r5_4 r4_4 r3_4 r2_4 r1_4 (R5)	; 22-24		n 25
	vmovapd	zmm21, [screg1+1*128]		;; sine for w^2 (8-complex w^1)

	vsubpd	zmm17, zmm11, zmm9		;; R4 - R8 (newer R8)					; 21-24		n 29
	vaddpd	zmm11, zmm11, zmm9		;; R4 + R8 (newer R4)					; 22-25		n 31
	bump	srcreg, srcinc

	vshuff64x2 zmm9, zmm8, zmm12, 00010001b ;; r8_2 r7_2 r6_2 r5_2 r4_2 r3_2 r2_2 r1_2 (R3)		; 23-25		n 27
	vshuff64x2 zmm8, zmm8, zmm12, 10111011b ;; r8_6 r7_6 r6_6 r5_6 r4_6 r3_6 r2_6 r1_6 (R7)		; 24-26		n 27
	bump	wgtreg, wgtinc

	vsubpd	zmm12, zmm13, zmm10		;; R2 - R6 (newer R6)					; 23-26		n 29
	vaddpd	zmm13, zmm13, zmm10		;; R2 + R6 (newer R2)					; 24-27		n 31
	L1prefetchw dstreg, L1pt - L1PREFETCH_DEST_NONE

	vshuff64x2 zmm7, zmm2, zmm6, 01000100b	;; r16_1 r15_1 r14_1 r13_1 r12_1 r11_1 r10_1 r9_1 (R10)	; 25-27		n 36
	vshuff64x2 zmm2, zmm2, zmm6, 11101110b	;; r16_5 r15_5 r14_5 r13_5 r12_5 r11_5 r10_5 r9_5 (R14)	; 26-28		n 36
	L1prefetchw dstreg+64, L1pt - L1PREFETCH_DEST_NONE

	vaddpd	zmm10, zmm14, zmm15		;; R1 + R5 (newer R1)					; 25-28		n 33
	vsubpd	zmm14, zmm14, zmm15		;; R1 - R5 (newer R5)					; 26-29		n 34
	L1prefetchw dstreg+e1reg, L1pt - L1PREFETCH_DEST_NONE

	vshuff64x2 zmm6, zmm1, zmm5, 00010001b	;; r16_3 r15_3 r14_3 r13_3 r12_3 r11_3 r10_3 r9_3 (R12)	; 27-29		n 37
	vshuff64x2 zmm1, zmm1, zmm5, 10111011b	;; r16_7 r15_7 r14_7 r13_7 r12_7 r11_7 r10_7 r9_7 (R16)	; 28-30		n 37
	L1prefetchw dstreg+e1reg+64, L1pt - L1PREFETCH_DEST_NONE

	vaddpd	zmm15, zmm9, zmm8		;; R3 + R7 (newer R3)					; 27-30		n 33
	vsubpd	zmm9, zmm9, zmm8		;; R3 - R7 (newer R7)					; 28-31		n 35
	L1prefetchw dstreg+2*e1reg, L1pt - L1PREFETCH_DEST_NONE

	vshuff64x2 zmm5, zmm16, zmm3, 01000100b;; r16_0 r15_0 r14_0 r13_0 r12_0 r11_0 r10_0 r9_0 (R9)	; 29-31		n 45
	vshuff64x2 zmm16, zmm16, zmm3, 11101110b;; r16_4 r15_4 r14_4 r13_4 r12_4 r11_4 r10_4 r9_4 (R13)	; 30-32		n 46
	L1prefetchw dstreg+2*e1reg+64, L1pt - L1PREFETCH_DEST_NONE

						;; R6/R8 becomes newest R6/I6
						;; mul R6/I6 by w^2 = .707 + .707i
	vsubpd	zmm8, zmm12, zmm17		;; R6 - I6 (newest R6/SQRTHALF)				; 29-32		n 34
	vaddpd	zmm12, zmm12, zmm17		;; R6 + I6 (newest I6/SQRTHALF)				; 30-33		n 35
	L1prefetchw dstreg+e3reg, L1pt - L1PREFETCH_DEST_NONE

	vshuff64x2 zmm3, zmm0, zmm4, 00010001b	;; r16_2 r15_2 r14_2 r13_2 r12_2 r11_2 r10_2 r9_2 (R11)	; 31-33		n 38
	vshuff64x2 zmm0, zmm0, zmm4, 10111011b	;; r16_6 r15_6 r14_6 r13_6 r12_6 r11_6 r10_6 r9_6 (R15)	; 32-34		n 38
	L1prefetchw dstreg+e3reg+64, L1pt - L1PREFETCH_DEST_NONE

	vsubpd	zmm17, zmm13, zmm11		;; R2 - R4 (newest R4)					; 31-34		n 40
	vaddpd	zmm13, zmm13, zmm11		;; R2 + R4 (newest R2)					; 32-35		n 44
	L1prefetchw dst4reg, L1pt - L1PREFETCH_DEST_NONE


 	vaddpd	zmm11, zmm10, zmm15		;; R1 + R3 (newest R1)					; 33-36		n 37
	vsubpd	zmm10, zmm10, zmm15		;; R1 - R3 (newest R3)					; 33-36		n 40
	L1prefetchw dst4reg+64, L1pt - L1PREFETCH_DEST_NONE
						;; R3/R4 becomes last R3/I3

						;; R5/R7 becomes newest R5/I5
	zfmaddpd zmm15, zmm8, zmm29, zmm14	;; R5 + R6 * SQRTHALF (last R5)				; 34-37		n 42
	zfnmaddpd zmm8, zmm8, zmm29, zmm14	;; R5 - R6 * SQRTHALF (last R6)				; 34-37		n 43
	L1prefetchw dst4reg+e1reg, L1pt - L1PREFETCH_DEST_NONE

	zfmaddpd zmm14, zmm12, zmm29, zmm9	;; I5 + I6 * SQRTHALF (last I5)				; 35-38		n 42
	zfnmaddpd zmm12, zmm12, zmm29, zmm9	;; I5 - I6 * SQRTHALF (last I6)				; 35-38		n 43
	L1prefetchw dst4reg+e1reg+64, L1pt - L1PREFETCH_DEST_NONE

						;; R10/R14 becomes newer R10/I10
						;; mul R10/I10 by w^1 = .924 + .383i
	zfmsubpd zmm4, zmm7, zmm28, zmm2	;; R10*.924/.383 - I10 (newer R10/.383)			; 36-39		n 41
	zfmaddpd zmm2, zmm2, zmm28, zmm7	;; R10 + I10*.924/.383 (newer I10/.383)			; 36-39		n 41
	L1prefetchw dst4reg+2*e1reg, L1pt - L1PREFETCH_DEST_NONE

						;; R1/R2 becomes last & final R1A and R1B
	vmulpd	zmm18, zmm27, zmm26		;; Weight .383						; 37-40		n 41
	vmulpd	zmm11, zmm11, zmm26		;; Weight R1A						; 37-40		n 44
	L1prefetchw dst4reg+2*e1reg+64, L1pt - L1PREFETCH_DEST_NONE

						;; R12/R16 becomes newer R12/I12
						;; mul R12/I12 by w^3 = .383 + .924i
	zfnmaddpd zmm7, zmm1, zmm28, zmm6	;; R12 - I12*.924/.383 (newer R12/.383)			; 38-41		n 45
	zfmaddpd zmm6, zmm6, zmm28, zmm1	;; R12*.924/.383 + I12 (newer I12/.383)			; 38-41		n 46
	L1prefetchw dst4reg+e3reg, L1pt - L1PREFETCH_DEST_NONE

						;; R11/R15 becomes newer R11/I11
						;; mul R11/I11 by SQRTHALF + i*SQRTHALF
	vsubpd	zmm1, zmm3, zmm0		;; R11 - I11 (newer R11/SQRTHALF)			; 39-42		n 47
	vaddpd	zmm3, zmm3, zmm0		;; R11 + I11 (newer I11/SQRTHALF)			; 39-42		n 48
	L1prefetchw dst4reg+e3reg+64, L1pt - L1PREFETCH_DEST_NONE

	vmulpd	zmm9, zmm10, zmm25		;; A3 = R3 * cosine					; 40-43		n 49
	vmulpd	zmm25, zmm17, zmm25		;; B3 = I3 * cosine					; 40-43		n 49

	vmulpd	zmm4, zmm4, zmm18		;; R10 = R10 * .383*wgt					; 41-44		n 45
	vmulpd	zmm2, zmm2, zmm18		;; I10 = I10 * .383*wgt					; 41-44		n 46

	vmulpd	zmm19, zmm15, zmm24		;; A5 = R5 * cosine					; 42-45		n 50
	vmulpd	zmm24, zmm14, zmm24		;; B5 = I5 * cosine					; 42-45		n 50

	vmulpd	zmm20, zmm8, zmm23		;; A6 = R6 * cosine					; 43-46		n 51
	vmulpd	zmm23, zmm12, zmm23		;; B6 = I6 * cosine					; 43-46		n 51

	zfmaddpd zmm0, zmm13, zmm26, zmm11	;; R1A + wgt * R1B (final R1A)				; 44-47
	zfnmaddpd zmm13, zmm13, zmm26, zmm11	;; R1A - wgt * R1B (final R1B)				; 44-47

	zfmaddpd zmm11, zmm7, zmm18, zmm4	;; R10 + R12 * .383*wgt (newest R10*wgt)		; 45-48		n 52
	zfnmaddpd zmm7, zmm7, zmm18, zmm4	;; R10 - R12 * .383*wgt (newest R12*wgt)		; 45-48		n 54

	zfmaddpd zmm4, zmm6, zmm18, zmm2	;; I10 + I12 * .383*wgt (newest I10*wgt)		; 46-49		n 52
	zfnmaddpd zmm6, zmm6, zmm18, zmm2	;; I10 - I12 * .383*wgt (newest I12*wgt)		; 46-49		n 54
	vmovapd	zmm18, [screg1+5*128]		;; sine for w^10 (8-complex w^5)

						;; R9/R13 becomes newer R9/I9
	zfmaddpd zmm2, zmm1, zmm29, zmm5	;; R9 + R11 * SQRTHALF (newest R9)			; 47-50		n 52
	zfnmaddpd zmm1, zmm1, zmm29, zmm5	;; R9 - R11 * SQRTHALF (newest R11)			; 47-50		n 54

	zfmaddpd zmm5, zmm3, zmm29, zmm16	;; I9 + I11 * SQRTHALF (newest I9)			; 48-51		n 52
	zfnmaddpd zmm3, zmm3, zmm29, zmm16	;; I9 - I11 * SQRTHALF (newest I11)			; 48-51		n 54
	vmovapd	zmm16, [screg2+0*128+64]	;; cosine/sine for w^1
	zstore	[dstreg], zmm0			;; Save R1A						; 48

	zfnmaddpd zmm17, zmm17, zmm22, zmm9	;; A3 - I3 * sine (final R3)				; 49-52
	zfmaddpd zmm10, zmm10, zmm22, zmm25	;; B3 + R3 * sine (final I3)				; 49-52
	vmovapd	zmm22, [screg2+2*128+64]	;; cosine/sine for w^9
	zstore	[dstreg+64], zmm13		;; Save R1B						; 48+1

	zfnmaddpd zmm14, zmm14, zmm21, zmm19	;; A5 - I5 * sine (final R5)				; 50-53
	zfmaddpd zmm15, zmm15, zmm21, zmm24	;; B5 + R5 * sine (final I5)				; 50-53
	vmovapd	zmm25, [screg2+1*128+64]	;; cosine/sine for w^5

	zfnmaddpd zmm12, zmm12, zmm18, zmm20	;; A6 - I6 * sine (final R6)				; 51-54
	zfmaddpd zmm8, zmm8, zmm18, zmm23	;; B6 + R6 * sine (final I6)				; 51-54
	vmovapd	zmm13, [screg2+3*128+64]	;; cosine/sine for w^13

	zfmaddpd zmm9, zmm2, zmm26, zmm11	;; R9 * wgt + R10 (last R9)				; 52-55		n 56
	zfmaddpd zmm0, zmm5, zmm26, zmm4	;; I9 * wgt + I10 (last I9)				; 52-55		n 56
	vmovapd	zmm19, [screg2+0*128]		;; sine for w^1

	zfmsubpd zmm2, zmm2, zmm26, zmm11	;; R9 * wgt - R10 (last R10)				; 53-56		n 57
	zfmsubpd zmm5, zmm5, zmm26, zmm4	;; I9 * wgt - I10 (last I10)				; 53-56		n 57
	vmovapd	zmm21, [screg2+2*128]		;; sine for w^9
	zstore	[dstreg+e1reg], zmm17		;; Save R2 (final R3)					; 53

	zfmsubpd zmm11, zmm1, zmm26, zmm6	;; R11 * wgt - I12 (last R11)				; 54-57		n 58
	zfmaddpd zmm4, zmm3, zmm26, zmm7	;; I11 * wgt + R12 (last I11)				; 54-57		n 58
	vmovapd	zmm24, [screg2+1*128]		;; sine for w^5
	zstore	[dstreg+e1reg+64], zmm10	;; Save I2 (final I3)					; 53+1

	zfmaddpd zmm1, zmm1, zmm26, zmm6	;; R11 * wgt + I12 (last R12)				; 55-58		n 59
	zfmsubpd zmm3, zmm3, zmm26, zmm7	;; I11 * wgt - R12 (last I12)				; 55-58		n 59
	vmovapd	zmm20, [screg2+3*128]		;; sine for w^13
	zstore	[dstreg+2*e1reg], zmm14		;; Save R3 (final R5)					; 54+1

	zfmsubpd zmm6, zmm9, zmm16, zmm0	;; A9 = R9 * cosine/sine - I9				; 56-59		n 60
	zfmaddpd zmm0, zmm0, zmm16, zmm9	;; B9 = I9 * cosine/sine + R9				; 56-59		n 60
	bump	screg1, scinc1
	zstore	[dstreg+2*e1reg+64], zmm15	;; Save I3 (final I5)					; 54+2

	zfmsubpd zmm9, zmm2, zmm22, zmm5	;; A10 = R10 * cosine/sine - I10			; 57-60		n 61
	zfmaddpd zmm5, zmm5, zmm22, zmm2	;; B10 = I10 * cosine/sine + R10			; 57-60		n 61
	bump	screg2, scinc2
	zstore	[dstreg+e3reg], zmm12		;; Save R4 (final R6)					; 55+2

	zfmsubpd zmm2, zmm11, zmm25, zmm4	;; A11 = R11 * cosine/sine - I11			; 58-61		n 62
	zfmaddpd zmm4, zmm4, zmm25, zmm11	;; B11 = I11 * cosine/sine + R11			; 58-61		n 62
	zstore	[dstreg+e3reg+64], zmm8		;; Save I4 (final I6)					; 55+3

	zfmsubpd zmm11, zmm1, zmm13, zmm3	;; A12 = R12 * cosine/sine - I12			; 59-62		n 63
	zfmaddpd zmm3, zmm3, zmm13, zmm1	;; B12 = I12 * cosine/sine + R12			; 59-62		n 63

	vmulpd	zmm6, zmm6, zmm19		;; A9 * sine (final R9)					; 60-63
	vmulpd	zmm0, zmm0, zmm19		;; B9 * sine (final I9)					; 60-63

	vmulpd	zmm9, zmm9, zmm21		;; A10 * sine (final R10)				; 61-64
	vmulpd	zmm5, zmm5, zmm21		;; B10 * sine (final I10)				; 61-64

	vmulpd	zmm2, zmm2, zmm24		;; A11 * sine (final R11)				; 62-65
	vmulpd	zmm4, zmm4, zmm24		;; B11 * sine (final I11)				; 62-65

	vmulpd	zmm11, zmm11, zmm20		;; A12 * sine (final R12)				; 63-66
	vmulpd	zmm3, zmm3, zmm20		;; B12 * sine (final I12)				; 63-66

	zstore	[dst4reg], zmm6			;; Save R5 (final R9)					; 64
	zstore	[dst4reg+64], zmm0		;; Save I5 (final I9)					; 64+1
	zstore	[dst4reg+e1reg], zmm9		;; Save R6 (final R10)					; 65+1
	zstore	[dst4reg+e1reg+64], zmm5	;; Save I6 (final I10)					; 65+2
	zstore	[dst4reg+2*e1reg], zmm2		;; Save R7 (final R11)					; 66+2
	zstore	[dst4reg+2*e1reg+64], zmm4	;; Save I7 (final I11)					; 66+3
	zstore	[dst4reg+e3reg], zmm11		;; Save R8 (final R12)					; 67+3
	zstore	[dst4reg+e3reg+64], zmm3	;; Save I8 (final I12)					; 67+4
	bump	dstreg, dstinc
	bump	dst4reg, dstinc
	ENDM


;; These versions use registers for distances between blocks.  This lets us share pass1 code.
;; Weights must be applied.  The complex s/c data referenced by screg1 has already been weighted.
;; The real s/c data referenced by screg2 has not been weighted.

zr8_rsc_wpn_sgreg_2sc_sixteen_reals_unfft8_preload MACRO
	use zr8_rsc_complex_and_real_preload
	ENDM

zr8_rsc_wpn_sgreg_2sc_sixteen_reals_unfft8 MACRO srcreg,srcinc,d1reg,d3reg,src4reg,dstreg,dstinc,e1,e2,e4,screg1,scinc1,wgtreg,wgtinc,screg2,scinc2,maxrpt,L1pt,L1preg,L1p4reg
	vmovapd	zmm24, [screg1+1*128+64]	;; cosine for w^2 (8-complex w^1)
	vmovapd zmm2, [srcreg+2*d1reg]		;; Load R3 (aka R5)
	vmovapd zmm10, [srcreg+2*d1reg+64]	;; Load I3 (aka I5)
	vmulpd	zmm16, zmm2, zmm24		;; A5 = R5 * cosine					; 1-4		n 7
	vmulpd	zmm17, zmm10, zmm24		;; B5 = I5 * cosine					; 2-5		n 8

	vmovapd	zmm24, [screg1+5*128+64]	;; cosine for w^10 (8-complex w^5)
	vmovapd zmm3, [srcreg+d3reg]		;; Load R4 (aka R6)
	vmovapd zmm11, [srcreg+d3reg+64]	;; Load I4 (aka I6)
	vmulpd	zmm18, zmm3, zmm24		;; A6 = R6 * cosine					; 2-5		n 8
	vmulpd	zmm19, zmm11, zmm24		;; B6 = I6 * cosine					; 3-6		n 9

	vmovapd	zmm24, [screg2+0*128+64]	;; cosine/sine for w^1
	vmovapd zmm4, [src4reg]			;; Load R5 (aka R9)
	vmovapd zmm12, [src4reg+64]		;; Load I5 (aka I9)
	zfmaddpd zmm20, zmm4, zmm24, zmm12	;; A9 = R9 * cosine/sine + I9				; 3-6		n 10
	zfmsubpd zmm12, zmm12, zmm24, zmm4	;; B9 = I9 * cosine/sine - R9				; 4-7		n 11

	vmovapd	zmm24, [screg2+1*128+64]	;; cosine/sine for w^5
	vmovapd zmm6, [src4reg+2*d1reg]		;; Load R7 (aka R11)
	vmovapd zmm14, [src4reg+2*d1reg+64]	;; Load I7 (aka I11)
	zfmaddpd zmm4, zmm6, zmm24, zmm14	;; A11 = R11 * cosine/sine + I11			; 4-7		n 12
	zfmsubpd zmm14, zmm14, zmm24, zmm6	;; B11 = I11 * cosine/sine - R11			; 5-8		n 13

	vmovapd	zmm24, [screg1+2*128+64]	;; cosine for w^4 (8-complex w^2)
	vmovapd zmm1, [srcreg+d1reg]		;; Load R2 (aka R3)
	vmovapd zmm9, [srcreg+d1reg+64]		;; Load I2 (aka I3)
	vmulpd	zmm6, zmm1, zmm24		;; A3 = R3 * cosine					; 5-8		n 13
	vmulpd	zmm21, zmm9, zmm24		;; B3 = I3 * cosine					; 6-9		n 14

	vmovapd	zmm26, [wgtreg]			;; Load the weights to apply to screg2 data and R1/I1
	vmulpd	zmm25, zmm26, [screg2+1*128]	;; weighted sine for w^5				; 6-9		n 12
	vmulpd	zmm22, zmm26, [screg2+3*128]	;; weighted sine for w^13				; 7-10		n 17

	vmovapd zmm24, [screg1+1*128]		;; sine for w^2 (8-complex w^1)
	zfmaddpd zmm16, zmm10, zmm24, zmm16	;; A5 + I5 * sine (first R5)				; 7-10		n 14
	zfnmaddpd zmm17, zmm2, zmm24, zmm17	;; B5 - R5 * sine (first I5)				; 8-11		n 15

	vmovapd	zmm24, [screg1+5*128]		;; sine for w^10 (8-complex w^5)
	zfmaddpd zmm18, zmm11, zmm24, zmm18	;; A6 + I6 * sine (first R6)				; 8-11		n 14
	zfnmaddpd zmm19, zmm3, zmm24, zmm19	;; B6 - R6 * sine (first I6)				; 9-12		n 15

	vmovapd	zmm24, [screg2+2*128+64]	;; cosine/sine for w^9
	vmovapd zmm5, [src4reg+d1reg]		;; Load R6 (aka R10)
	vmovapd zmm13, [src4reg+d1reg+64]	;; Load I6 (aka I10)
	zfmaddpd zmm10, zmm5, zmm24, zmm13	;; A10 = R10 * cosine/sine + I10			; 9-12		n 16
	zfmsubpd zmm13, zmm13, zmm24, zmm5	;; B10 = I10 * cosine/sine - R10			; 10-13		n 18

	vmovapd zmm24, [screg2+0*128]		;; sine for w^1
	vmulpd	zmm20, zmm20, zmm24		;; A9 * sine (first R9/weight)				; 10-13		n 16
	vmulpd	zmm12, zmm12, zmm24		;; B9 * sine (first I9/weight)				; 11-14		n 18

	vmovapd	zmm24, [screg2+3*128+64]	;; cosine/sine for w^13
	vmovapd zmm7, [src4reg+d3reg]		;; Load R8 (aka R12)
	vmovapd zmm15, [src4reg+d3reg+64]	;; Load I8 (aka I12)
	zfmaddpd zmm2, zmm7, zmm24, zmm15	;; A12 = R12 * cosine/sine + I12			; 11-14		n 17
	zfmsubpd zmm15, zmm15, zmm24, zmm7	;; B12 = I12 * cosine/sine - R12			; 12-15		n 19

	vmulpd	zmm4, zmm4, zmm25		;; A11 * sine (first R11)				; 12-15		n 17
	vmulpd	zmm14, zmm14, zmm25		;; B11 * sine (first I11)				; 13-16		n 19

	vmovapd	zmm24, [screg1+2*128]		;; sine for w^4 (8-complex w^2)
	zfmaddpd zmm6, zmm9, zmm24, zmm6	;; A3 + I3 * sine (first R3)				; 13-16		n 20
	zfnmaddpd zmm21, zmm1, zmm24, zmm21	;; B3 - R3 * sine (first I3)				; 14-17		n 21

						;; R1/I1 becomes R1/R2
						;; R3/I3 becomes R3/R4

	vmovapd zmm23, [screg2+2*128]		;; sine for w^9
	vsubpd	zmm11, zmm16, zmm18		;; R5 - R6 (new R6)					; 14-17		n 22
	vaddpd	zmm16, zmm16, zmm18		;; R5 + R6 (new R5)					; 15-18		n 28

	vmovapd zmm0, [srcreg]			;; Load R1
	vsubpd	zmm3, zmm17, zmm19		;; I5 - I6 (new I6)					; 15-18		n 22
	vaddpd	zmm17, zmm17, zmm19		;; I5 + I6 (new I5)					; 16-19		n 26

	vmovapd zmm8, [srcreg+64]		;; Load I1
	zfmaddpd zmm5, zmm10, zmm23, zmm20	;; R9 + R10 * sine (new R9/weight)			; 16-19		n 23
	zfnmaddpd zmm10, zmm10, zmm23, zmm20	;; R9 - R10 * sine (new R10/weight)			; 17-20		n 24

	L1prefetch L1preg, L1pt
	zfmaddpd zmm7, zmm2, zmm22, zmm4	;; R12 * sine + R11 (new R11)				; 17-20		n 23
	zfmsubpd zmm2, zmm2, zmm22, zmm4	;; R12 * sine - R11 (new I12)				; 18-21		n 25

	L1prefetch L1preg+64, L1pt
	zfmaddpd zmm1, zmm15, zmm22, zmm14	;; I11 + I12 * sine (new I11)				; 18-21		n 24
	zfnmaddpd zmm15, zmm15, zmm22, zmm14	;; I11 - I12 * sine (new R12)				; 19-22		n 24

	L1prefetch L1preg+d1reg, L1pt
	zfmaddpd zmm9, zmm13, zmm23, zmm12	;; I9 + I10 * sine (new I9/weight)			; 19-22		n 24
	zfnmaddpd zmm13, zmm13, zmm23, zmm12	;; I9 - I10 * sine (new I10/weight)			; 20-23		n 25

	L1prefetch L1preg+d1reg+64, L1pt
	zfmsubpd zmm4, zmm0, zmm26, zmm6	;; R1*weight - R3 (new R3)				; 20-23		n 26
	zfmaddpd zmm0, zmm0, zmm26, zmm6	;; R1*weight + R3 (new R1)				; 21-24		n 28

	L1prefetch L1preg+2*d1reg, L1pt
	zfmsubpd zmm12, zmm8, zmm26, zmm21	;; R2*weight - R4 (new R4)				; 21-24		n 27
	zfmaddpd zmm8, zmm8, zmm26, zmm21	;; R2*weight + R4 (new R2)				; 22-25		n 29

						;; R5/I5 becomes newer R5/R7

	L1prefetch L1preg+2*d1reg+64, L1pt
						;; mul R6/I6 by w^2 = .707 - .707i
	vsubpd	zmm14, zmm3, zmm11		;; I6 / SQRTHALF = I6 - R6				; 22-25		n 27
	vaddpd	zmm3, zmm3, zmm11		;; R6 / SQRTHALF = I6 + R6				; 23-26		n 29
						;; R6/I6 becomes newer R6/R8

	L1prefetch L1preg+d3reg, L1pt
	zfmsubpd zmm6, zmm5, zmm26, zmm7	;; R9*weight - R11 (newer R11)				; 23-26		n 28
	zfmsubpd zmm11, zmm9, zmm26, zmm1	;; I9*weight - I11 (newer I11)				; 24-27		n 28

	L1prefetch L1preg+d3reg+64, L1pt
	zfmsubpd zmm18, zmm10, zmm26, zmm15	;; R10*weight - R12 (newer R12)				; 24-27		n 29
	zfmsubpd zmm19, zmm13, zmm26, zmm2	;; I10*weight - I12 (newer I12)				; 25-28		n 29

	L1prefetch L1p4reg, L1pt
	zfmaddpd zmm10, zmm10, zmm26, zmm15	;; R10*weight + R12 (newer R10)				; 25-28		n 30
	zfmaddpd zmm13, zmm13, zmm26, zmm2	;; I10*weight + I12 (newer I10)				; 26-29		n 30

	L1prefetch L1p4reg+64, L1pt
	vaddpd	zmm20, zmm4, zmm17		;; R3 + R7 (newer R3)					; 26-29		n 32
	zfmaddpd zmm21, zmm14, zmm29, zmm12	;; R4 + R8 * SQRTHALF (newer R4)			; 27-30		n 33

	L1prefetch L1p4reg+d1reg, L1pt
	zfmaddpd zmm5, zmm5, zmm26, zmm7	;; R9*weight + R11 (newer R9)				; 27-30		n 33
	vaddpd	zmm7, zmm0, zmm16		;; R1 + R5 (newer R1)					; 28-31		n 33

	L1prefetch L1p4reg+d1reg+64, L1pt
						;; mul R11/I11 by SQRTHALF - i*SQRTHALF
	vaddpd	zmm22, zmm11, zmm6		;; Twiddled R11 / SQRTHALF = I11 + R11			; 28-31		n 32
						;; mul R12/I12 by w^3 = .383 - .924i
	zfmaddpd zmm23, zmm19, zmm28, zmm18	;; Twiddled R12/.383 = R12 + I12*.924/.383		; 29-32		n 33

	L1prefetch L1p4reg+2*d1reg, L1pt
	zfmaddpd zmm15, zmm3, zmm29, zmm8	;; R2 + R6 * SQRTHALF (newer R2)			; 29-32		n 34
						;; mul R10/I10 by w^1 = .924 - .383i
	zfmaddpd zmm2, zmm10, zmm28, zmm13	;; Twiddled R10/.383 = R10*.924/.383 + I10		; 30-33		n 34

	L1prefetch L1p4reg+2*d1reg+64, L1pt
	vsubpd	zmm4, zmm4, zmm17		;; R3 - R7 (newer R7)					; 30-33		n 40
	vsubpd	zmm11, zmm11, zmm6		;; Twiddled I11 / SQRTHALF = I11 - R11			; 31-34		n 40
						;; R11/I11 becomes newer R11/R15

	L1prefetch L1p4reg+d3reg, L1pt
	zfnmaddpd zmm14, zmm14, zmm29, zmm12	;; R4 - R8 * SQRTHALF (newer R8)			; 31-34		n 41
	zfnmaddpd zmm18, zmm18, zmm28, zmm19	;; Twiddled I12/.383 = I12 - R12*.924/.383		; 32-35		n 41
						;; R12/I12 becomes newer R12/R16

	L1prefetch L1p4reg+d3reg+64, L1pt
	zfmaddpd zmm17, zmm22, zmm29, zmm20	;; R3 + R11 * SQRTHALF (last R3)			; 32-35		n 37
	zfmaddpd zmm6, zmm23, zmm27, zmm21	;; R4 + R12*.383 (last R4)				; 33-36		n 37

	vaddpd	zmm12, zmm7, zmm5		;; R1 + R9 (last R1)					; 33-36		n 39
	zfmaddpd zmm19, zmm2, zmm27, zmm15	;; R2 + R10*.383 (last R2)				; 34-37		n 39
	bump	srcreg, srcinc

	zfnmaddpd zmm22, zmm22, zmm29, zmm20	;; R3 - R11 * SQRTHALF (last R11)			; 34-37		n 41
	zfnmaddpd zmm23, zmm23, zmm27, zmm21	;; R4 - R12*.383 (last R12)				; 35-38		n 41
	bump	src4reg, srcinc

	vsubpd	zmm0, zmm0, zmm16		;; R1 - R5 (newer R5)					; 35-38		n 42
	zfmaddpd zmm9, zmm9, zmm26, zmm1	;; I9*weight + I11 (newer I9)				; 36-39		n 42
						;; R9/I9 becomes newer R9/R13

	vsubpd	zmm7, zmm7, zmm5		;; R1 - R9 (last R9)					; 36-39		n 43
	bump	wgtreg, wgtinc

	vmovapd	zmm16, zmm17
	vpermt2pd zmm16, zmm31, zmm6		;; r4_4 r3_4 r4_6 r3_6 r4_0 r3_0 r4_2 r3_2		; 37-39		n 48
	zfnmaddpd zmm2, zmm2, zmm27, zmm15	;; R2 - R10*.383 (last R10)				; 37-40		n 43
	bump	screg1, scinc1

	vpermt2pd zmm17, zmm30, zmm6		;; r4_5 r3_5 r4_7 r3_7 r4_1 r3_1 r4_3 r3_3		; 38-40		n 50
	zfnmaddpd zmm3, zmm3, zmm29, zmm8	;; R2 - R6 * SQRTHALF (newer R6)			; 38-41		n 43
	bump	screg2, scinc2

	vshufpd	zmm6, zmm12, zmm19, 00000000b	;; r2_6 r1_6 r2_4 r1_4 r2_2 r1_2 r2_0 r1_0		; 39		n 48
	zfmsubpd zmm13, zmm13, zmm28, zmm10	;; Twiddled I10/.383 = I10*.924/.383 - R10		; 39-42		n 43
						;; R10/I10 becomes newer R10/R14
	bump	L1preg, srcinc

	vshufpd	zmm12, zmm12, zmm19, 11111111b	;; r2_7 r1_7 r2_5 r1_5 r2_3 r1_3 r2_1 r1_1		; 40		n 50
	zfmaddpd zmm5, zmm11, zmm29, zmm4	;; R7 + R15 * SQRTHALF (last R7)			;  40-43	n 45
	bump	L1p4reg, srcinc

	vmovapd	zmm19, zmm22
	vpermt2pd zmm19, zmm31, zmm23		;; i4_4 i3_4 i4_6 i3_6 i4_0 i3_0 i4_2 i3_2		; 41-43		n 56
	zfmaddpd zmm15, zmm18, zmm27, zmm14	;; R8 + R16*.383 (last R8)				;  41-44	n 45

	vpermt2pd zmm22, zmm30, zmm23		;; i4_5 i3_5 i4_7 i3_7 i4_1 i3_1 i4_3 i3_3		; 42-44		n 58
	vaddpd	zmm8, zmm0, zmm9		;; R5 + R13 (last R5)					;  42-45	n 47

	vshufpd	zmm23, zmm7, zmm2, 00000000b	;; i2_6 i1_6 i2_4 i1_4 i2_2 i1_2 i2_0 i1_0		; 43		n 56
	zfmaddpd zmm10, zmm13, zmm27, zmm3	;; R6 + R14*.383 (last R6)				;  43-46	n 47

	vshufpd	zmm7, zmm7, zmm2, 11111111b	;; i2_7 i1_7 i2_5 i1_5 i2_3 i1_3 i2_1 i1_1		; 44		n 58
	zfnmaddpd zmm11, zmm11, zmm29, zmm4	;; R7 - R15 * SQRTHALF (last R15)			;  44-47	n 49

	vmovapd zmm2, zmm5
	vpermt2pd zmm2, zmm31, zmm15		;; r8_4 r7_4 r8_6 r7_6 r8_0 r7_0 r8_2 r7_2		; 45-47		n 52
	zfnmaddpd zmm18, zmm18, zmm27, zmm14	;; R8 - R16*.383 (last R16)				;  45-48	n 49

	vpermt2pd zmm5, zmm30, zmm15		;; r8_5 r7_5 r8_7 r7_7 r8_1 r7_1 r8_3 r7_3		; 46-48		n 54
	vsubpd	zmm0, zmm0, zmm9		;; R5 - R13 (last R13)					;  46-49	n 51

	vshufpd	zmm15, zmm8, zmm10, 00000000b	;; r6_6 r5_6 r6_4 r5_4 r6_2 r5_2 r6_0 r5_0		; 47		n 52
	zfnmaddpd zmm13, zmm13, zmm27, zmm3	;; R6 - R14*.383 (last R14)				;  47-50	n 51

	vshufpd	zmm8, zmm8, zmm10, 11111111b	;; r6_7 r5_7 r6_5 r5_5 r6_3 r5_3 r6_1 r5_1		; 48		n 54
	vblendmpd zmm9{k7}, zmm16, zmm6		;; r4_4 r3_4 r2_4 r1_4 r4_0 r3_0 r2_0 r1_0		;  48		n 53

	vmovapd zmm10, zmm11
	vpermt2pd zmm10, zmm31, zmm18		;; i8_4 i7_4 i8_6 i7_6 i8_0 i7_0 i8_2 i7_2		; 49-51		n 60
	vblendmpd zmm6{k7}, zmm6, zmm16		;; r2_6 r1_6 r4_6 r3_6 r2_2 r1_2 r4_2 r3_2		;  49		n 55

	vpermt2pd zmm11, zmm30, zmm18		;; i8_5 i7_5 i8_7 i7_7 i8_1 i7_1 i8_3 i7_3		; 50-52		n 62
	vblendmpd zmm16{k7}, zmm17, zmm12	;; r4_5 r3_5 r2_5 r1_5 r4_1 r3_1 r2_1 r1_1		;  50		n 57

	vshufpd	zmm18, zmm0, zmm13, 00000000b	;; i6_6 i5_6 i6_4 i5_4 i6_2 i5_2 i6_0 i5_0		; 51		n 60
	vblendmpd zmm12{k7}, zmm12, zmm17	;; r2_7 r1_7 r4_7 r3_7 r2_3 r1_3 r4_3 r3_3		;  51		n 59

	vshufpd	zmm0, zmm0, zmm13, 11111111b	;; i6_7 i5_7 i6_5 i5_5 i6_3 i5_3 i6_1 i5_1		; 52		n 62
 	vblendmpd zmm17{k7}, zmm2, zmm15	;; r8_4 r7_4 r6_4 r5_4 r8_0 r7_0 r6_0 r5_0		;  52		n 53

	vshuff64x2 zmm13, zmm9, zmm17, 01000100b;; r8_0 r7_0 r6_0 r5_0 r4_0 r3_0 r2_0 r1_0		; 53-55
	vblendmpd zmm15{k7}, zmm15, zmm2	;; r6_6 r5_6 r8_6 r7_6 r6_2 r5_2 r8_2 r7_2		;  53		n 55

	vshuff64x2 zmm9, zmm9, zmm17, 11101110b	;; r8_4 r7_4 r6_4 r5_4 r4_4 r3_4 r2_4 r1_4		; 54-56
	vblendmpd zmm2{k7}, zmm5, zmm8		;; r8_5 r7_5 r6_5 r6_5 r8_1 r7_1 r6_1 r5_1		;  54		n 57

	vshuff64x2 zmm17, zmm6, zmm15, 00010001b;; r8_2 r7_2 r6_2 r5_2 r4_2 r3_2 r2_2 r1_2		; 55-57
	vblendmpd zmm8{k7}, zmm8, zmm5		;; r6_7 r5_7 r8_7 r7_7 r6_3 r5_3 r8_3 r7_3		;  55		n 59

	vshuff64x2 zmm6, zmm6, zmm15, 10111011b ;; r8_6 r7_6 r6_6 r5_6 r4_6 r3_6 r2_6 r1_6		; 56-58
	vblendmpd zmm5{k7}, zmm19, zmm23	;; i4_4 i3_4 i2_4 i1_4 i4_0 i3_0 i2_0 i1_0		;  56		n 61
	zstore	[dstreg], zmm13										; 56

	vshuff64x2 zmm15, zmm16, zmm2, 01000100b;; r8_1 r7_1 r6_1 r5_1 r4_1 r3_1 r2_1 r1_1		; 57-59
	vblendmpd zmm23{k7}, zmm23, zmm19	;; i2_6 i1_6 i4_6 i3_6 i2_2 i1_2 i4_2 i3_2		;  57		n 63
	zstore	[dstreg+e4], zmm9									; 57

	vshuff64x2 zmm16, zmm16, zmm2, 11101110b;; r8_5 r7_5 r6_5 r5_5 r4_5 r3_5 r2_5 r1_5		; 58-60
	vblendmpd zmm19{k7}, zmm22, zmm7	;; i4_5 i3_5 i2_5 i1_5 i4_1 i3_1 i2_1 i1_1		;  58		n 65
	zstore	[dstreg+e2], zmm17									; 58

	vshuff64x2 zmm2, zmm12, zmm8, 00010001b	;; r8_3 r7_3 r6_3 r5_3 r4_3 r3_3 r2_3 r1_3		; 59-61
	vblendmpd zmm7{k7}, zmm7, zmm22		;; i2_7 i1_7 i4_7 i3_7 i2_3 i1_3 i4_3 i3_3		;  59		n 67
	zstore	[dstreg+e4+e2], zmm6									; 59

	vshuff64x2 zmm12, zmm12, zmm8, 10111011b;; r8_7 r7_7 r6_7 r5_7 r4_7 r3_7 r2_7 r1_7		; 60-62
	vblendmpd zmm22{k7}, zmm10, zmm18	;; i8_4 i7_4 i6_4 i5_4 i8_0 i7_0 i6_0 i5_0		;  60		n 61
	zstore	[dstreg+e1], zmm15									; 60

	vshuff64x2 zmm8, zmm5, zmm22, 01000100b ;; i8_0 i7_0 i6_0 i5_0 i4_0 i3_0 i2_0 i1_0		; 61-63
	vblendmpd zmm18{k7}, zmm18, zmm10	;; i6_6 i5_6 i8_6 i7_6 i6_2 i5_2 i8_2 i7_2		;  61		n 63
	zstore	[dstreg+e4+e1], zmm16									; 61

	vshuff64x2 zmm5, zmm5, zmm22, 11101110b;; i8_4 i7_4 i6_4 i5_4 i4_4 i3_4 i2_4 i1_4		; 62-64
	vblendmpd zmm10{k7}, zmm11, zmm0	;; i8_5 i7_5 i6_5 i5_5 i8_1 i7_1 i6_1 i5_1		;  62		n 65
	zstore	[dstreg+e2+e1], zmm2									; 62

	vshuff64x2 zmm22, zmm23, zmm18, 00010001b;; i8_2 i7_2 i6_2 i5_2 i4_2 i3_2 i2_2 i1_2		; 63-65
	vblendmpd zmm0{k7}, zmm0, zmm11		;; i6_7 i5_7 i8_7 i7_7 i6_3 i5_3 i8_3 i7_3		;  63		n 67
	zstore	[dstreg+e4+e2+e1], zmm12								; 63

	vshuff64x2 zmm23, zmm23, zmm18, 10111011b;; i8_6 i7_6 i6_6 i5_6 i4_6 i3_6 i2_6 i1_6		; 64-66
	zstore	[dstreg+64], zmm8									; 64

	vshuff64x2 zmm18, zmm19, zmm10, 01000100b;; i8_1 i7_1 i6_1 i5_1 i4_1 i3_1 i2_1 i1_1		; 65-67
	zstore	[dstreg+e4+64], zmm5									; 65

	vshuff64x2 zmm19, zmm19, zmm10, 11101110b;; i8_5 i7_5 i6_5 i5_5 i4_5 i3_5 i2_5 i1_5		; 66-68
	zstore	[dstreg+e2+64], zmm22									; 66

	vshuff64x2 zmm10, zmm7, zmm0, 00010001b ;; i8_3 i7_3 i6_3 i5_3 i4_3 i3_3 i2_3 i1_3		; 67-69
	zstore	[dstreg+e4+e2+64], zmm23									; 67

	vshuff64x2 zmm7, zmm7, zmm0, 10111011b	;; i8_7 i7_7 i6_7 i5_7 i4_7 i3_7 i2_7 i1_7		; 68-70

	zstore	[dstreg+e1+64], zmm18									; 68
	zstore	[dstreg+e4+e1+64], zmm19								; 69
	zstore	[dstreg+e2+e1+64], zmm10								; 70
	zstore	[dstreg+e4+e2+e1+64], zmm7								; 71
	bump	dstreg, dstinc
	ENDM


;;*******************************************************************************
;; In theory, these macros should reduce roundoff error, in practice they did not
;;*******************************************************************************

IFDEF TRY_SQRT2_TO_REDUCE_ROUNDOFF

;; NOTE:  To reduce roundoff error, we premultiply sine28 and sine46 by SQRTHALF.  It is more accurate to compute
;;	R2 = ((SQRT2*(r1-r5)+((r2-r6)-(r4-r8)))  - (SQRT2*(i3-i7)+((i2-i6)+(i4-i8))))	*  SQRTHALF*sine28
;;		rather than
;;	R2 = (((r1-r5)+.707((r2-r6)-(r4-r8)))  - ((i3-i7)+.707((i2-i6)+(i4-i8))))	*  sine28

zr8_8c_djbfft_cmn_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRT2
	ENDM
zr8_8c_djbfft_cmn MACRO srcreg,srcoff,srcinc,d1,d2,d4,bcast,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	zmm1, [srcreg+srcoff+d1]		;; R2
	vmovapd	zmm2, [srcreg+srcoff+d4+d1]		;; R6
	vsubpd	zmm0, zmm1, zmm2			;; R2-R6				; 1-4		n 9
	vaddpd	zmm1, zmm1, zmm2			;; R2+R6				; 1-4		n 11

	vmovapd	zmm3, [srcreg+srcoff+d2+d1]		;; R4
	vmovapd	zmm4, [srcreg+srcoff+d4+d2+d1]		;; R8
	vsubpd	zmm2, zmm3, zmm4			;; R4-R8				; 2-5		n 9
	vaddpd	zmm3, zmm3, zmm4			;; R4+R8				; 2-5		n 11

	vmovapd	zmm5, [srcreg+srcoff+d2+d1+64]		;; I4
	vmovapd	zmm6, [srcreg+srcoff+d4+d2+d1+64]	;; I8
	vsubpd	zmm4, zmm5, zmm6			;; I4-I8				; 3-6		n 10
	vaddpd	zmm5, zmm5, zmm6			;; I4+I8				; 3-6		n 12

	vmovapd	zmm7, [srcreg+srcoff+d1+64]		;; I2
	vmovapd	zmm8, [srcreg+srcoff+d4+d1+64]		;; I6
	vsubpd	zmm6, zmm7, zmm8			;; I2-I6				; 4-7		n 10
	vaddpd	zmm7, zmm7, zmm8			;; I2+I6				; 4-7		n 12

	vmovapd	zmm9, [srcreg+srcoff]			;; R1
	vmovapd	zmm10, [srcreg+srcoff+d4]		;; R5
	vaddpd	zmm8, zmm9, zmm10			;; R1+R5				; 5-8		n 13
	vsubpd	zmm9, zmm9, zmm10			;; R1-R5				; 5-8		n 14

	vmovapd	zmm11, [srcreg+srcoff+d2]		;; R3
	vmovapd	zmm12, [srcreg+srcoff+d4+d2]		;; R7
	vaddpd	zmm10, zmm11, zmm12			;; R3+R7				; 6-9		n 13
	vsubpd	zmm11, zmm11, zmm12			;; R3-R7				; 6-9		n 17

	vmovapd	zmm13, [srcreg+srcoff+64]		;; I1
	vmovapd	zmm14, [srcreg+srcoff+d4+64]		;; I5
	vaddpd	zmm12, zmm13, zmm14			;; I1+I5				; 7-10		n 16
	vsubpd	zmm13, zmm13, zmm14			;; I1-I5				; 7-10		n 15

	vmovapd	zmm15, [srcreg+srcoff+d2+64]		;; I3
	vmovapd	zmm16, [srcreg+srcoff+d4+d2+64]		;; I7
	vaddpd	zmm14, zmm15, zmm16			;; I3+I7				; 8-11		n 16
	vsubpd	zmm15, zmm15, zmm16			;; I3-I7				; 8-11		n 18

	vaddpd	zmm16, zmm0, zmm2		;; r2-+ = (r2-r6) + (r4-r8)			; 9-12		n 17
	vsubpd	zmm0, zmm0, zmm2		;; r2-- = (r2-r6) - (r4-r8)			; 9-12		n 14
bcast	vbroadcastsd zmm30, [screg+1*16]	;; sine for R3/I3 and R7/I7 (w^2)
no bcast vmovapd zmm30, [screg+1*128]		;; sine for R3/I3 and R7/I7

	vsubpd	zmm2, zmm6, zmm4		;; i2-- = (i2-i6) - (i4-i8)			; 10-13		n 15
	vaddpd	zmm6, zmm6, zmm4		;; i2-+ = (i2-i6) + (i4-i8)			; 10-13		n 18
bcast	vbroadcastsd zmm23, [screg+0*16]	;; .707*sine for R2/I2 and R8/I8 (w^1)
no bcast vmovapd zmm23, [screg+0*128]		;; .707*sine for R2/I2 and R8/I8

	vsubpd	zmm4, zmm1, zmm3		;; r2+- = (r2+r6) - (r4+r8)			; 11-14		n 19
	vaddpd	zmm1, zmm1, zmm3		;; r2++ = (r2+r6) + (r4+r8)			; 11-14		n 22
bcast	vbroadcastsd zmm24, [screg+2*16]	;; .707*sine for R4/I4 and R6/I6 (w^3)
no bcast vmovapd zmm24, [screg+2*128]		;; .707*sine for R4/I4 and R6/I6

	vsubpd	zmm3, zmm7, zmm5		;; i2+- = (i2+i6) - (i4+i8)			; 12-15		n 19
	vaddpd	zmm7, zmm7, zmm5		;; i2++ = (i2+i6) + (i4+i8)			; 12-15		n 23
bcast	vbroadcastsd zmm26, [screg+1*16+8]	;; cosine/sine for R3/I3 and R7/I7 (w^2)
no bcast vmovapd zmm26, [screg+1*128+64]	;; cosine/sine for R3/I3 and R7/I7

	vaddpd	zmm5, zmm8, zmm10		;; r1++ = (r1+r5) + (r3+r7)			; 13-16		n 22
	vsubpd	zmm8, zmm8, zmm10		;; r1+- = (r1+r5) - (r3+r7)			; 13-16		n 24
bcast	vbroadcastsd zmm29, [screg+3*16+8]	;; cosine/sine for R5/I5 (w^4)
no bcast vmovapd zmm29, [screg+3*128+64]	;; cosine/sine for R5/I5

	zfmaddpd zmm10, zmm9, zmm31, zmm0	;; r1-+ = SQRT2*(r1-r5) + (r2--)		; 14-17		n 20
	zfmsubpd zmm0, zmm9, zmm31, zmm0	;; r1-- = SQRT2*(r1-r5) - (r2--)		; 14-17		n 21
bcast	vbroadcastsd zmm27, [screg+0*16+8]	;; cosine/sine for R2/I2 and R8/I8 (w^1)
no bcast vmovapd zmm27, [screg+0*128+64]	;; cosine/sine for R2/I2 and R8/I8

	zfmaddpd zmm9, zmm13, zmm31, zmm2	;; i1-+ = SQRT2*(i1-i5) + (i2--)		; 15-18		n 20
	zfmsubpd zmm2, zmm13, zmm31, zmm2	;; i1-- = SQRT2*(i1-i5) - (i2--)		; 15-18		n 21
bcast	vbroadcastsd zmm28, [screg+2*16+8]	;; cosine/sine for R4/I4 and R6/I6 (w^3)
no bcast vmovapd zmm28, [screg+2*128+64]	;; cosine/sine for R4/I4 and R6/I6

	vaddpd	zmm13, zmm12, zmm14		;; i1++ = (i1+i5) + (i3+i7)			; 16-19		n 23
	vsubpd	zmm12, zmm12, zmm14		;; i1+- = (i1+i5) - (i3+i7)			; 16-19		n 24
bcast	vbroadcastsd zmm25, [screg+3*16]	;; sine for R5/I5 (w^4)
no bcast vmovapd zmm25, [screg+3*128]		;; sine for R5/I5

	zfmaddpd zmm14, zmm11, zmm31, zmm16	;; r3-+ = SQRT2*(r3-r7) + (r2-+)		; 17-20		n 26
	zfmsubpd zmm16, zmm11, zmm31, zmm16	;; r3-- = SQRT2*(r3-r7) - (r2-+)		; 17-20		n 30
	bump	screg, scinc
	L1prefetchw srcreg+L1pd, L1pt

	zfmaddpd zmm11, zmm15, zmm31, zmm6	;; i3-+ = SQRT2*(i3-i7) + (i2-+)		; 18-21		n 26
	zfmsubpd zmm6, zmm15, zmm31, zmm6	;; i3-- = SQRT2*(i3-i7) - (i2-+)		; 18-21		n 30
	L1prefetchw srcreg+64+L1pd, L1pt

	vmulpd	zmm4, zmm4, zmm30		;; r2+-s = r2+- * sine37			; 19-22		n 24
	vmulpd	zmm3, zmm3, zmm30		;; i2+-s = i2+- * sine37			; 19-22		n 24
	L1prefetchw srcreg+d1+L1pd, L1pt

	vmulpd	zmm10, zmm10, zmm23		;; r1-+s = r1-+ * .707*sine28			; 20-23		n 26
	vmulpd	zmm9, zmm9, zmm23		;; i1-+s = i1-+ * .707*sine28			; 20-23		n 26
	L1prefetchw srcreg+d1+64+L1pd, L1pt

	vmulpd	zmm0, zmm0, zmm24		;; r1--s = r1-- * .707*sine46			; 21-24		n 30
	vmulpd	zmm2, zmm2, zmm24		;; i1--s = i1-- * .707*sine46			; 21-24		n 30
	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	zmm15, zmm5, zmm1		;; R5 = (r1++) - (r2++)				; 22-25		n 32
	vaddpd	zmm5, zmm5, zmm1		;; R1 = (r1++) + (r2++)				; 22-25
	L1prefetchw srcreg+d2+64+L1pd, L1pt

	vsubpd	zmm1, zmm13, zmm7		;; I5 = (i1++) - (i2++)				; 23-26		n 32
	vaddpd	zmm13, zmm13, zmm7		;; I1 = (i1++) + (i2++)				; 23-26
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	zfmsubpd zmm7, zmm8, zmm30, zmm3	;; R3s = (r1+-)*sine37 - (i2+-s)		; 24-27		n 28
	zfmaddpd zmm17, zmm12, zmm30, zmm4	;; I3s = (i1+-)*sine37 + (r2+-s)		; 24-27		n 28
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt

	zfmaddpd zmm8, zmm8, zmm30, zmm3	;; R7s = (r1+-)*sine37 + (i2+-s)		; 25-28		n 32
	zfmsubpd zmm12, zmm12, zmm30, zmm4	;; I7s = (i1+-)*sine37 - (r2+-s)		; 25-28		n 32
	L1prefetchw srcreg+d4+L1pd, L1pt

	zfnmaddpd zmm3, zmm11, zmm23, zmm10	;; R2s = (r1-+s) - .707*sine28*(i3-+)		; 26-29		n 33
	zfmaddpd zmm4, zmm14, zmm23, zmm9	;; I2s = (i1-+s) + .707*sine28*(r3-+)		; 26-29		n 33
	zstore	[srcreg], zmm5			;; Store R1					; 26
	L1prefetchw srcreg+d4+64+L1pd, L1pt

	zfmaddpd zmm11, zmm11, zmm23, zmm10	;; R8s = (r1-+s) + .707*sine28*(i3-+)		; 27-30		n 34
	zfnmaddpd zmm14, zmm14, zmm23, zmm9	;; I8s = (i1-+s) - .707*sine28*(r3-+)		; 27-30		n 34
	zstore	[srcreg+64], zmm13		;; Store I1					; 27
	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	zfmsubpd zmm10, zmm7, zmm26, zmm17	;; R3s * cosine/sine - I3s (final R3)		; 28-31
	zfmaddpd zmm17, zmm17, zmm26, zmm7	;; I3s * cosine/sine + R3s (final I3)		; 28-31
	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt

	zfmaddpd zmm9, zmm8, zmm26, zmm12	;; R7s * cosine/sine + I7s (final R7)		; 29-32
	zfmsubpd zmm12, zmm12, zmm26, zmm8	;; I7s * cosine/sine - R7s (final I7)		; 29-32
	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	zfmaddpd zmm7, zmm6, zmm24, zmm0	;; R4s = (r1--s) + .707*sine46*(i3--)		; 30-33		n 35
	zfnmaddpd zmm8, zmm16, zmm24, zmm2	;; I4s = (i1--s) - .707*sine46*(r3--)		; 30-33		n 35
	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt

	zfnmaddpd zmm6, zmm6, zmm24, zmm0	;; R6s = (r1--s) - .707*sine46*(i3--)		; 31-34		n 36
	zfmaddpd zmm16, zmm16, zmm24, zmm2	;; I6s = (i1--s) + .707*sine46*(r3--)		; 31-34		n 36
	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	zfmsubpd zmm0, zmm15, zmm29, zmm1	;; A5 = R5 * cosine/sine - I5			; 32-35		n 37
	zfmaddpd zmm1, zmm1, zmm29, zmm15	;; B5 = I5 * cosine/sine + R5			; 32-35		n 37
	zstore	[srcreg+d2], zmm10		;; Store R3					; 32
	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt

	zfmsubpd zmm2, zmm3, zmm27, zmm4	;; R2s * cosine/sine - I2s (final R2)		; 33-36
	zfmaddpd zmm4, zmm4, zmm27, zmm3	;; I2s * cosine/sine + R2s (final I2)		; 33-36
	zstore	[srcreg+d2+64], zmm17		;; Store I3					; 32+1

	zfmaddpd zmm15, zmm11, zmm27, zmm14	;; R8s * cosine/sine + I8s (final R8)		; 34-37
	zfmsubpd zmm14, zmm14, zmm27, zmm11	;; I8s * cosine/sine - R8s (final I8)		; 34-37
	zstore	[srcreg+d4+d2], zmm9		;; Store R7					; 33+1

	zfmsubpd zmm3, zmm7, zmm28, zmm8	;; R4s * cosine/sine - I4s (final R4)		; 35-38
	zfmaddpd zmm8, zmm8, zmm28, zmm7	;; I4s * cosine/sine + R4s (final I4)		; 35-38
	zstore	[srcreg+d4+d2+64], zmm12	;; Store I7					; 33+2

	zfmaddpd zmm11, zmm6, zmm28, zmm16	;; R6s * cosine/sine + I6s (final R6)		; 36-39
	zfmsubpd zmm16, zmm16, zmm28, zmm6	;; I6s * cosine/sine - R6s (final I6)		; 36-39

	vmulpd	zmm0, zmm0, zmm25		;; A5 = A5 * sine (final R5)			; 37-40
	vmulpd	zmm1, zmm1, zmm25		;; B5 = B5 * sine (final I5)			; 37-40

	zstore	[srcreg+d1], zmm2		;; Store R2					; 37
	zstore	[srcreg+d1+64], zmm4		;; Store I2					; 37+1
	zstore	[srcreg+d4+d2+d1], zmm15	;; Store R8					; 38+1
	zstore	[srcreg+d4+d2+d1+64], zmm14	;; Store I8					; 38+2
	zstore	[srcreg+d2+d1], zmm3		;; Store R4					; 39+2
	zstore	[srcreg+d2+d1+64], zmm8		;; Store I4					; 39+3
	zstore	[srcreg+d4+d1], zmm11		;; Store R6					; 40+3
	zstore	[srcreg+d4+d1+64], zmm16	;; Store I6					; 40+4
	zstore	[srcreg+d4], zmm0		;; Store R5					; 41+4
	zstore	[srcreg+d4+64], zmm1		;; Store I5					; 41+5
	bump	srcreg, srcinc
	ENDM


;; NOTE:  To reduce roundoff error, sine28 and sine46 are premultiplied by SQRTHALF.

zr8_8c_djbunfft_cmn_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRT2
	ENDM
zr8_8c_djbunfft_cmn MACRO srcreg,srcinc,d1,d2,d4,bcast,screg,scinc,maxrpt,L1pt,L1pd
bcast	vbroadcastsd zmm30, [screg+0*16+8]	;; cosine/sine for R2/I2 and R8/I8 (w^1)
no bcast vmovapd zmm30, [screg+0*128+64]	;; cosine/sine for R2/I2 and R8/I8
	vmovapd zmm4, [srcreg+d1]		;; Load R2
	vmovapd zmm12, [srcreg+d1+64]		;; Load I2
	zfmaddpd zmm16, zmm4, zmm30, zmm12	;; A2 = R2 * cosine/sine + I2			; 1-4		n 5
	zfmsubpd zmm12, zmm12, zmm30, zmm4	;; B2 = I2 * cosine/sine - R2			; 1-4		n 5

	vmovapd zmm7, [srcreg+d4+d2+d1]		;; Load R8
	vmovapd zmm15, [srcreg+d4+d2+d1+64]	;; Load I8
	zfmsubpd zmm4, zmm7, zmm30, zmm15	;; A8 = R8 * cosine/sine - I8			; 2-5		n 11
	zfmaddpd zmm15, zmm15, zmm30, zmm7	;; B8 = I8 * cosine/sine + R8			; 2-5		n 12

bcast	vbroadcastsd zmm30, [screg+2*16+8]	;; cosine/sine for R4/I4 and R6/I6 (w^3)
no bcast vmovapd zmm30, [screg+2*128+64]	;; cosine/sine for R4/I4 and R6/I6
	vmovapd zmm6, [srcreg+d2+d1]		;; Load R4
	vmovapd zmm14, [srcreg+d2+d1+64]	;; Load I4
	zfmaddpd zmm7, zmm6, zmm30, zmm14	;; A4 = R4 * cosine/sine + I4 (first R4/sine)	; 3-6		n 9
	zfmsubpd zmm14, zmm14, zmm30, zmm6	;; B4 = I4 * cosine/sine - R4 (first I4/sine)	; 3-6		n 10

	vmovapd zmm5, [srcreg+d4+d1]		;; Load R6
	vmovapd zmm13, [srcreg+d4+d1+64]	;; Load I6
	zfmsubpd zmm6, zmm5, zmm30, zmm13	;; A6 = R6 * cosine/sine - I6 (first R6/sine)	; 4-7		n 9
	zfmaddpd zmm13, zmm13, zmm30, zmm5	;; B6 = I6 * cosine/sine + R6 (first I6/sine)	; 4-7		n 10

bcast	vbroadcastsd zmm29, [screg+0*16]	;; .707*sine for R2/I2 and R8/I8 (w^1)
no bcast vmovapd zmm29, [screg+0*128]		;; .707*sine for R2/I2 and R8/I8
	vmulpd	zmm16, zmm16, zmm29		;; A2 = A2 * .707*sine (first R2)		; 5-8		n 11
	vmulpd	zmm12, zmm12, zmm29		;; B2 = B2 * .707*sine (first I2)		; 5-8		n 12

bcast	vbroadcastsd zmm30, [screg+3*16+8]	;; cosine/sine for R5/I5 (w^4)
no bcast vmovapd zmm30, [screg+3*128+64]	;; cosine/sine for R5/I5
	vmovapd zmm1, [srcreg+d4]		;; Load R5
	vmovapd zmm9, [srcreg+d4+64]		;; Load I5
	zfmaddpd zmm5, zmm1, zmm30, zmm9	;; A5 = R5 * cosine/sine + I5 (first R5/sine)	; 6-9		n 13
	zfmsubpd zmm9, zmm9, zmm30, zmm1	;; B5 = I5 * cosine/sine - R5 (first I5/sine)	; 6-9		n 14

bcast	vbroadcastsd zmm30, [screg+1*16+8]	;; cosine/sine for R3/I3 and R7/I7 (w^2)
no bcast vmovapd zmm30, [screg+1*128+64]	;; cosine/sine for R3/I3 and R7/I7
	vmovapd zmm2, [srcreg+d2]		;; Load R3
	vmovapd zmm10, [srcreg+d2+64]		;; Load I3
	zfmaddpd zmm1, zmm2, zmm30, zmm10	;; A3 = R3 * cosine/sine + I3 (first R3/sine)	; 7-10		n 15
	zfmsubpd zmm10, zmm10, zmm30, zmm2	;; B3 = I3 * cosine/sine - R3 (first I3/sine)	; 7-10		n 16

	vmovapd zmm3, [srcreg+d4+d2]		;; Load R7
	vmovapd zmm11, [srcreg+d4+d2+64]	;; Load I7
	zfmsubpd zmm2, zmm3, zmm30, zmm11	;; A7 = R7 * cosine/sine - I7 (first R7/sine)	; 8-11		n 15
	zfmaddpd zmm11, zmm11, zmm30, zmm3	;; B7 = I7 * cosine/sine + R7 (first I7/sine)	; 8-11		n 16

	vmovapd zmm0, [srcreg]			;; Load R1
	vaddpd	zmm3, zmm7, zmm6		;; R4/sine + R6/sine				; 9-12		n 17
	vsubpd	zmm7, zmm7, zmm6		;; R4/sine - R6/sine				; 9-12		n 18

	vmovapd zmm8, [srcreg+64]		;; Load I1
	vaddpd	zmm6, zmm14, zmm13		;; I4/sine + I6/sine				; 10-13		n 20
	vsubpd	zmm14, zmm14, zmm13		;; I4/sine - I6/sine				; 10-13		n 19

bcast	vbroadcastsd zmm28, [screg+3*16]	;; sine for R5/I5 (w^4)
no bcast vmovapd zmm28, [screg+3*128]		;; sine for R5/I5
	zfmaddpd zmm13, zmm4, zmm29, zmm16	;; R2 + R8 * .707*sine				; 11-14		n 17
	zfnmaddpd zmm4, zmm4, zmm29, zmm16	;; R2 - R8 * .707*sine				; 11-14		n 18

bcast	vbroadcastsd zmm27, [screg+2*16]	;; .707*sine for R4/I4 and R6/I6 (w^3)
no bcast vmovapd zmm27, [screg+2*128]		;; .707*sine for R4/I4 and R6/I6
	zfmaddpd zmm16, zmm15, zmm29, zmm12	;; I2 + I8 * .707*sine				; 12-15		n 20
	zfnmaddpd zmm15, zmm15, zmm29, zmm12	;; I2 - I8 * .707*sine				; 12-15		n 19

bcast	vbroadcastsd zmm26, [screg+1*16]	;; sine for R3/I3 and R7/I7 (w^2)
no bcast vmovapd zmm26, [screg+1*128]		;; sine for R3/I3
	zfmaddpd zmm12, zmm5, zmm28, zmm0	;; R1 + R5 * sine				; 13-16		n 21
	zfnmaddpd zmm5, zmm5, zmm28, zmm0	;; R1 - R5 * sine				; 13-16		n 23
	bump	screg, scinc

	L1prefetch srcreg+L1pd, L1pt
	zfmaddpd zmm0, zmm9, zmm28, zmm8	;; I1 + I5 * sine				; 14-17		n 22
	zfnmaddpd zmm9, zmm9, zmm28, zmm8	;; I1 - I5 * sine				; 14-17		n 24

	L1prefetch srcreg+64+L1pd, L1pt
	vaddpd	zmm8, zmm1, zmm2		;; R3/sine + R7/sine				; 15-18		n 21
	vsubpd	zmm1, zmm1, zmm2		;; R3/sine - R7/sine				; 15-18		n 24

	L1prefetch srcreg+d1+L1pd, L1pt
	vaddpd	zmm2, zmm10, zmm11		;; I3/sine + I7/sine				; 16-19		n 22
	vsubpd	zmm10, zmm10, zmm11		;; I3/sine - I7/sine				; 16-19		n 23

	L1prefetch srcreg+d1+64+L1pd, L1pt
	zfmaddpd zmm11, zmm3, zmm27, zmm13	;; r2++ = (r2+r8) + (r4+r6) * .707*sine		; 17-20		n 27
	zfnmaddpd zmm3, zmm3, zmm27, zmm13	;; r2+- = (r2+r8) - (r4+r6) * .707*sine		; 17-20		n 25

	L1prefetch srcreg+d2+L1pd, L1pt
	zfmaddpd zmm13, zmm7, zmm27, zmm4	;; r2-+ = (r2-r8) + (r4-r6) * .707*sine		; 18-21		n 26
	zfnmaddpd zmm7, zmm7, zmm27, zmm4	;; r2-- = (r2-r8) - (r4-r6) * .707*sine		; 18-21		n 30

	L1prefetch srcreg+d2+64+L1pd, L1pt
	zfmaddpd zmm4, zmm14, zmm27, zmm15	;; i2-+ = (i2-i8) + (i4-i6) * .707*sine		; 19-22		n 25
	zfnmaddpd zmm14, zmm14, zmm27, zmm15	;; i2-- = (i2-i8) - (i4-i6) * .707*sine		; 19-22		n 29

	L1prefetch srcreg+d2+d1+L1pd, L1pt
	zfmaddpd zmm15, zmm6, zmm27, zmm16	;; i2++ = (i2+i8) + (i4+i6) * .707*sine		; 20-23		n 28
	zfnmaddpd zmm6, zmm6, zmm27, zmm16	;; i2+- = (i2+i8) - (i4+i6) * .707*sine		; 20-23		n 26

	L1prefetch srcreg+d2+d1+64+L1pd, L1pt
	zfmaddpd zmm16, zmm8, zmm26, zmm12	;; r1++ = (r1+r5) + (r3+r7) * sine		; 21-24		n 27
	zfnmaddpd zmm8, zmm8, zmm26, zmm12	;; r1+- = (r1+r5) - (r3+r7) * sine		; 21-24		n 29

	L1prefetch srcreg+d4+L1pd, L1pt
	zfmaddpd zmm12, zmm2, zmm26, zmm0	;; i1++ = (i1+i5) + (i3+i7) * sine		; 22-25		n 28
	zfnmaddpd zmm2, zmm2, zmm26, zmm0	;; i1+- = (i1+i5) - (i3+i7) * sine		; 22-25		n 30

	L1prefetch srcreg+d4+64+L1pd, L1pt
	zfmaddpd zmm0, zmm10, zmm26, zmm5	;; r1-+ = (r1-r5) + (i3-i7) * sine		; 23-26		n 31
	zfnmaddpd zmm10, zmm10, zmm26, zmm5	;; r1-- = (r1-r5) - (i3-i7) * sine		; 23-26		n 33

	L1prefetch srcreg+d4+d1+L1pd, L1pt
	zfmaddpd zmm5, zmm1, zmm26, zmm9	;; i1-+ = (i1-i5) + (r3-r7) * sine		; 24-27		n 34
	zfnmaddpd zmm1, zmm1, zmm26, zmm9	;; i1-- = (i1-i5) - (r3-r7) * sine		; 24-27		n 32

	L1prefetch srcreg+d4+d1+64+L1pd, L1pt
	vaddpd	zmm9, zmm3, zmm4		;; r2+-+ = (r2+-) + (i2-+)			; 25-28		n 31
	vsubpd	zmm3, zmm3, zmm4		;; r2+-- = (r2+-) - (i2-+)			; 25-28		n 33

	L1prefetch srcreg+d4+d2+L1pd, L1pt
	vaddpd	zmm4, zmm13, zmm6		;; r2-++ = (r2-+) + (i2+-)			; 26-29		n 34
	vsubpd	zmm13, zmm13, zmm6		;; r2-+- = (r2-+) - (i2+-)			; 26-29		n 32

	L1prefetch srcreg+d4+d2+64+L1pd, L1pt
	zfmaddpd zmm6, zmm11, zmm31, zmm16	;; R1 = (r1++) + SQRT2*(r2++)			; 27-30
	zfnmaddpd zmm16, zmm11, zmm31, zmm16	;; R5 = (r1++) - SQRT2*(r2++)			; 27-30

	L1prefetch srcreg+d4+d2+d1+L1pd, L1pt
	zfmaddpd zmm11, zmm15, zmm31, zmm12	;; I1 = (i1++) + SQRT2*(i2++)			; 28-31
	zfnmaddpd zmm12, zmm15, zmm31, zmm12	;; I5 = (i1++) - SQRT2*(i2++)			; 28-31

	L1prefetch srcreg+d4+d2+d1+64+L1pd, L1pt
	zfmaddpd zmm15, zmm14, zmm31, zmm8	;; R3 = (r1+-) + SQRT2*(i2--)			; 29-32
	zfnmaddpd zmm8, zmm14, zmm31, zmm8	;; R7 = (r1+-) - SQRT2*(i2--)			; 29-32

	zfnmaddpd zmm14, zmm7, zmm31, zmm2	;; I3 = (i1+-) - SQRT2*(r2--)			; 30-33
	zfmaddpd zmm2, zmm7, zmm31, zmm2	;; I7 = (i1+-) + SQRT2*(r2--)			; 30-33

	vaddpd	zmm7, zmm0, zmm9		;; R2 = (r1-+) + (r2+-+)			; 31-34
	vsubpd	zmm9, zmm0, zmm9		;; R6 = (r1-+) - (r2+-+)			; 31-34
	zstore	[srcreg], zmm6			;; Save R1					; 31

	vsubpd	zmm0, zmm1, zmm13		;; I2 = (i1--) - (r2-+-)			; 32-35
	vaddpd	zmm13, zmm1, zmm13		;; I6 = (i1--) + (r2-+-)			; 32-35
	zstore	[srcreg+d4], zmm16		;; Save R5					; 31+1

	vsubpd	zmm1, zmm10, zmm3		;; R4 = (r1--) - (r2+--)			; 33-36
	vaddpd	zmm3, zmm10, zmm3		;; R8 = (r1--) + (r2+--)			; 33-36
	zstore	[srcreg+64], zmm11		;; Save I1					; 32+1

	vsubpd	zmm10, zmm5, zmm4		;; I4 = (i1-+) - (r2-++)			; 34-37
	vaddpd	zmm4, zmm5, zmm4		;; I8 = (i1-+) + (r2-++)			; 34-37
	zstore	[srcreg+d4+64], zmm12		;; Save I5					; 32+2

	zstore	[srcreg+d2], zmm15		;; Save R3					; 33+2
	zstore	[srcreg+d4+d2], zmm8		;; Save R7					; 33+3
	zstore	[srcreg+d2+64], zmm14		;; Save I3					; 34+3
	zstore	[srcreg+d4+d2+64], zmm2		;; Save I7					; 34+4
	zstore	[srcreg+d1], zmm7		;; Save R2					; 35+4
	zstore	[srcreg+d4+d1], zmm9		;; Save R6					; 35+5
	zstore	[srcreg+d1+64], zmm0		;; Save I2					; 36+5
	zstore	[srcreg+d4+d1+64], zmm13	;; Save I6					; 36+6
	zstore	[srcreg+d2+d1], zmm1		;; Save R4					; 37+6
	zstore	[srcreg+d4+d2+d1], zmm3		;; Save R8					; 37+7
	zstore	[srcreg+d2+d1+64], zmm10	;; Save I4					; 38+7
	zstore	[srcreg+d4+d2+d1+64], zmm4	;; Save I8					; 38+8
	bump	srcreg, srcinc
	ENDM


zr8_csc_wpn_eight_complex_first_djbfft_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRT2
	vbroadcastsd zmm28, ZMM_ONE_OVER_B
	ENDM
zr8_csc_wpn_eight_complex_first_djbfft MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maskreg,maskinc,grpreg,grpinc,maxrpt,L1pt,L1pd
	mov	bl, [maskreg+0*1]		;; Load index into compressed fudges
	mov	r14, [r13+0*8]			;; Load the xor mask
	xor	r14, [r12+rbx*8]		;; Apply the xor mask to 64 bits of fudge flags

	kmovw	k1, r14d			;; Load R1 and I1 fudge factor mask		; 1
	kshiftrw k2, k1, 8			;; I1's fudge					; 2
	vmovapd zmm0, [grpreg+0*64]		;; group multiplier for R1
	vmulpd	zmm0{k1}, zmm0, zmm28		;; fudged group multiplier for R1		; 2-5
	vmovapd zmm8, [grpreg+1*64]		;; group multiplier for I1
	vmulpd	zmm8{k2}, zmm8, zmm28		;; fudged group multiplier for I1		; 3-6

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R2 and I2 fudge factor mask		; 3
	kshiftrw k2, k1, 8			;; I2's fudge					; 4
	vmovapd zmm1, [grpreg+2*64]		;; group multiplier for R2
	vmulpd	zmm1{k1}, zmm1, zmm28		;; fudged group multiplier for R2		; 4-7
	vmovapd zmm9, [grpreg+3*64]		;; group multiplier for I2
	vmulpd	zmm9{k2}, zmm9, zmm28		;; fudged group multiplier for I2		; 5-8

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R3 and I3 fudge factor mask		; 5
	kshiftrw k2, k1, 8			;; I3's fudge					; 6
	vmovapd zmm2, [grpreg+4*64]		;; group multiplier for R3
	vmulpd	zmm2{k1}, zmm2, zmm28		;; fudged group multiplier for R3		; 6-9
	vmovapd zmm10, [grpreg+5*64]		;; group multiplier for I3
	vmulpd	zmm10{k2}, zmm10, zmm28		;; fudged group multiplier for I3		; 7-10

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R4 and I4 fudge factor mask		; 7
	kshiftrw k2, k1, 8			;; I4's fudge					; 8
	vmovapd zmm3, [grpreg+6*64]		;; group multiplier for R4
	vmulpd	zmm3{k1}, zmm3, zmm28		;; fudged group multiplier for R4		; 8-11
	vmovapd zmm11, [grpreg+7*64]		;; group multiplier for I4
	vmulpd	zmm11{k2}, zmm11, zmm28		;; fudged group multiplier for I4		; 9-12

	vmulpd	zmm0, zmm0, [srcreg]		;; apply the fudged group multiplier to R1	; 9-12
	vmulpd	zmm8, zmm8, [srcreg+64]		;; apply the fudged group multiplier to I1	; 10-13
	vmulpd	zmm1, zmm1, [srcreg+d1]		;; apply the fudged group multiplier to R2	; 10-13
	vmulpd	zmm9, zmm9, [srcreg+d1+64]	;; apply the fudged group multiplier to I2	; 11-14
	vmulpd	zmm2, zmm2, [srcreg+d2]		;; apply the fudged group multiplier to R3	; 11-14
	vmulpd	zmm10, zmm10, [srcreg+d2+64]	;; apply the fudged group multiplier to I3	; 12-15
	vmulpd	zmm3, zmm3, [srcreg+d2+d1]	;; apply the fudged group multiplier to R4	; 12-15
	vmulpd	zmm11, zmm11, [srcreg+d2+d1+64]	;; apply the fudged group multiplier to I4	; 13-16

	mov	bl, [maskreg+1*1]		;; Load index into compressed fudges
	mov	r14, [r13+1*8]			;; Load the xor mask
	xor	r14, [r12+rbx*8]		;; Apply the xor mask to 64 bits of fudge flags

	kmovw	k1, r14d			;; Load R5 and I5 fudge factor mask		; 13
	kshiftrw k2, k1, 8			;; I5's fudge					; 14
	vmovapd zmm4, [grpreg+8*64]		;; group multiplier for R5
	vmulpd	zmm4{k1}, zmm4, zmm28		;; fudged group multiplier for R5		; 14-17
	vmovapd zmm12, [grpreg+9*64]		;; group multiplier for I5
	vmulpd	zmm12{k2}, zmm12, zmm28		;; fudged group multiplier for I5		; 15-18

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R6 and I6 fudge factor mask		; 15
	kshiftrw k2, k1, 8			;; I6's fudge					; 16
	vmovapd zmm5, [grpreg+10*64]		;; group multiplier for R6
	vmulpd	zmm5{k1}, zmm5, zmm28		;; fudged group multiplier for R6		; 16-19
	vmovapd zmm13, [grpreg+11*64]		;; group multiplier for I6
	vmulpd	zmm13{k2}, zmm13, zmm28		;; fudged group multiplier for I6		; 17-20

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R7 and I7 fudge factor mask		; 17
	kshiftrw k2, k1, 8			;; I7's fudge					; 18
	vmovapd zmm6, [grpreg+12*64]		;; group multiplier for R7
	vmulpd	zmm6{k1}, zmm6, zmm28		;; fudged group multiplier for R7		; 18-21
	vmovapd zmm14, [grpreg+13*64]		;; group multiplier for I7
	vmulpd	zmm14{k2}, zmm14, zmm28		;; fudged group multiplier for I7		; 19-22

	shr	r14, 16				;; Next 16 bits of fudge flags
	kmovw	k1, r14d			;; Load R8 and I8 fudge factor mask		; 19
	kshiftrw k2, k1, 8			;; I8's fudge					; 20
	vmovapd zmm7, [grpreg+14*64]		;; group multiplier for R8
	vmulpd	zmm7{k1}, zmm7, zmm28		;; fudged group multiplier for R8		; 20-23
	vmovapd zmm15, [grpreg+15*64]		;; group multiplier for I8
	vmulpd	zmm15{k2}, zmm15, zmm28		;; fudged group multiplier for I8		; 21-24

	vmulpd	zmm4, zmm4, [srcreg+d4]		;; apply the fudged group multiplier to R5	; 21-24
	vmulpd	zmm12, zmm12, [srcreg+d4+64]	;; apply the fudged group multiplier to I5	; 22-25
	vmulpd	zmm5, zmm5, [srcreg+d4+d1]	;; apply the fudged group multiplier to R6	; 22-25
	vmulpd	zmm13, zmm13, [srcreg+d4+d1+64]	;; apply the fudged group multiplier to I6	; 23-26
	vmulpd	zmm6, zmm6, [srcreg+d4+d2]	;; apply the fudged group multiplier to R7	; 23-26
	vmulpd	zmm14, zmm14, [srcreg+d4+d2+64]	;; apply the fudged group multiplier to I7	; 24-27
	vmulpd	zmm7, zmm7, [srcreg+d4+d2+d1]	;; apply the fudged group multiplier to R8	; 24-27
	vmulpd	zmm15, zmm15, [srcreg+d4+d2+d1+64] ;; apply the fudged group multiplier to I8	; 25-28

	bump	maskreg, maskinc
	bump	grpreg, grpinc

;; Apply the complex premultipliers

	vmovapd zmm30, [screg+1*64]		;; premultiplier cosine/sine for R2/I2
	zfmsubpd zmm16, zmm1, zmm30, zmm9	;; A2 = R2 * cosine - I2			; 1-4		n 
	zfmaddpd zmm9, zmm9, zmm30, zmm1	;; B2 = I2 * cosine + R2			; 1-4		n 

	vmovapd zmm30, [screg+3*64]		;; premultiplier cosine/sine for R4/I4
	zfmsubpd zmm1, zmm3, zmm30, zmm11	;; A4 = R4 * cosine - I4			; 2-5		n 
	zfmaddpd zmm11, zmm11, zmm30, zmm3	;; B4 = I4 * cosine + R4			; 2-5		n 

	vmovapd zmm30, [screg+5*64]		;; premultiplier cosine/sine for R6/I6
	zfmsubpd zmm3, zmm5, zmm30, zmm13	;; A6 = R6 * cosine - I6			; 3-6		n 
	zfmaddpd zmm13, zmm13, zmm30, zmm5	;; B6 = I6 * cosine + R6			; 3-6		n 

	vmovapd zmm30, [screg+7*64]		;; premultiplier cosine/sine for R8/I8
	zfmsubpd zmm5, zmm7, zmm30, zmm15	;; A8 = R8 * cosine - I8			; 4-7		n 
	zfmaddpd zmm15, zmm15, zmm30, zmm7	;; B8 = I8 * cosine + R8			; 4-7		n 

	vmovapd zmm30, [screg+0*64]		;; premultiplier cosine/sine for R1/I1
	zfmsubpd zmm7, zmm0, zmm30, zmm8	;; A1 = R1 * cosine - I1			; 5-8		n 
	zfmaddpd zmm8, zmm8, zmm30, zmm0	;; B1 = I1 * cosine + R1			; 5-8		n 

	vmovapd zmm30, [screg+2*64]		;; premultiplier cosine/sine for R3/I3
	zfmsubpd zmm0, zmm2, zmm30, zmm10	;; A3 = R3 * cosine - I3			; 6-9		n 
	zfmaddpd zmm10, zmm10, zmm30, zmm2	;; B3 = I3 * cosine + R3			; 6-9		n 

	vmovapd zmm30, [screg+4*64]		;; premultiplier cosine/sine for R5/I5
	zfmsubpd zmm2, zmm4, zmm30, zmm12	;; A5 = R5 * cosine - I5			; 7-10		n 
	zfmaddpd zmm12, zmm12, zmm30, zmm4	;; B5 = I5 * cosine + R5			; 7-10		n 

	vmovapd zmm30, [screg+6*64]		;; premultiplier cosine/sine for R7/I7
	zfmsubpd zmm4, zmm6, zmm30, zmm14	;; A7 = R7 * cosine - I7			; 8-11		n 
	zfmaddpd zmm14, zmm14, zmm30, zmm6	;; B7 = I7 * cosine + R7			; 8-11		n 

;; Copied from common 8-complex DJB FFT macro

	vsubpd	zmm6, zmm16, zmm3		;; R2-R6					; 1-4		n 9
	vaddpd	zmm16, zmm16, zmm3		;; R2+R6					; 1-4		n 11
	vmovapd zmm30, [screg+8*64+1*128]	;; sine for R3/I3 and R7/I7 (w^2)

	vsubpd	zmm3, zmm1, zmm5		;; R4-R8					; 2-5		n 9
	vaddpd	zmm1, zmm1, zmm5		;; R4+R8					; 2-5		n 11
	vmovapd zmm29, [screg+8*64+0*128]	;; .707*sine for R2/I2 and R8/I8 (w^1)

	vsubpd	zmm5, zmm11, zmm15		;; I4-I8					; 3-6		n 10
	vaddpd	zmm11, zmm11, zmm15		;; I4+I8					; 3-6		n 12
	vmovapd zmm27, [screg+8*64+2*128]	;; .707*sine for R4/I4 and R6/I6 (w^3)

	vsubpd	zmm15, zmm9, zmm13		;; I2-I6					; 4-7		n 10
	vaddpd	zmm9, zmm9, zmm13		;; I2+I6					; 4-7		n 12
	vmovapd zmm26, [screg+8*64+1*128+64]	;; cosine/sine for R3/I3 and R7/I7 (w^2)

	vaddpd	zmm13, zmm7, zmm2		;; R1+R5					; 5-8		n 13
	vsubpd	zmm7, zmm7, zmm2		;; R1-R5					; 5-8		n 14
	vmovapd zmm25, [screg+8*64+3*128+64]	;; cosine/sine for R5/I5 (w^4)

	vaddpd	zmm2, zmm0, zmm4		;; R3+R7					; 6-9		n 13
	vsubpd	zmm0, zmm0, zmm4		;; R3-R7					; 6-9		n 17
	vmovapd zmm24, [screg+8*64+0*128+64]	;; cosine/sine for R2/I2 and R8/I8 (w^1)

	vaddpd	zmm4, zmm8, zmm12		;; I1+I5					; 7-10		n 16
	vsubpd	zmm8, zmm8, zmm12		;; I1-I5					; 7-10		n 15
	vmovapd zmm23, [screg+8*64+2*128+64]	;; cosine/sine for R4/I4 and R6/I6 (w^3)

	vaddpd	zmm12, zmm10, zmm14		;; I3+I7					; 8-11		n 16
	vsubpd	zmm10, zmm10, zmm14		;; I3-I7					; 8-11		n 18
	vmovapd zmm22, [screg+8*64+3*128]	;; sine for R5/I5 (w^4)

	vaddpd	zmm14, zmm6, zmm3		;; r2-+ = (r2-r6) + (r4-r8)			; 9-12		n 17
	vsubpd	zmm6, zmm6, zmm3		;; r2-- = (r2-r6) - (r4-r8)			; 9-12		n 14
	bump	screg, scinc
	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	zmm3, zmm15, zmm5		;; i2-- = (i2-i6) - (i4-i8)			; 10-13		n 15
	vaddpd	zmm15, zmm15, zmm5		;; i2-+ = (i2-i6) + (i4-i8)			; 10-13		n 18
	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	zmm5, zmm16, zmm1		;; r2+- = (r2+r6) - (r4+r8)			; 11-14		n 19
	vaddpd	zmm16, zmm16, zmm1		;; r2++ = (r2+r6) + (r4+r8)			; 11-14		n 22
	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	zmm1, zmm9, zmm11		;; i2+- = (i2+i6) - (i4+i8)			; 12-15		n 19
	vaddpd	zmm9, zmm9, zmm11		;; i2++ = (i2+i6) + (i4+i8)			; 12-15		n 23
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vaddpd	zmm11, zmm13, zmm2		;; r1++ = (r1+r5) + (r3+r7)			; 13-16		n 22
	vsubpd	zmm13, zmm13, zmm2		;; r1+- = (r1+r5) - (r3+r7)			; 13-16		n 24
	L1prefetchw srcreg+d4+L1pd, L1pt

	zfmaddpd zmm2, zmm7, zmm31, zmm6	;; r1-+ = SQRT2*(r1-r5) + (r2--)		; 14-17		n 20
	zfmsubpd zmm6, zmm7, zmm31, zmm6	;; r1-- = SQRT2*(r1-r5) - (r2--)		; 14-17		n 21
	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	zfmaddpd zmm7, zmm8, zmm31, zmm3	;; i1-+ = SQRT2*(i1-i5) + (i2--)		; 15-18		n 20
	zfmsubpd zmm3, zmm8, zmm31, zmm3	;; i1-- = SQRT2*(i1-i5) - (i2--)		; 15-18		n 21
	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	vaddpd	zmm8, zmm4, zmm12		;; i1++ = (i1+i5) + (i3+i7)			; 16-19		n 23
	vsubpd	zmm4, zmm4, zmm12		;; i1+- = (i1+i5) - (i3+i7)			; 16-19		n 24
	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	zfmaddpd zmm12, zmm0, zmm31, zmm14	;; r3-+ = SQRT2*(r3-r7) + (r2-+)		; 17-20		n 26
	zfmsubpd zmm14, zmm0, zmm31, zmm14	;; r3-- = SQRT2*(r3-r7) - (r2-+)		; 17-20		n 30
	L1prefetchw srcreg+64+L1pd, L1pt

	zfmaddpd zmm0, zmm10, zmm31, zmm15	;; i3-+ = SQRT2*(i3-i7) + (i2-+)		; 18-21		n 26
	zfmsubpd zmm15, zmm10, zmm31, zmm15	;; i3-- = SQRT2*(i3-i7) - (i2-+)		; 18-21		n 30
	L1prefetchw srcreg+d1+64+L1pd, L1pt

	vmulpd	zmm5, zmm5, zmm30		;; r2+-s = r2+- * sine37			; 19-22		n 24
	vmulpd	zmm1, zmm1, zmm30		;; i2+-s = i2+- * sine37			; 19-22		n 24
	L1prefetchw srcreg+d2+64+L1pd, L1pt

	vmulpd	zmm2, zmm2, zmm29		;; r1-+s = r1-+ * .707*sine28			; 20-23		n 26
	vmulpd	zmm7, zmm7, zmm29		;; i1-+s = i1-+ * .707*sine28			; 20-23		n 26
	L1prefetchw srcreg+d2+d1+64+L1pd, L1pt

	vmulpd	zmm6, zmm6, zmm27		;; r1--s = r1-- * .707*sine46			; 21-24		n 30
	vmulpd	zmm3, zmm3, zmm27		;; i1--s = i1-- * .707*sine46			; 21-24		n 30
	L1prefetchw srcreg+d4+64+L1pd, L1pt

	vsubpd	zmm10, zmm11, zmm16		;; R5 = (r1++) - (r2++)				; 22-25		n 32
	vaddpd	zmm11, zmm11, zmm16		;; R1 = (r1++) + (r2++)				; 22-25
	L1prefetchw srcreg+d4+d1+64+L1pd, L1pt

	vsubpd	zmm16, zmm8, zmm9		;; I5 = (i1++) - (i2++)				; 23-26		n 32
	vaddpd	zmm8, zmm8, zmm9		;; I1 = (i1++) + (i2++)				; 23-26
	L1prefetchw srcreg+d4+d2+64+L1pd, L1pt

	zfmsubpd zmm9, zmm13, zmm30, zmm1	;; R3s = (r1+-)*sine37 - (i2+-s)		; 24-27		n 28
	zfmaddpd zmm17, zmm4, zmm30, zmm5	;; I3s = (i1+-)*sine37 + (r2+-s)		; 24-27		n 28
	L1prefetchw srcreg+d4+d2+d1+64+L1pd, L1pt

	zfmaddpd zmm13, zmm13, zmm30, zmm1	;; R7s = (r1+-)*sine37 + (i2+-s)		; 25-28		n 32
	zfmsubpd zmm4, zmm4, zmm30, zmm5	;; I7s = (i1+-)*sine37 - (r2+-s)		; 25-28		n 32

	zfnmaddpd zmm1, zmm0, zmm29, zmm2	;; R2s = (r1-+s) - .707*sine28*(i3-+)		; 26-29		n 33
	zfmaddpd zmm5, zmm12, zmm29, zmm7	;; I2s = (i1-+s) + .707*sine28*(r3-+)		; 26-29		n 33
	zstore	[srcreg], zmm11			;; Store R1					; 26

	zfmaddpd zmm0, zmm0, zmm29, zmm2	;; R8s = (r1-+s) + .707*sine28*(i3-+)		; 27-30		n 34
	zfnmaddpd zmm12, zmm12, zmm29, zmm7	;; I8s = (i1-+s) - .707*sine28*(r3-+)		; 27-30		n 34
	zstore	[srcreg+64], zmm8		;; Store I1					; 27

	zfmsubpd zmm2, zmm9, zmm26, zmm17	;; R3s * cosine/sine - I3s (final R3)		; 28-31
	zfmaddpd zmm17, zmm17, zmm26, zmm9	;; I3s * cosine/sine + R3s (final I3)		; 28-31

	zfmaddpd zmm7, zmm13, zmm26, zmm4	;; R7s * cosine/sine + I7s (final R7)		; 29-32
	zfmsubpd zmm4, zmm4, zmm26, zmm13	;; I7s * cosine/sine - R7s (final I7)		; 29-32

	zfmaddpd zmm9, zmm15, zmm27, zmm6	;; R4s = (r1--s) + .707*sine46*(i3--)		; 30-33		n 35
	zfnmaddpd zmm13, zmm14, zmm27, zmm3	;; I4s = (i1--s) - .707*sine46*(r3--)		; 30-33		n 35

	zfnmaddpd zmm15, zmm15, zmm27, zmm6	;; R6s = (r1--s) - .707*sine46*(i3--)		; 31-34		n 36
	zfmaddpd zmm14, zmm14, zmm27, zmm3	;; I6s = (i1--s) + .707*sine46*(r3--)		; 31-34		n 36

	zfmsubpd zmm6, zmm10, zmm25, zmm16	;; A5 = R5 * cosine/sine - I5			; 32-35		n 37
	zfmaddpd zmm16, zmm16, zmm25, zmm10	;; B5 = I5 * cosine/sine + R5			; 32-35		n 37
	zstore	[srcreg+d2], zmm2		;; Store R3					; 32

	zfmsubpd zmm3, zmm1, zmm24, zmm5	;; R2s * cosine/sine - I2s (final R2)		; 33-36
	zfmaddpd zmm5, zmm5, zmm24, zmm1	;; I2s * cosine/sine + R2s (final I2)		; 33-36
	zstore	[srcreg+d2+64], zmm17		;; Store I3					; 32+1

	zfmaddpd zmm10, zmm0, zmm24, zmm12	;; R8s * cosine/sine + I8s (final R8)		; 34-37
	zfmsubpd zmm12, zmm12, zmm24, zmm0	;; I8s * cosine/sine - R8s (final I8)		; 34-37
	zstore	[srcreg+d4+d2], zmm7		;; Store R7					; 33+1

	zfmsubpd zmm1, zmm9, zmm23, zmm13	;; R4s * cosine/sine - I4s (final R4)		; 35-38
	zfmaddpd zmm13, zmm13, zmm23, zmm9	;; I4s * cosine/sine + R4s (final I4)		; 35-38
	zstore	[srcreg+d4+d2+64], zmm4		;; Store I7					; 33+2

	zfmaddpd zmm0, zmm15, zmm23, zmm14	;; R6s * cosine/sine + I6s (final R6)		; 36-39
	zfmsubpd zmm14, zmm14, zmm23, zmm15	;; I6s * cosine/sine - R6s (final I6)		; 36-39

	vmulpd	zmm6, zmm6, zmm22		;; A5 = A5 * sine (final R5)			; 37-40
	vmulpd	zmm16, zmm16, zmm22		;; B5 = B5 * sine (final I5)			; 37-40

	zstore	[srcreg+d1], zmm3		;; Store R2					; 37
	zstore	[srcreg+d1+64], zmm5		;; Store I2					; 37+1
	zstore	[srcreg+d4+d2+d1], zmm10	;; Store R8					; 38+1
	zstore	[srcreg+d4+d2+d1+64], zmm12	;; Store I8					; 38+2
	zstore	[srcreg+d2+d1], zmm1		;; Store R4					; 39+2
	zstore	[srcreg+d2+d1+64], zmm13	;; Store I4					; 39+3
	zstore	[srcreg+d4+d1], zmm0		;; Store R6					; 40+3
	zstore	[srcreg+d4+d1+64], zmm14	;; Store I6					; 40+4
	zstore	[srcreg+d4], zmm6		;; Store R5					; 41+4
	zstore	[srcreg+d4+64], zmm16		;; Store I5					; 41+5
	bump	srcreg, srcinc
	ENDM


zr8_csc_wpn_eight_complex_last_djbunfft_preload MACRO
	vbroadcastsd zmm31, ZMM_SQRT2
	vbroadcastsd zmm28, ZMM_B
	ENDM
zr8_csc_wpn_eight_complex_last_djbunfft MACRO srcreg,srcinc,d1,d2,d4,screg,scinc,maskreg,maskinc,maxrpt,L1pt,L1pd
	vmovapd zmm30, [screg+8*64+0*128+64]	;; cosine/sine for R2/I2 and R8/I8
	vmovapd zmm4, [srcreg+d1]		;; Load R2
	vmovapd zmm12, [srcreg+d1+64]		;; Load I2
	zfmaddpd zmm16, zmm4, zmm30, zmm12	;; A2 = R2 * cosine/sine + I2			; 1-4		n 5
	zfmsubpd zmm12, zmm12, zmm30, zmm4	;; B2 = I2 * cosine/sine - R2			; 1-4		n 5

	vmovapd zmm7, [srcreg+d4+d2+d1]		;; Load R8
	vmovapd zmm15, [srcreg+d4+d2+d1+64]	;; Load I8
	zfmsubpd zmm4, zmm7, zmm30, zmm15	;; A8 = R8 * cosine/sine - I8			; 2-5		n 11
	zfmaddpd zmm15, zmm15, zmm30, zmm7	;; B8 = I8 * cosine/sine + R8			; 2-5		n 12

	vmovapd zmm30, [screg+8*64+2*128+64]	;; cosine/sine for R4/I4 and R6/I6
	vmovapd zmm6, [srcreg+d2+d1]		;; Load R4
	vmovapd zmm14, [srcreg+d2+d1+64]	;; Load I4
	zfmaddpd zmm7, zmm6, zmm30, zmm14	;; A4 = R4 * cosine/sine + I4 (first R4/sine)	; 3-6		n 9
	zfmsubpd zmm14, zmm14, zmm30, zmm6	;; B4 = I4 * cosine/sine - R4 (first I4/sine)	; 3-6		n 10

	vmovapd zmm5, [srcreg+d4+d1]		;; Load R6
	vmovapd zmm13, [srcreg+d4+d1+64]	;; Load I6
	zfmsubpd zmm6, zmm5, zmm30, zmm13	;; A6 = R6 * cosine/sine - I6 (first R6/sine)	; 4-7		n 9
	zfmaddpd zmm13, zmm13, zmm30, zmm5	;; B6 = I6 * cosine/sine + R6 (first I6/sine)	; 4-7		n 10

	vmovapd zmm29, [screg+8*64+0*128]	;; .707*sine for R2/I2 and R8/I8
	vmulpd	zmm16, zmm16, zmm29		;; A2 = A2 * .707*sine (first R2)		; 5-8		n 11
	vmulpd	zmm12, zmm12, zmm29		;; B2 = B2 * .707*sine (first I2)		; 5-8		n 12

	vmovapd zmm30, [screg+8*64+3*128+64]	;; cosine/sine for R5/I5
	vmovapd zmm1, [srcreg+d4]		;; Load R5
	vmovapd zmm9, [srcreg+d4+64]		;; Load I5
	zfmaddpd zmm5, zmm1, zmm30, zmm9	;; A5 = R5 * cosine/sine + I5 (first R5/sine)	; 6-9		n 13
	zfmsubpd zmm9, zmm9, zmm30, zmm1	;; B5 = I5 * cosine/sine - R5 (first I5/sine)	; 6-9		n 14

	vmovapd zmm30, [screg+8*64+1*128+64]	;; cosine/sine for R3/I3 and R7/I7
	vmovapd zmm2, [srcreg+d2]		;; Load R3
	vmovapd zmm10, [srcreg+d2+64]		;; Load I3
	zfmaddpd zmm1, zmm2, zmm30, zmm10	;; A3 = R3 * cosine/sine + I3 (first R3/sine)	; 7-10		n 15
	zfmsubpd zmm10, zmm10, zmm30, zmm2	;; B3 = I3 * cosine/sine - R3 (first I3/sine)	; 7-10		n 16

	vmovapd zmm3, [srcreg+d4+d2]		;; Load R7
	vmovapd zmm11, [srcreg+d4+d2+64]	;; Load I7
	zfmsubpd zmm2, zmm3, zmm30, zmm11	;; A7 = R7 * cosine/sine - I7 (first R7/sine)	; 8-11		n 15
	zfmaddpd zmm11, zmm11, zmm30, zmm3	;; B7 = I7 * cosine/sine + R7 (first I7/sine)	; 8-11		n 16

	vmovapd zmm0, [srcreg]			;; Load R1
	vaddpd	zmm3, zmm7, zmm6		;; R4/sine + R6/sine				; 9-12		n 17
	vsubpd	zmm7, zmm7, zmm6		;; R4/sine - R6/sine				; 9-12		n 18

	vmovapd zmm25, [screg+8*64+3*128]	;; sine for R5/I5
	vaddpd	zmm6, zmm14, zmm13		;; I4/sine + I6/sine				; 10-13		n 22
	vsubpd	zmm14, zmm14, zmm13		;; I4/sine - I6/sine				; 10-13		n 20

	mov	bl, [maskreg+0*1]		;; Load index into compressed fudges
	zfmaddpd zmm13, zmm4, zmm29, zmm16	;; R2 + R8 * .707*sine				; 11-14		n 17
	zfnmaddpd zmm4, zmm4, zmm29, zmm16	;; R2 - R8 * .707*sine				; 11-14		n 18

	vmovapd zmm8, [srcreg+64]		;; Load I1
	zfmaddpd zmm16, zmm15, zmm29, zmm12	;; I2 + I8 * .707*sine				; 12-15		n 22
	zfnmaddpd zmm15, zmm15, zmm29, zmm12	;; I2 - I8 * .707*sine				; 12-15		n 20

	vmovapd zmm26, [screg+8*64+1*128]	;; sine for R3/I3 and R7/I7
	zfmaddpd zmm12, zmm5, zmm25, zmm0	;; R1 + R5 * sine				; 13-16		n 23
	zfnmaddpd zmm5, zmm5, zmm25, zmm0	;; R1 - R5 * sine				; 13-16		n 26

	vmovapd zmm27, [screg+8*64+2*128]	;; sine for R4/I4 and R6/I6
	zfmaddpd zmm0, zmm9, zmm25, zmm8	;; I1 + I5 * sine				; 14-17		n 24
	zfnmaddpd zmm9, zmm9, zmm25, zmm8	;; I1 - I5 * sine				; 14-17		n 27

	vaddpd	zmm8, zmm1, zmm2		;; R3/sine + R7/sine				; 15-18		n 23
	vsubpd	zmm1, zmm1, zmm2		;; R3/sine - R7/sine				; 15-18		n 27
	mov	r14, [r13+0*8]			;; Load the xor mask

	vaddpd	zmm2, zmm10, zmm11		;; I3/sine + I7/sine				; 16-19		n 24
	vsubpd	zmm10, zmm10, zmm11		;; I3/sine - I7/sine				; 16-19		n 26
	xor	r14, [r12+rbx*8]		;; Apply the xor mask to 64 bits of fudge flags

	zfmaddpd zmm11, zmm3, zmm27, zmm13	;; r2++ = (r2+r8) + (r4+r6) * .707*sine		; 17-20		n 30
	zfnmaddpd zmm3, zmm3, zmm27, zmm13	;; r2+- = (r2+r8) - (r4+r6) * .707*sine		; 17-20		n 28
	mov	bl, [maskreg+1*1]		;; Load index into compressed fudges
	bump	maskreg, maskinc

	kmovw	k1, r14d			;; Load R1 and I1 fudge factor mask		; 18		n 46
	zfmaddpd zmm13, zmm7, zmm27, zmm4	;; r2-+ = (r2-r8) + (r4-r6) * .707*sine		; 18-21		n 29
	shr	r14, 16				;; Next 16 bits of fudge flags

	kmovw	k2, r14d			;; Load R2 and I2 fudge factor mask		; 19		n 52
	zfnmaddpd zmm7, zmm7, zmm27, zmm4	;; r2-- = (r2-r8) - (r4-r6) * .707*sine		; 19-22		n 33
	shr	r14, 16				;; Next 16 bits of fudge flags

	kmovw	k3, r14d			;; Load R3 and I3 fudge factor mask		; 20		n 47
	zfmaddpd zmm4, zmm14, zmm27, zmm15	;; i2-+ = (i2-i8) + (i4-i6) * .707*sine		; 20-23		n 28
	shr	r14, 16				;; Next 16 bits of fudge flags

	kmovw	k4, r14d			;; Load R4 and I4 fudge factor mask		; 21		n 54
	zfnmaddpd zmm14, zmm14, zmm27, zmm15	;; i2-- = (i2-i8) - (i4-i6) * .707*sine		; 21-24		n 32
	mov	r14, [r13+1*8]			;; Load the xor mask

	zfmaddpd zmm15, zmm6, zmm27, zmm16	;; i2++ = (i2+i8) + (i4+i6) * .707*sine		; 22-25		n 31
	zfnmaddpd zmm6, zmm6, zmm27, zmm16	;; i2+- = (i2+i8) - (i4+i6) * .707*sine		; 22-25		n 29
	xor	r14, [r12+rbx*8]		;; Apply the xor mask to 64 bits of fudge flags

	zfmaddpd zmm16, zmm8, zmm26, zmm12	;; r1++ = (r1+r5) + (r3+r7) * sine		; 23-26		n 30
	zfnmaddpd zmm8, zmm8, zmm26, zmm12	;; r1+- = (r1+r5) - (r3+r7) * sine		; 23-26		n 32
	vmovapd zmm20, [screg+0*64]		;; premultiplier cosine/sine for R1/I1

	kmovw	k5, r14d			;; Load R5 and I5 fudge factor mask		; 24		n 49
	zfmaddpd zmm12, zmm2, zmm26, zmm0	;; i1++ = (i1+i5) + (i3+i7) * sine		; 24-27		n 31
	shr	r14, 16				;; Next 16 bits of fudge flags
	vmovapd zmm19, [screg+2*64]		;; premultiplier cosine/sine for R3/I3

	kmovw	k6, r14d			;; Load R6 and I6 fudge factor mask		; 25		n 55
	zfnmaddpd zmm2, zmm2, zmm26, zmm0	;; i1+- = (i1+i5) - (i3+i7) * sine		; 25-28		n 33
	shr	r14, 16				;; Next 16 bits of fudge flags
	vmovapd zmm18, [screg+4*64]		;; premultiplier cosine/sine for R5/I5

	kmovw	k7, r14d			;; Load R7 and I7 fudge factor mask		; 26		n 51
	zfmaddpd zmm0, zmm10, zmm26, zmm5	;; r1-+ = (r1-r5) + (i3-i7) * sine		; 26-29		n 34
	shr	r14, 16				;; Next 16 bits of fudge flags
	vmovapd zmm25, [screg+6*64]		;; premultiplier cosine/sine for R7/I7

	zfnmaddpd zmm10, zmm10, zmm26, zmm5	;; r1-- = (r1-r5) - (i3-i7) * sine		; 27-30		n 36
	zfmaddpd zmm5, zmm1, zmm26, zmm9	;; i1-+ = (i1-i5) + (r3-r7) * sine		; 27-30		n 37
	vmovapd zmm24, [screg+1*64]		;; premultiplier cosine/sine for R2/I2

	zfnmaddpd zmm1, zmm1, zmm26, zmm9	;; i1-- = (i1-i5) - (r3-r7) * sine		; 28-31		n 35
	vaddpd	zmm9, zmm3, zmm4		;; r2+-+ = (r2+-) + (i2-+)			; 28-31		n 34
	vmovapd zmm23, [screg+3*64]		;; premultiplier cosine/sine for R4/I4

	vsubpd	zmm3, zmm3, zmm4		;; r2+-- = (r2+-) - (i2-+)			; 29-32		n 36
	vaddpd	zmm4, zmm13, zmm6		;; r2-++ = (r2-+) + (i2+-)			; 29-32		n 37
	vmovapd zmm22, [screg+5*64]		;; premultiplier cosine/sine for R6/I6

	vsubpd	zmm13, zmm13, zmm6		;; r2-+- = (r2-+) - (i2+-)			; 30-33		n 35
	zfmaddpd zmm6, zmm11, zmm31, zmm16	;; R1 = (r1++) + SQRT2*(r2++)			; 30-33		n 38
	vmovapd zmm21, [screg+7*64]		;; premultiplier cosine/sine for R8/I8

	zfnmaddpd zmm16, zmm11, zmm31, zmm16	;; R5 = (r1++) - SQRT2*(r2++)			; 31-34		n 40
	zfmaddpd zmm11, zmm15, zmm31, zmm12	;; I1 = (i1++) + SQRT2*(i2++)			; 31-34		n 38
	bump	screg, scinc
	L1prefetch srcreg+L1pd, L1pt

	zfnmaddpd zmm12, zmm15, zmm31, zmm12	;; I5 = (i1++) - SQRT2*(i2++)			; 32-35		n 40
	zfmaddpd zmm15, zmm14, zmm31, zmm8	;; R3 = (r1+-) + SQRT2*(i2--)			; 32-35		n 39
	L1prefetch srcreg+64+L1pd, L1pt

	zfnmaddpd zmm8, zmm14, zmm31, zmm8	;; R7 = (r1+-) - SQRT2*(i2--)			; 33-36		n 41
	zfnmaddpd zmm14, zmm7, zmm31, zmm2	;; I3 = (i1+-) - SQRT2*(r2--)			; 33-36		n 39
	L1prefetch srcreg+d1+L1pd, L1pt

	zfmaddpd zmm2, zmm7, zmm31, zmm2	;; I7 = (i1+-) + SQRT2*(r2--)			; 34-37		n 41
	vaddpd	zmm7, zmm0, zmm9		;; R2 = (r1-+) + (r2+-+)			; 34-37		n 42
	L1prefetch srcreg+d1+64+L1pd, L1pt

	vsubpd	zmm9, zmm0, zmm9		;; R6 = (r1-+) - (r2+-+)			; 35-38		n 44
	vsubpd	zmm0, zmm1, zmm13		;; I2 = (i1--) - (r2-+-)			; 35-38		n 42
	L1prefetch srcreg+d2+L1pd, L1pt

	vaddpd	zmm13, zmm1, zmm13		;; I6 = (i1--) + (r2-+-)			; 36-39		n 44
	vsubpd	zmm1, zmm10, zmm3		;; R4 = (r1--) - (r2+--)			; 36-39		n 43
	L1prefetch srcreg+d2+64+L1pd, L1pt

	vaddpd	zmm3, zmm10, zmm3		;; R8 = (r1--) + (r2+--)			; 37-40		n 45
	vsubpd	zmm10, zmm5, zmm4		;; I4 = (i1-+) - (r2-++)			; 37-40		n 43
	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vaddpd	zmm4, zmm5, zmm4		;; I8 = (i1-+) + (r2-++)			; 38-41		n 45
	zfmaddpd zmm5, zmm6, zmm20, zmm11	;; A1 = R1 * cosine + I1			; 38-41		n 46
	L1prefetch srcreg+d2+d1+64+L1pd, L1pt

	zfmsubpd zmm11, zmm11, zmm20, zmm6	;; B1 = I1 * cosine - R1			; 39-42		n 48
	zfmaddpd zmm6, zmm15, zmm19, zmm14	;; A3 = R3 * cosine + I3			; 39-42		n 47
	L1prefetch srcreg+d4+L1pd, L1pt

	zfmsubpd zmm14, zmm14, zmm19, zmm15	;; B3 = I3 * cosine - R3			; 40-43		n 50
	zfmaddpd zmm15, zmm16, zmm18, zmm12	;; A5 = R5 * cosine + I5			; 40-43		n 49
	L1prefetch srcreg+d4+64+L1pd, L1pt

	zfmsubpd zmm12, zmm12, zmm18, zmm16	;; B5 = I5 * cosine - R5			; 41-44		n 51
	zfmaddpd zmm16, zmm8, zmm25, zmm2	;; A7 = R7 * cosine + I7			; 41-44		n 51
	L1prefetch srcreg+d4+d1+L1pd, L1pt

	zfmsubpd zmm2, zmm2, zmm25, zmm8	;; B7 = I7 * cosine - R7			; 42-45		n 53
	zfmaddpd zmm8, zmm7, zmm24, zmm0	;; A2 = R2 * cosine + I2			; 42-45		n 52
	L1prefetch srcreg+d4+d1+64+L1pd, L1pt

	zfmsubpd zmm0, zmm0, zmm24, zmm7	;; B2 = I2 * cosine - R2			; 43-46		n 54
	zfmaddpd zmm7, zmm1, zmm23, zmm10	;; A4 = R4 * cosine + I4			; 43-46		n 54
	L1prefetch srcreg+d4+d2+L1pd, L1pt

	zfmsubpd zmm10, zmm10, zmm23, zmm1	;; B4 = I4 * cosine - R4			; 44-47		n 57
	zfmaddpd zmm1, zmm9, zmm22, zmm13	;; A6 = R6 * cosine + I6			; 44-47		n 55
	L1prefetch srcreg+d4+d2+64+L1pd, L1pt

	zfmsubpd zmm13, zmm13, zmm22, zmm9	;; B6 = I6 * cosine - R6			; 45-48		n 58
	zfmaddpd zmm9, zmm3, zmm21, zmm4	;; A8 = R8 * cosine + I8			; 45-48		n 56
	L1prefetch srcreg+d4+d2+d1+L1pd, L1pt

	zfmsubpd zmm4, zmm4, zmm21, zmm3	;; B8 = I8 * cosine - R8			; 46-49		n 58
	vmulpd	zmm5{k1}, zmm5, zmm28		;; apply fudge multiplier for R1		; 46-49
	L1prefetch srcreg+d4+d2+d1+64+L1pd, L1pt

	kshiftrw k1, k1, 8			;; I1's fudge					; 47		n 48
	vmulpd	zmm6{k3}, zmm6, zmm28		;; apply fudge multiplier for R3		; 47-50

	kshiftrw k3, k3, 8			;; I3's fudge					; 48		n 50
	vmulpd	zmm11{k1}, zmm11, zmm28		;; apply fudge multiplier for I1		; 48-51

	kmovw	k1, r14d			;; Load R8 and I8 fudge factor mask		; 49		n 56
	vmulpd	zmm15{k5}, zmm15, zmm28		;; apply fudge multiplier for R5		; 49-52

	kshiftrw k5, k5, 8			;; I5's fudge					; 50		n 51
	vmulpd	zmm14{k3}, zmm14, zmm28		;; apply fudge multiplier for I3		; 50-53
	zstore	[srcreg], zmm5			;; Save R1					; 50

	vmulpd	zmm16{k7}, zmm16, zmm28		;; apply fudge multiplier for R7		; 51-54
	vmulpd	zmm12{k5}, zmm12, zmm28		;; apply fudge multiplier for I5		; 51-54
	zstore	[srcreg+d2], zmm6		;; Save R3					; 51

	kshiftrw k7, k7, 8			;; I7's fudge					; 52		n 53
	vmulpd	zmm8{k2}, zmm8, zmm28		;; apply fudge multiplier for R2		; 52-55
	zstore	[srcreg+64], zmm11		;; Save I1					; 52

	kshiftrw k2, k2, 8			;; I2's fudge					; 53		n 54
	vmulpd	zmm2{k7}, zmm2, zmm28		;; apply fudge multiplier for I7		; 53-56
	zstore	[srcreg+d4], zmm15		;; Save R5					; 53

	vmulpd	zmm0{k2}, zmm0, zmm28		;; apply fudge multiplier for I2		; 54-57
	vmulpd	zmm7{k4}, zmm7, zmm28		;; apply fudge multiplier for R4		; 54-57
	zstore	[srcreg+d2+64], zmm14		;; Save I3					; 54

	kshiftrw k4, k4, 8			;; I4's fudge					; 55		n 57
	vmulpd	zmm1{k6}, zmm1, zmm28		;; apply fudge multiplier for R6		; 55-58
	zstore	[srcreg+d4+d2], zmm16		;; Save R7					; 55

	kshiftrw k6, k6, 8			;; I6's fudge					; 56		n 58
	vmulpd	zmm9{k1}, zmm9, zmm28		;; apply fudge multiplier for R8		; 56-59
	zstore	[srcreg+d4+64], zmm12		;; Save I5					; 55+1

	kshiftrw k1, k1, 8			;; I8's fudge					; 57		n 58
	vmulpd	zmm10{k4}, zmm10, zmm28		;; apply fudge multiplier for I4		; 57-60
	zstore	[srcreg+d1], zmm8		;; Save R2					; 56+1

	vmulpd	zmm13{k6}, zmm13, zmm28		;; apply fudge multiplier for I6		; 58-61
	vmulpd	zmm4{k1}, zmm4, zmm28		;; apply fudge multiplier for I8		; 58-61
	zstore	[srcreg+d4+d2+64], zmm2		;; Save I7					; 57+1

	zstore	[srcreg+d1+64], zmm0		;; Save I2					; 58+1
	zstore	[srcreg+d2+d1], zmm7		;; Save R4					; 58+2
	zstore	[srcreg+d4+d1], zmm1		;; Save R6					; 59+2
	zstore	[srcreg+d4+d2+d1], zmm9		;; Save R8					; 60+2
	zstore	[srcreg+d2+d1+64], zmm10	;; Save I4					; 61+2
	zstore	[srcreg+d4+d1+64], zmm13	;; Save I6					; 62+2
	zstore	[srcreg+d4+d2+d1+64], zmm4	;; Save I8					; 62+3
	bump	srcreg, srcinc
	ENDM
ENDIF

