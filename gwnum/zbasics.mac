; Copyright 2011-2021 - Mersenne Research, Inc.  All rights reserved
; Author:  George Woltman
; Email: woltman@alum.mit.edu
;
; These macros implement basic AVX-512 building blocks that will be used by all FFT types.
;

;;
;; Store a 512-bit zmm register in memory.  In older (AMD?) architectures there was a difference
;; between using vmovaps and vmovapd.  We continue to use a store macro for those historical reasons. 
;;

zstore MACRO address, reg
	vmovapd address, reg
	ENDM

;; Macros that try to make it easier to write code that works best on both Bulldozer 
;; which supports the superior FMA4 instruction and Intel which only supports the
;; more clumsy FMA3 instructions.

zfmaddpd MACRO dest, mulval1, mulval2, addval
	LOCAL	destreglen
	destreglen = @SIZESTR(<&dest>)
	IF @INSTR(,@substr(<&dest>, 1, destreglen),<{>) NE 0
		destreglen = @INSTR(,@substr(<&dest>, 1, destreglen),<{>) - 1
	ENDIF
	IF @INSTR(,@substr(<&dest>, 1, destreglen),< >) NE 0
		destreglen = @INSTR(,@substr(<&dest>, 1, destreglen),< >) - 1
	ENDIF
	IFIDNI @substr(<&dest>, 1, destreglen), <&addval>
		IF (OPATTR (mulval1)) AND 10000b
			vfmadd231pd dest, mulval1, mulval2
		ELSE
			vfmadd231pd dest, mulval2, mulval1
		ENDIF
	ELSE
	IFIDNI @substr(<&dest>, 1, destreglen), <&mulval1>
		IF (OPATTR (mulval2)) AND 10000b
			vfmadd213pd dest, mulval2, addval
		ELSE
			vfmadd132pd dest, addval, mulval2
		ENDIF
	ELSE
	IFIDNI @substr(<&dest>, 1, destreglen), <&mulval2>
		IF (OPATTR (mulval1)) AND 10000b
			vfmadd213pd dest, mulval1, addval
		ELSE
			vfmadd132pd dest, addval, mulval1
		ENDIF
	ELSE
	IF (OPATTR (mulval2)) AND 10000b
		vmovapd dest, mulval2
		IF (OPATTR (mulval1)) AND 10000b
			vfmadd213pd dest, mulval1, addval
		ELSE
			vfmadd132pd dest, addval, mulval1
		ENDIF
	ELSE
		vmovapd dest, mulval1
		vfmadd132pd dest, addval, mulval2
	ENDIF
	ENDIF
	ENDIF
	ENDIF
	ENDM

zfmsubpd MACRO dest, mulval1, mulval2, addval
	LOCAL	destreglen
	destreglen = @SIZESTR(<&dest>)
	IF @INSTR(,@substr(<&dest>, 1, destreglen),<{>) NE 0
		destreglen = @INSTR(,@substr(<&dest>, 1, destreglen),<{>) - 1
	ENDIF
	IF @INSTR(,@substr(<&dest>, 1, destreglen),< >) NE 0
		destreglen = @INSTR(,@substr(<&dest>, 1, destreglen),< >) - 1
	ENDIF
	IFIDNI @substr(<&dest>, 1, destreglen), <&addval>
		IF (OPATTR (mulval1)) AND 10000b
			vfmsub231pd dest, mulval1, mulval2
		ELSE
			vfmsub231pd dest, mulval2, mulval1
		ENDIF
	ELSE
	IFIDNI @substr(<&dest>, 1, destreglen), <&mulval1>
		IF (OPATTR (mulval2)) AND 10000b
			vfmsub213pd dest, mulval2, addval
		ELSE
			vfmsub132pd dest, addval, mulval2
		ENDIF
	ELSE
	IFIDNI @substr(<&dest>, 1, destreglen), <&mulval2>
		IF (OPATTR (mulval1)) AND 10000b
			vfmsub213pd dest, mulval1, addval
		ELSE
			vfmsub132pd dest, addval, mulval1
		ENDIF
	ELSE
	IF (OPATTR (mulval2)) AND 10000b
		vmovapd dest, mulval2
		IF (OPATTR (mulval1)) AND 10000b
			vfmsub213pd dest, mulval1, addval
		ELSE
			vfmsub132pd dest, addval, mulval1
		ENDIF
	ELSE
		vmovapd dest, mulval1
		vfmsub132pd dest, addval, mulval2
	ENDIF
	ENDIF
	ENDIF
	ENDIF
	ENDM

zfnmaddpd MACRO dest, mulval1, mulval2, addval
	LOCAL	destreglen
	destreglen = @SIZESTR(<&dest>)
	IF @INSTR(,@substr(<&dest>, 1, destreglen),<{>) NE 0
		destreglen = @INSTR(,@substr(<&dest>, 1, destreglen),<{>) - 1
	ENDIF
	IF @INSTR(,@substr(<&dest>, 1, destreglen),< >) NE 0
		destreglen = @INSTR(,@substr(<&dest>, 1, destreglen),< >) - 1
	ENDIF
	IFIDNI @substr(<&dest>, 1, destreglen), <&addval>
		IF (OPATTR (mulval1)) AND 10000b
			vfnmadd231pd dest, mulval1, mulval2
		ELSE
			vfnmadd231pd dest, mulval2, mulval1
		ENDIF
	ELSE
	IFIDNI @substr(<&dest>, 1, destreglen), <&mulval1>
		IF (OPATTR (mulval2)) AND 10000b
			vfnmadd213pd dest, mulval2, addval
		ELSE
			vfnmadd132pd dest, addval, mulval2
		ENDIF
	ELSE
	IFIDNI @substr(<&dest>, 1, destreglen), <&mulval2>
		IF (OPATTR (mulval1)) AND 10000b
			vfnmadd213pd dest, mulval1, addval
		ELSE
			vfnmadd132pd dest, addval, mulval1
		ENDIF
	ELSE
	IF (OPATTR (mulval2)) AND 10000b
		vmovapd dest, mulval2
		IF (OPATTR (mulval1)) AND 10000b
			vfnmadd213pd dest, mulval1, addval
		ELSE
			vfnmadd132pd dest, addval, mulval1
		ENDIF
	ELSE
		vmovapd dest, mulval1
		vfnmadd132pd dest, addval, mulval2
	ENDIF
	ENDIF
	ENDIF
	ENDIF
	ENDM

zfnmsubpd MACRO dest, mulval1, mulval2, addval
	LOCAL	destreglen
	destreglen = @SIZESTR(<&dest>)
	IF @INSTR(,@substr(<&dest>, 1, destreglen),<{>) NE 0
		destreglen = @INSTR(,@substr(<&dest>, 1, destreglen),<{>) - 1
	ENDIF
	IF @INSTR(,@substr(<&dest>, 1, destreglen),< >) NE 0
		destreglen = @INSTR(,@substr(<&dest>, 1, destreglen),< >) - 1
	ENDIF
	IFIDNI @substr(<&dest>, 1, destreglen), <&addval>
		IF (OPATTR (mulval1)) AND 10000b
			vfnmsub231pd dest, mulval1, mulval2
		ELSE
			vfnmsub231pd dest, mulval2, mulval1
		ENDIF
	ELSE
	IFIDNI @substr(<&dest>, 1, destreglen), <&mulval1>
		IF (OPATTR (mulval2)) AND 10000b
			vfnmsub213pd dest, mulval2, addval
		ELSE
			vfnmsub132pd dest, addval, mulval2
		ENDIF
	ELSE
	IFIDNI @substr(<&dest>, 1, destreglen), <&mulval2>
		IF (OPATTR (mulval1)) AND 10000b
			vfnmsub213pd dest, mulval1, addval
		ELSE
			vfnmsub132pd dest, addval, mulval1
		ENDIF
	ELSE
	IF (OPATTR (mulval2)) AND 10000b
		vmovapd dest, mulval2
		IF (OPATTR (mulval1)) AND 10000b
			vfnmsub213pd dest, mulval1, addval
		ELSE
			vfnmsub132pd dest, addval, mulval1
		ENDIF
	ELSE
		vmovapd dest, mulval1
		vfnmsub132pd dest, addval, mulval2
	ENDIF
	ENDIF 
	ENDIF 
	ENDIF
	ENDM


zfmaddsd MACRO dest, mulval1, mulval2, addval
	IFIDNI <&dest>, <&addval>
		IF (OPATTR (mulval1)) AND 10000b
			vfmadd231sd addval, mulval1, mulval2
		ELSE
			vfmadd231sd addval, mulval2, mulval1
		ENDIF
	ELSE
	IFIDNI <&dest>, <&mulval1>
		IF (OPATTR (mulval2)) AND 10000b
			vfmadd213sd mulval1, mulval2, addval
		ELSE
			vfmadd132sd mulval1, addval, mulval2
		ENDIF
	ELSE
	IFIDNI <&dest>, <&mulval2>
		IF (OPATTR (mulval1)) AND 10000b
			vfmadd213sd mulval2, mulval1, addval
		ELSE
			vfmadd132sd mulval2, addval, mulval1
		ENDIF
	ELSE
	IF (OPATTR (mulval2)) AND 10000b
		vmovsd	dest, mulval2, mulval2
		IF (OPATTR (mulval1)) AND 10000b
			vfmadd213sd dest, mulval1, addval
		ELSE
			vfmadd132sd dest, addval, mulval1
		ENDIF
	ELSE
		vmovsd	dest, mulval1, mulval1
		vfmadd132sd dest, addval, mulval2
	ENDIF
	ENDIF
	ENDIF
	ENDIF
	ENDM

zfmsubsd MACRO dest, mulval1, mulval2, addval
	IFIDNI <&dest>, <&addval>
		IF (OPATTR (mulval1)) AND 10000b
			vfmsub231sd addval, mulval1, mulval2
		ELSE
			vfmsub231sd addval, mulval2, mulval1
		ENDIF
	ELSE
	IFIDNI <&dest>, <&mulval1>
		IF (OPATTR (mulval2)) AND 10000b
			vfmsub213sd mulval1, mulval2, addval
		ELSE
			vfmsub132sd mulval1, addval, mulval2
		ENDIF
	ELSE
	IFIDNI <&dest>, <&mulval2>
		IF (OPATTR (mulval1)) AND 10000b
			vfmsub213sd mulval2, mulval1, addval
		ELSE
			vfmsub132sd mulval2, addval, mulval1
		ENDIF
	ELSE
	IF (OPATTR (mulval2)) AND 10000b
		vmovsd	dest, mulval2, mulval2
		IF (OPATTR (mulval1)) AND 10000b
			vfmsub213sd dest, mulval1, addval
		ELSE
			vfmsub132sd dest, addval, mulval1
		ENDIF
	ELSE
		vmovsd	dest, mulval1, mulval1
		vfmsub132sd dest, addval, mulval2
	ENDIF
	ENDIF
	ENDIF
	ENDIF
	ENDM

zfnmaddsd MACRO dest, mulval1, mulval2, addval
	IFIDNI <&dest>, <&addval>
		IF (OPATTR (mulval1)) AND 10000b
			vfnmadd231sd addval, mulval1, mulval2
		ELSE
			vfnmadd231sd addval, mulval2, mulval1
		ENDIF
	ELSE
	IFIDNI <&dest>, <&mulval1>
		IF (OPATTR (mulval2)) AND 10000b
			vfnmadd213sd mulval1, mulval2, addval
		ELSE
			vfnmadd132sd mulval1, addval, mulval2
		ENDIF
	ELSE
	IFIDNI <&dest>, <&mulval2>
		IF (OPATTR (mulval1)) AND 10000b
			vfnmadd213sd mulval2, mulval1, addval
		ELSE
			vfnmadd132sd mulval2, addval, mulval1
		ENDIF
	ELSE
	IF (OPATTR (mulval2)) AND 10000b
		vmovsd	dest, mulval2, mulval2
		IF (OPATTR (mulval1)) AND 10000b
			vfnmadd213sd dest, mulval1, addval
		ELSE
			vfnmadd132sd dest, addval, mulval1
		ENDIF
	ELSE
		vmovsd	dest, mulval1, mulval1
		vfnmadd132sd dest, addval, mulval2
	ENDIF
	ENDIF
	ENDIF
	ENDIF
	ENDM

zfnmsubsd MACRO dest, mulval1, mulval2, addval
	IFIDNI <&dest>, <&addval>
		IF (OPATTR (mulval1)) AND 10000b
			vfnmsub231sd addval, mulval1, mulval2
		ELSE
			vfnmsub231sd addval, mulval2, mulval1
		ENDIF
	ELSE
	IFIDNI <&dest>, <&mulval1>
		IF (OPATTR (mulval2)) AND 10000b
			vfnmsub213sd mulval1, mulval2, addval
		ELSE
			vfnmsub132sd mulval1, addval, mulval2
		ENDIF
	ELSE
	IFIDNI <&dest>, <&mulval2>
		IF (OPATTR (mulval1)) AND 10000b
			vfnmsub213sd mulval2, mulval1, addval
		ELSE
			vfnmsub132sd mulval2, addval, mulval1
		ENDIF
	ELSE
	IF (OPATTR (mulval2)) AND 10000b
		vmovsd	dest, mulval2, mulval2
		IF (OPATTR (mulval1)) AND 10000b
			vfnmsub213sd dest, mulval1, addval
		ELSE
			vfnmsub132sd dest, addval, mulval1
		ENDIF
	ELSE
		vmovsd	dest, mulval1, mulval1
		vfnmsub132sd dest, addval, mulval2
	ENDIF
	ENDIF 
	ENDIF 
	ENDIF
	ENDM

;;
;; Macros to gather load or scatter store 8 registers with an 8x8 transpose
;;

;; For SkylakeX, it is faster to load and shuffle rather than using the gather instruction

zgather_preload MACRO unused
	ENDM
zgather MACRO srcreg,d1,d2,d4,r1,r2,r3,r4,r5,r6,r7,r8, r9
	vmovapd	r4, [srcreg]		;; d0_7	d0_6 d0_5 d0_4 d0_3 d0_2 d0_1 d0_0
	vmovapd	r5, [srcreg+d1]		;; d1_7	d1_6 d1_5 d1_4 d1_3 d1_2 d1_1 d1_0
	vmovapd	r6, [srcreg+d2]		;; d2_7	d2_6 d2_5 d2_4 d2_3 d2_2 d2_1 d2_0
	vmovapd	r9, [srcreg+d2+d1]	;; d3_7	d3_6 d3_5 d3_4 d3_3 d3_2 d3_1 d3_0
	vmovapd	r7, [srcreg+d4]		;; d4_7	d4_6 d4_5 d4_4 d4_3 d4_2 d4_1 d4_0
	vmovapd	r1, [srcreg+d4+d1]	;; d5_7	d5_6 d5_5 d5_4 d5_3 d5_2 d5_1 d5_0
	vmovapd	r2, [srcreg+d4+d2]	;; d6_7	d6_6 d6_5 d6_4 d6_3 d6_2 d6_1 d6_0
	vmovapd	r8, [srcreg+d4+d2+d1]	;; d7_7	d7_6 d7_5 d7_4 d7_3 d7_2 d7_1 d7_0

	vshufpd	r3, r4, r5, 00000000b	;; d1_6 d0_6 d1_4 d0_4 d1_2 d0_2 d1_0 d0_0
	vshufpd	r5, r4, r5, 11111111b	;; d1_7 d0_7 d1_5 d0_5 d1_3 d0_3 d1_1 d0_1
	vshufpd	r4, r6, r9, 00000000b	;; d3_6 d2_6 d3_4 d2_4 d3_2 d2_2 d3_0 d2_0
	vshufpd	r9, r6, r9, 11111111b	;; d3_7 d2_7 d3_5 d2_5 d3_3 d2_3 d3_1 d2_1
	vshufpd	r6, r7, r1, 00000000b	;; d5_6 d4_6 d5_4 d4_4 d5_2 d4_2 d5_0 d4_0
	vshufpd	r1, r7, r1, 11111111b	;; d5_7 d4_7 d5_5 d4_5 d5_3 d4_3 d5_1 d4_1
	vshufpd	r7, r2, r8, 00000000b	;; d7_6 d6_6 d7_4 d6_4 d7_2 d6_2 d7_0 d6_0
	vshufpd	r8, r2, r8, 11111111b	;; d7_7 d6_7 d7_5 d6_5 d7_3 d6_3 d7_1 d6_1

	vshuff64x2 r2, r3, r4, 10001000b ;; d3_4 d2_4 d3_0 d2_0 d1_4 d0_4 d1_0 d0_0
	vshuff64x2 r4, r3, r4, 11011101b ;; d3_6 d2_6 d3_2 d2_2 d1_6 d0_6 d1_2 d0_2
	vshuff64x2 r3, r5, r9, 10001000b ;; d3_5 d2_5 d3_1 d2_1 d1_5 d0_5 d1_1 d0_1
	vshuff64x2 r9, r5, r9, 11011101b ;; d3_7 d2_7 d3_3 d2_3 d1_7 d0_7 d1_3 d0_3
	vshuff64x2 r5, r6, r7, 10001000b ;; d7_4 d6_4 d7_0 d6_0 d5_4 d4_4 d5_0 d4_0
	vshuff64x2 r7, r6, r7, 11011101b ;; d7_6 d6_6 d7_2 d6_2 d5_6 d4_6 d5_2 d4_2
	vshuff64x2 r6, r1, r8, 10001000b ;; d7_5 d6_5 d7_1 d6_1 d5_5 d4_5 d5_1 d4_1
	vshuff64x2 r8, r1, r8, 11011101b ;; d7_7 d6_7 d7_3 d6_3 d5_7 d4_7 d5_3 d4_3

	vshuff64x2 r1, r2, r5, 10001000b ;; d7_0 d6_0 d5_0 d4_0 d3_0 d2_0 d1_0 d0_0
	vshuff64x2 r5, r2, r5, 11011101b ;; d7_4 d6_4 d5_4 d4_4 d3_4 d2_4 d1_4 d0_4
	vshuff64x2 r2, r3, r6, 10001000b ;; d7_1 d6_1 d5_1 d4_1 d3_1 d2_1 d1_1 d0_1
	vshuff64x2 r6, r3, r6, 11011101b ;; d7_5 d6_5 d5_5 d4_5 d3_5 d2_5 d1_5 d0_5
	vshuff64x2 r3, r4, r7, 10001000b ;; d7_2 d6_2 d5_2 d4_2 d3_2 d2_2 d1_2 d0_2
	vshuff64x2 r7, r4, r7, 11011101b ;; d7_6 d6_6 d5_6 d4_6 d3_6 d2_6 d1_6 d0_6
	vshuff64x2 r4, r9, r8, 10001000b ;; d7_3 d6_3 d5_3 d4_3 d3_3 d2_3 d1_3 d0_3
	vshuff64x2 r8, r9, r8, 11011101b ;; d7_7 d6_7 d5_7 d4_7 d3_7 d2_7 d1_7 d0_7
	ENDM

;;; New better swizzle (for Skylake-X)!!!!!  Uses vblendmpd instructions that run on port 0 and 5

zgather_preload MACRO unused
	set	k6 = 00110011b			;; Constant for vblendmpd instructions
						;; Constant for vpermt2pd instructions
						;; 0=src2,1=dest-is-src   xxx = double#
	set	zmm30 = 0+4 8+4 0+6 8+6 0+0 8+0 0+2 8+2
	set	zmm29 = 0+5 8+5 0+7 8+7 0+1 8+1 0+3 8+3
	ENDM
zgather MACRO srcreg,d1,d2,d4,r1,r2,r3,r4,r5,r6,r7,r8, r9
	;; Further possible optimization include using unaligned vmovupd and/or vbroadcastf64 instructions
	vmovapd	zmm0, [srcreg+64]		;; d0_7	d0_6 d0_5 d0_4 d0_3 d0_2 d0_1 d0_0
	vmovapd	zmm1, [srcreg+d1+64]		;; d1_7	d1_6 d1_5 d1_4 d1_3 d1_2 d1_1 d1_0
	vmovapd	zmm2, [srcreg+d2+64]		;; d2_7	d2_6 d2_5 d2_4 d2_3 d2_2 d2_1 d2_0
	vmovapd	zmm3, [srcreg+d2+d1+64]		;; d3_7	d3_6 d3_5 d3_4 d3_3 d3_2 d3_1 d3_0
	vmovapd	zmm4, [srcreg+d4+64]		;; d4_7	d4_6 d4_5 d4_4 d4_3 d4_2 d4_1 d4_0
	vmovapd	zmm5, [srcreg+d4+d1+64]		;; d5_7	d5_6 d5_5 d5_4 d5_3 d5_2 d5_1 d5_0
	vmovapd	zmm6, [srcreg+d4+d2+64]		;; d6_7	d6_6 d6_5 d6_4 d6_3 d6_2 d6_1 d6_0
	vmovapd	zmm7, [srcreg+d4+d2+d1+64]	;; d7_7	d7_6 d7_5 d7_4 d7_3 d7_2 d7_1 d7_0

	;; Shuffle the 4-aparts
	vshuff64x2 zmm8, zmm0, zmm4, 01000100b	;; d4_3 d4_2 d4_1 d4_0 d0_3 d0_2 d0_1 d0_0
	vshuff64x2 zmm9, zmm0, zmm4, 11101110b	;; d4_7	d4_6 d4_5 d4_4 d0_7 d0_6 d0_5 d0_4
	vshuff64x2 zmm10, zmm1, zmm5, 01000100b ;; d5_3 d5_2 d5_1 d5_0 d1_3 d1_2 d1_1 d1_0
	vshuff64x2 zmm11, zmm1, zmm5, 11101110b ;; d5_7	d5_6 d5_5 d5_4 d1_7 d1_6 d1_5 d1_4
	vshuff64x2 zmm12, zmm2, zmm6, 00010001b ;; d6_1 d6_0 d6_3 d6_2 d2_1 d2_0 d2_3 d2_2
	vshuff64x2 zmm13, zmm2, zmm6, 10111011b ;; d6_5	d6_4 d6_7 d6_6 d2_5 d2_4 d2_7 d2_6
	vshuff64x2 zmm14, zmm3, zmm7, 00010001b ;; d7_1 d7_0 d7_3 d7_2 d3_1 d3_0 d3_3 d3_2
	vshuff64x2 zmm15, zmm3, zmm7, 10111011b ;; d7_5	d7_4 d7_7 d7_6 d3_5 d3_4 d3_7 d3_6

	;; Blend in the 2-aparts
	vblendmpd zmm16 {k6}, zmm12, zmm8	;; d6_1 d6_0 d4_1 d4_0 d2_1 d2_0 d0_1 d0_0
	vblendmpd zmm17 {k6}, zmm8, zmm12	;; d4_3 d4_2 d6_3 d6_2 d0_3 d0_2 d2_3 d2_2
	vblendmpd zmm18 {k6}, zmm13, zmm9	;; d6_5 d6_4 d4_5 d4_4 d2_5 d2_4 d0_5 d0_4
	vblendmpd zmm19 {k6}, zmm9, zmm13	;; d4_7 d4_6 d6_7 d6_6 d0_7 d0_6 d2_7 d2_6
	vblendmpd zmm20 {k6}, zmm14, zmm10	;; d7_1 d7_0 d5_1 d5_0 d3_1 d3_0 d1_1 d1_0
	vblendmpd zmm21 {k6}, zmm10, zmm14	;; d5_3 d5_2 d7_3 d7_2 d1_3 d1_2 d3_3 d3_2
	vblendmpd zmm22 {k6}, zmm15, zmm11	;; d7_5 d7_4 d5_5 d5_4 d3_5 d3_4 d1_5 d1_4
	vblendmpd zmm23 {k6}, zmm11, zmm15	;; d5_7 d5_6 d7_7 d7_6 d1_7 d1_6 d3_7 d3_6

	;; Shufffle or permute in the 1-aparts
	vshufpd	zmm0, zmm16, zmm20, 00000000b	;; d7_0 d6_0 d5_0 d4_0 d3_0 d2_0 d1_0 d0_0
	vshufpd	zmm1, zmm16, zmm20, 11111111b	;; d7_1 d6_1 d5_1 d4_1 d3_1 d2_1 d1_1 d0_1
	vmovapd zmm2, zmm17
	vpermt2pd zmm2, zmm30, zmm21		;; d7_2 d6_2 d5_2 d4_2 d3_2 d2_2 d1_2 d0_2
	vpermt2pd zmm17, zmm29, zmm21		;; d7_3 d6_3 d5_3 d4_3 d3_3 d2_3 d1_3 d0_3
	vshufpd	zmm4, zmm18, zmm22, 00000000b 	;; d7_4 d6_4 d5_4 d4_4 d3_4 d2_4 d1_4 d0_4
	vshufpd	zmm5, zmm18, zmm22, 11111111b	;; d7_5 d6_5 d5_5 d4_5 d3_5 d2_5 d1_5 d0_5
	vmovapd zmm6, zmm19
	vpermt2pd zmm6, zmm30, zmm23		;; d7_6 d6_6 d5_6 d4_6 d3_6 d2_6 d1_6 d0_6
	vpermt2pd zmm19, zmm29, zmm23		;; d7_7 d6_7 d5_7 d4_7 d3_7 d2_7 d1_7 d0_7
	ENDM

;;; And an even better swizzle (for Skylake-X), but it uses more constants in registers

zgather_preload MACRO unused
	set	k6 = 00110011b			;; Constant for vblendmpd instructions
	set	k7 = 11110000b			;; Constant for vblendmpd instructions
						;; Constants for vpermt2pd instructions
						;; 0=src2,1=dest-is-src   xxx = double#
	set	zmm30 = 0+4 8+4 0+6 8+6 0+0 8+0 0+2 8+2
	set	zmm29 = 0+5 8+5 0+7 8+7 0+1 8+1 0+3 8+3
	set	zmm28 = 0+2 8+2 0+0 8+0 0+6 8+6 0+4 8+4
	set	zmm27 = 0+3 8+3 0+1 8+1 0+7 8+7 0+5 8+5
	set	zmm26 = 0+0 8+0 0+2 8+2 0+4 8+4 0+6 8+6
	set	zmm25 = 0+1 8+1 0+3 8+3 0+5 8+5 0+7 8+7
	ENDM

zgather MACRO srcreg,d1,d2,d4,r1,r2,r3,r4,r5,r6,r7,r8, r9
	;; Further possible optimization include using unaligned vmovupd and/or vbroadcastf64 instructions?
	vmovapd	zmm0, [srcreg+64]		;; d0_7	d0_6 d0_5 d0_4 d0_3 d0_2 d0_1 d0_0
	vmovapd	zmm1, [srcreg+d1+64]		;; d1_7	d1_6 d1_5 d1_4 d1_3 d1_2 d1_1 d1_0
	vmovapd	zmm2, [srcreg+d2+64]		;; d2_7	d2_6 d2_5 d2_4 d2_3 d2_2 d2_1 d2_0
	vmovapd	zmm3, [srcreg+d2+d1+64]		;; d3_7	d3_6 d3_5 d3_4 d3_3 d3_2 d3_1 d3_0
	vmovapd	zmm4, [srcreg+d4+64]		;; d4_7	d4_6 d4_5 d4_4 d4_3 d4_2 d4_1 d4_0
	vmovapd	zmm5, [srcreg+d4+d1+64]		;; d5_7	d5_6 d5_5 d5_4 d5_3 d5_2 d5_1 d5_0
	vmovapd	zmm6, [srcreg+d4+d2+64]		;; d6_7	d6_6 d6_5 d6_4 d6_3 d6_2 d6_1 d6_0
	vmovapd	zmm7, [srcreg+d4+d2+d1+64]	;; d7_7	d7_6 d7_5 d7_4 d7_3 d7_2 d7_1 d7_0

	;; Shufffle or permute in the 1-aparts
	vshufpd	zmm8, zmm0, zmm1, 00000000b	;; d1_6	d0_6 d1_4 d0_4 d1_2 d0_2 d1_0 d0_0
	vshufpd	zmm9, zmm0, zmm1, 11111111b	;; d1_7	d0_7 d1_5 d0_5 d1_3 d0_3 d1_1 d0_1
	vmovapd zmm10, zmm2
	vpermt2pd zmm10, zmm30, zmm3		;; d3_4 d2_4 d3_6 d2_6 d3_0 d2_0 d3_2 d2_2
	vpermt2pd zmm2, zmm29, zmm3		;; d3_5 d2_5 d3_7 d2_7 d3_1 d2_1 d3_3 d2_3
	vmovapd zmm12, zmm4
	vpermt2pd zmm12, zmm28, zmm5	 	;; d5_2 d4_2 d5_0 d4_0 d5_6 d4_6 d5_4 d4_4
	vpermt2pd zmm4, zmm27, zmm5		;; d5_3 d4_3 d5_1 d4_1 d5_7 d4_7 d5_5 d4_5
	vmovapd zmm14, zmm19
	vpermt2pd zmm14, zmm26, zmm7		;; d7_0 d6_0 d7_2 d6_2 d7_4 d6_4 d7_6 d6_6
	vpermt2pd zmm6, zmm25, zmm7		;; d7_1 d6_1 d7_3 d6_3 d7_5 d6_5 d7_7 d6_7

	;; Blend the 2-aparts
	vblendmpd zmm16 {k6}, zmm10, zmm8	;; d3_4 d2_4 d1_4 d0_4 d3_0 d2_0 d1_0 d0_0
	vblendmpd zmm17 {k6}, zmm8, zmm10	;; d1_6	d0_6 d3_6 d2_6 d1_2 d0_2 d3_2 d2_2
	vblendmpd zmm18 {k6}, zmm2, zmm9	;; d3_5 d2_5 d1_5 d0_5 d3_1 d2_1 d1_1 d0_1
	vblendmpd zmm19 {k6}, zmm9, zmm2	;; d1_7	d0_7 d3_7 d2_7 d1_3 d0_3 d3_3 d2_3
	vblendmpd zmm20 {k6}, zmm14, zmm12	;; d7_0 d6_0 d5_0 d4_0 d7_4 d6_4 d5_4 d4_4
	vblendmpd zmm21 {k6}, zmm12, zmm14	;; d5_2 d4_2 d7_2 d6_2 d5_6 d4_6 d7_6 d6_6
	vblendmpd zmm22 {k6}, zmm6, zmm4	;; d7_1 d6_1 d5_1 d4_1 d7_5 d6_5 d5_5 d4_5
	vblendmpd zmm23 {k6}, zmm4, zmm6	;; d5_3 d4_3 d7_3 d6_3 d5_7 d4_7 d7_7 d6_7

	;; Shuffle/blend the 4-aparts
	vblendmpd zmm0 {k7}, zmm16, zmm20	;; d7_0 d6_0 d5_0 d4_0 d3_0 d2_0 d1_0 d0_0
	vshuff64x2 zmm1, zmm16, zmm20, 10011110b;; d7_4 d6_4 d5_4 d4_4 d3_4 d2_4 d1_4 d0_4
	vshuff64x2 zmm2, zmm17, zmm21, 10110001b;; d7_2 d6_2 d5_2 d4_2 d3_2 d2_2 d1_2 d0_2
	vshuff64x2 zmm3, zmm17, zmm21, 00011011b;; d7_6 d6_6 d5_6 d4_6 d3_6 d2_6 d1_6 d0_6
	vblendmpd zmm4 {k7}, zmm18, zmm22	;; d7_1 d6_1 d5_1 d4_1 d3_1 d2_1 d1_1 d0_1
	vshuff64x2 zmm5, zmm18, zmm22, 01001110b;; d7_5 d6_5 d5_5 d4_5 d3_5 d2_5 d1_5 d0_5
	vshuff64x2 zmm6, zmm19, zmm23, 10110001b;; d7_3 d6_3 d5_3 d4_3 d3_3 d2_3 d1_3 d0_3
	vshuff64x2 zmm7, zmm19, zmm23, 00011011b;; d7_7 d6_7 d5_7 d4_7 d3_7 d2_7 d1_7 d0_7
	ENDM

zscatter_preload MACRO unused
	ENDM
zscatter MACRO srcreg,d1,d2,d4,r1,r2,r3,r4,r5,r6,r7,r8,     r9
	;; BUG - use new swizzle code from zgather above
	vshufpd	r9, r1, r2, 00000000b	;; d1_6 d0_6 d1_4 d0_4 d1_2 d0_2 d1_0 d0_0
	vshufpd	r1, r1, r2, 11111111b	;; d1_7 d0_7 d1_5 d0_5 d1_3 d0_3 d1_1 d0_1
	vshufpd	r2, r3, r4, 00000000b	;; d3_6 d2_6 d3_4 d2_4 d3_2 d2_2 d3_0 d2_0
	vshufpd	r3, r3, r4, 11111111b	;; d3_7 d2_7 d3_5 d2_5 d3_3 d2_3 d3_1 d2_1
	vshufpd	r4, r5, r6, 00000000b	;; d5_6 d4_6 d5_4 d4_4 d5_2 d4_2 d5_0 d4_0
	vshufpd	r5, r5, r6, 11111111b	;; d5_7 d4_7 d5_5 d4_5 d5_3 d4_3 d5_1 d4_1
	vshufpd	r6, r7, r8, 00000000b	;; d7_6 d6_6 d7_4 d6_4 d7_2 d6_2 d7_0 d6_0
	vshufpd	r7, r7, r8, 11111111b	;; d7_7 d6_7 d7_5 d6_5 d7_3 d6_3 d7_1 d6_1

	vshuff64x2 r8, r9, r2, 10001000b ;; d3_4 d2_4 d3_0 d2_0 d1_4 d0_4 d1_0 d0_0
	vshuff64x2 r9, r9, r2, 11011101b ;; d3_6 d2_6 d3_2 d2_2 d1_6 d0_6 d1_2 d0_2
	vshuff64x2 r2, r1, r3, 10001000b ;; d3_5 d2_5 d3_1 d2_1 d1_5 d0_5 d1_1 d0_1
	vshuff64x2 r1, r1, r3, 11011101b ;; d3_7 d2_7 d3_3 d2_3 d1_7 d0_7 d1_3 d0_3
	vshuff64x2 r3, r4, r6, 10001000b ;; d7_4 d6_4 d7_0 d6_0 d5_4 d4_4 d5_0 d4_0
	vshuff64x2 r4, r4, r6, 11011101b ;; d7_6 d6_6 d7_2 d6_2 d5_6 d4_6 d5_2 d4_2
	vshuff64x2 r6, r5, r7, 10001000b ;; d7_5 d6_5 d7_1 d6_1 d5_5 d4_5 d5_1 d4_1
	vshuff64x2 r5, r5, r7, 11011101b ;; d7_7 d6_7 d7_3 d6_3 d5_7 d4_7 d5_3 d4_3

	vshuff64x2 r7, r8, r3, 10001000b ;; d7_0 d6_0 d5_0 d4_0 d3_0 d2_0 d1_0 d0_0
	vshuff64x2 r8, r8, r3, 11011101b ;; d7_4 d6_4 d5_4 d4_4 d3_4 d2_4 d1_4 d0_4
	vshuff64x2 r3, r9, r4, 10001000b ;; d7_2 d6_2 d5_2 d4_2 d3_2 d2_2 d1_2 d0_2
	vshuff64x2 r9, r9, r4, 11011101b ;; d7_6 d6_6 d5_6 d4_6 d3_6 d2_6 d1_6 d0_6
	vshuff64x2 r4, r2, r6, 10001000b ;; d7_1 d6_1 d5_1 d4_1 d3_1 d2_1 d1_1 d0_1
	vshuff64x2 r2, r2, r6, 11011101b ;; d7_5 d6_5 d5_5 d4_5 d3_5 d2_5 d1_5 d0_5
	vshuff64x2 r6, r1, r5, 10001000b ;; d7_3 d6_3 d5_3 d4_3 d3_3 d2_3 d1_3 d0_3
	vshuff64x2 r1, r1, r5, 11011101b ;; d7_7 d6_7 d5_7 d4_7 d3_7 d2_7 d1_7 d0_7

	zstore	[srcreg], r7
	zstore	[srcreg+d4], r8
	zstore	[srcreg+d2], r3
	zstore	[srcreg+d4+d2], r9
	zstore	[srcreg+d1], r4
	zstore	[srcreg+d4+d1], r2
	zstore	[srcreg+d2+d1], r6
	zstore	[srcreg+d4+d2+d1], r1
	ENDM

;;
;; Prefetching macros
;;

; Macros to prefetch a 64-byte line into the L1 cache

L1PREFETCH_NONE		EQU	0		; No L1 prefetching
L1PREFETCH_ALL		EQU	3		; L1 prefetching on
L1PREFETCH_DEST_NONE	EQU	1000		; No L1 prefetching for destination
L1PREFETCH_DEST_ALL	EQU	1003		; L1 prefetching on for destination

L1prefetch MACRO addr, type
	IFNB <type>
	IF (type EQ L1PREFETCH_ALL)
	prefetcht0 [addr]
	ENDIF
	ENDIF
	ENDM
L1prefetchw MACRO addr, type
	IFNB <type>
	IF (type EQ L1PREFETCH_ALL)
	prefetcht0 [addr]
	ENDIF
	ENDIF
	ENDM

;;
;; Macros that do a complex squaring or multiplication
;;

zp_complex_square MACRO real, imag, tmp
	vmulpd	tmp, imag, real		;; imag * real
	vmulpd	real, real, real	;; real * real
	vmulpd	imag, imag, imag	;; imag * imag
	vsubpd	real, real, imag	;; real^2 - imag^2 (new real)
	vaddpd	imag, tmp, tmp		;; imag * real * 2 (new imag)
	ENDM

zp_complex_mult MACRO real1, imag1, real2, imag2, tmp1, tmp2
	vmulpd	tmp1, real1, real2	;; real1 * real2
	vmulpd	tmp2, imag1, imag2	;; imag1 * imag2
	vmulpd	real1, real1, imag2	;; real1 * imag2
	vmulpd	imag1, imag1, real2	;; imag1 * real2
	vaddpd	imag1, real1, imag1	;; real1*imag2+real2*imag1 (new imag)
	vsubpd	real1, tmp1, tmp2	;; real1*real2-imag1*imag2 (new real)
	ENDM

zs_complex_square MACRO real, imag, tmp
	vmulsd	tmp, imag, real		;; imag * real
	vmulsd	real, real, real	;; real * real
	vmulsd	imag, imag, imag	;; imag * imag
	vsubsd	real, real, imag	;; real^2 - imag^2 (new real)
	vaddsd	imag, tmp, tmp		;; imag * real * 2 (new imag)
	ENDM

zs_complex_mult MACRO real1, imag1, real2, imag2, tmp1, tmp2
	vmulsd	tmp1, real1, real2	;; real1 * real2
	vmulsd	tmp2, imag1, imag2	;; imag1 * imag2
	vmulsd	real1, real1, imag2	;; real1 * imag2
	vmulsd	imag1, imag1, real2	;; imag1 * real2
	vaddsd	imag1, real1, imag1	;; real1*imag2+real2*imag1 (new imag)
	vsubsd	real1, tmp1, tmp2	;; real1*real2-imag1*imag2 (new real)
	ENDM

;; Do the brute-force multiplication of the 7 words near the half-way point.
;; These seven words were copied to an area 33-96 bytes before the FFT data.
;; This is done for zero-padded FFTs only.

zsquare7 MACRO	src1
	LOCAL	nozpad, muladd, mulsub, hard, done
	cmp	ZERO_PADDED_FFT, 0	;; Is this a zero-padded FFT?
	je	nozpad			;; No, skip 7 word multiply
	vmovsd	xmm1, Q [src1-88]	;; Load copy of input FFT word at halfway - 3
	vmovsd	xmm2, Q [src1-80]	;; Load copy of input FFT word at halfway - 2
	vmovsd	xmm3, Q [src1-72]	;; Load copy of input FFT word at halfway - 1
	vmovsd	xmm4, Q [src1-64]	;; Load copy of input FFT word at halfway + 0
	vmovsd	xmm5, Q [src1-56]	;; Load copy of input FFT word at halfway + 1
	vmovsd	xmm6, Q [src1-48]	;; Load copy of input FFT word at halfway + 2
	vmovsd	xmm7, Q [src1-40]	;; Load copy of input FFT word at halfway + 3

	vmulsd	xmm0, xmm1, xmm7	;; Result0 = word-3 * word3
	zfmaddsd xmm0, xmm2, xmm6, xmm0	;;	   + word-2 * word2
	zfmaddsd xmm0, xmm3, xmm5, xmm0	;;	   + word-1 * word1
	vaddsd	xmm0, xmm0, xmm0	;;	   + word1 * word-1
					;;	   + word2 * word-2
					;;	   + word3 * word-3
	zfmaddsd xmm0, xmm4, xmm4, xmm0	;;	   + word0 * word0

	vmulsd	xmm1, xmm2, xmm7	;; Result1 = word-2 * word3
	zfmaddsd xmm1, xmm3, xmm6, xmm1	;;	   + word-1 * word2
	zfmaddsd xmm1, xmm4, xmm5, xmm1	;;	   + word0 * word1
	vaddsd	xmm1, xmm1, xmm1	;;	   + word1 * word0
					;;	   + word2 * word-1
					;;	   + word3 * word-2

	vmulsd	xmm2, xmm3, xmm7	;; Result2 = word-1 * word3
	zfmaddsd xmm2, xmm4, xmm6, xmm2	;;	   + word0 * word2
	vaddsd	xmm2, xmm2, xmm2	;;	   + word2 * word0
					;;	   + word3 * word-1
	zfmaddsd xmm2, xmm5, xmm5, xmm2	;;	   + word1 * word1

	vmulsd	xmm3, xmm4, xmm7	;; Result3 = word0 * word3
	zfmaddsd xmm3, xmm5, xmm6, xmm3	;;	   + word1 * word2
	vaddsd	xmm3, xmm3, xmm3	;;	   + word2 * word1
					;;	   + word3 * word0

	vmulsd	xmm4, xmm5, xmm7	;; Result4 = word1 * word3
	vaddsd	xmm4, xmm4, xmm4	;;	   + word3 * word1
	zfmaddsd xmm4, xmm6, xmm6, xmm4	;;	   + word2 * word2

	vmulsd	xmm5, xmm6, xmm7	;; Result5 = word2 * word3
	vaddsd	xmm5, xmm5, xmm5	;;	   + word3 * word2

	vmulsd	xmm6, xmm7, xmm7	;; Result6 = word3 * word3

	mov	al, mul4_opcode		;; Load gwmul4 opcode
	and	al, 7fh			;; Mask out the save FFT results flag
	jz	done			;; Jump if not doing fma

	mov	r9, SRC2ARG		;; Distance to s3 arg from gwmuladd4 and gwmulsub4
	mov	r10, SRC3ARG		;; Distance to FFT(1)
	cmp	r10, 1			;; Do we have an FFT(1)
	jne	short hard		;; Yes, do it the hard way

	vmovsd	xmm7, Q [rsi+r9-88]	;; Load addin0
	vmovsd	xmm8, Q [rsi+r9-80]	;; Load addin1
	vmovsd	xmm9, Q [rsi+r9-72]	;; Load addin2
	vmovsd	xmm10, Q [rsi+r9-64]	;; Load addin3
	vmovsd	xmm11, Q [rsi+r9-56]	;; Load addin4
	vmovsd	xmm12, Q [rsi+r9-48]	;; Load addin5
	vmovsd	xmm13, Q [rsi+r9-40]	;; Load addin6
	cmp	al, 3			;; Are we doing an muladd or mulsub
	jg	mulsub			;; 4 = mulsub
	je	muladd			;; 3 = muladd, fall through

hard:	vmovsd	xmm8, Q [rsi+r9-88]	;; Load copy of add-in FFT word at halfway - 3
	vmovsd	xmm9, Q [rsi+r9-80]	;; Load copy of add-in FFT word at halfway - 2
	vmovsd	xmm10, Q [rsi+r9-72]	;; Load copy of add-in FFT word at halfway - 1
	vmovsd	xmm11, Q [rsi+r9-64]	;; Load copy of add-in FFT word at halfway + 0
	vmovsd	xmm12, Q [rsi+r9-56]	;; Load copy of add-in FFT word at halfway + 1
	vmovsd	xmm13, Q [rsi+r9-48]	;; Load copy of add-in FFT word at halfway + 2
	vmovsd	xmm14, Q [rsi+r9-40]	;; Load copy of add-in FFT word at halfway + 3
	vmovsd	xmm15, Q [rsi+r10-88]	;; Load copy of FFT(1) word at halfway - 3
	vmovsd	xmm16, Q [rsi+r10-80]	;; Load copy of FFT(1) word at halfway - 2
	vmovsd	xmm17, Q [rsi+r10-72]	;; Load copy of FFT(1) word at halfway - 1
	vmovsd	xmm18, Q [rsi+r10-64]	;; Load copy of FFT(1) word at halfway + 0
	vmovsd	xmm19, Q [rsi+r10-56]	;; Load copy of FFT(1) word at halfway + 1
	vmovsd	xmm20, Q [rsi+r10-48]	;; Load copy of FFT(1) word at halfway + 2
	vmovsd	xmm21, Q [rsi+r10-40]	;; Load copy of FFT(1) word at halfway + 3

	vmulsd	xmm7, xmm8, xmm21		;; Addin0 = word-3 * word3
	zfmaddsd xmm7, xmm9, xmm20, xmm7	;;	  + word-2 * word2
	zfmaddsd xmm7, xmm10, xmm19, xmm7	;;	  + word-1 * word1
	zfmaddsd xmm7, xmm11, xmm18, xmm7	;;	  + word0 * word0
	zfmaddsd xmm7, xmm12, xmm17, xmm7	;;	  + word1 * word-1
	zfmaddsd xmm7, xmm13, xmm16, xmm7	;;	  + word2 * word-2
	zfmaddsd xmm7, xmm14, xmm15, xmm7	;;	  + word3 * word-3

	vmulsd	xmm8, xmm9, xmm21		;; Addin1 = word-2 * word3
	zfmaddsd xmm8, xmm10, xmm20, xmm8	;;	  + word-1 * word2
	zfmaddsd xmm8, xmm11, xmm19, xmm8	;;	  + word0 * word1
	zfmaddsd xmm8, xmm12, xmm18, xmm8	;;	  + word1 * word0
	zfmaddsd xmm8, xmm13, xmm17, xmm8	;;	  + word2 * word-1
	zfmaddsd xmm8, xmm14, xmm16, xmm8	;;	  + word3 * word-2

	vmulsd	xmm9, xmm10, xmm21		;; Addin2 = word-1 * word3
	zfmaddsd xmm9, xmm11, xmm20, xmm9	;;	  + word0 * word2
	zfmaddsd xmm9, xmm12, xmm19, xmm9	;;	  + word1 * word1
	zfmaddsd xmm9, xmm13, xmm18, xmm9	;;	  + word2 * word0
	zfmaddsd xmm9, xmm14, xmm17, xmm9	;;	  + word3 * word-1

	vmulsd	xmm10, xmm11, xmm21		;; Addin3 = word0 * word3
	zfmaddsd xmm10, xmm12, xmm20, xmm10	;;	  + word1 * word2
	zfmaddsd xmm10, xmm13, xmm19, xmm10	;;	  + word2 * word1
	zfmaddsd xmm10, xmm14, xmm18, xmm10	;;	  + word3 * word0

	vmulsd	xmm11, xmm12, xmm21		;; Addin4 = word1 * word3
	zfmaddsd xmm11, xmm13, xmm20, xmm11	;;	  + word2 * word2
	zfmaddsd xmm11, xmm14, xmm19, xmm11	;;	  + word3 * word1

	vmulsd	xmm12, xmm13, xmm21		;; Addin5 = word2 * word3
	zfmaddsd xmm12, xmm14, xmm20, xmm12	;;	  + word3 * word2

	vmulsd	xmm13, xmm14, xmm21		;; Addin6 = word3 * word3

	cmp	al, 3				;; Are we doing muladd or mulsub
	jg	mulsub				;; 4 = mulsub
	;je	muladd				;; 3 = muladd, fall through

muladd:	vaddpd	xmm0, xmm0, xmm7		;; Add the addins
	vaddpd	xmm1, xmm1, xmm8
	vaddpd	xmm2, xmm2, xmm9
	vaddpd	xmm3, xmm3, xmm10
	vaddpd	xmm4, xmm4, xmm11
	vaddpd	xmm5, xmm5, xmm12
	vaddpd	xmm6, xmm6, xmm13
	jmp	done

mulsub:	vsubpd	xmm0, xmm0, xmm7		;; Subtract the addins
	vsubpd	xmm1, xmm1, xmm8
	vsubpd	xmm2, xmm2, xmm9
	vsubpd	xmm3, xmm3, xmm10
	vsubpd	xmm4, xmm4, xmm11
	vsubpd	xmm5, xmm5, xmm12
	vsubpd	xmm6, xmm6, xmm13

done:	vmovsd	ZPAD0, xmm0			;; Save the zero-pad adjustments
	vmovsd	ZPAD1, xmm1
	vmovsd	ZPAD2, xmm2
	vmovsd	ZPAD3, xmm3
	vmovsd	ZPAD4, xmm4
	vmovsd	ZPAD5, xmm5
	vmovsd	ZPAD6, xmm6

nozpad:
	ENDM

zmult7	MACRO	src1, src2
	LOCAL	nozpad, orig, addmul, submul, muladd, mulsub, hard, done
	cmp	ZERO_PADDED_FFT, 0	;; Is this a zero-padded FFT?
	je	nozpad			;; No, skip 7 word multiply

	vmovsd	xmm1, Q [src1-88]	;; Load copy of input FFT word at halfway - 3
	vmovsd	xmm2, Q [src1-80]	;; Load copy of input FFT word at halfway - 2
	vmovsd	xmm3, Q [src1-72]	;; Load copy of input FFT word at halfway - 1
	vmovsd	xmm4, Q [src1-64]	;; Load copy of input FFT word at halfway + 0
	vmovsd	xmm5, Q [src1-56]	;; Load copy of input FFT word at halfway + 1
	vmovsd	xmm6, Q [src1-48]	;; Load copy of input FFT word at halfway + 2
	vmovsd	xmm7, Q [src1-40]	;; Load copy of input FFT word at halfway + 3

	vmovsd	xmm8, Q [src2-88]	;; Load copy of input FFT word at halfway - 3
	vmovsd	xmm9, Q [src2-80]	;; Load copy of input FFT word at halfway - 2
	vmovsd	xmm10, Q [src2-72]	;; Load copy of input FFT word at halfway - 1
	vmovsd	xmm11, Q [src2-64]	;; Load copy of input FFT word at halfway + 0
	vmovsd	xmm12, Q [src2-56]	;; Load copy of input FFT word at halfway + 1
	vmovsd	xmm13, Q [src2-48]	;; Load copy of input FFT word at halfway + 2
	vmovsd	xmm14, Q [src2-40]	;; Load copy of input FFT word at halfway + 3

	mov	al, mul4_opcode		;; Load gwmul4 opcode
	and	al, 7fh			;; Mask out the save FFT results flag
	jz	orig			;; Jump if not doing addmul or submul of fma
	mov	r9, SRC2ARG		;; Distance to s2 arg from gwaddmul4 or gwsubmul4 or s3 arg from gwmuladd4 and gwmulsub4
	cmp	al, 2			;; Are we doing an addmul or submul or fma
	ja	orig			;; fma, we'll handle this later
	je	short submul		;; 2 = submul
	;jb	short addmul		;; 1 = addmul, fall through

addmul:	vaddsd	xmm8, xmm8, Q [rsi+r9-88]	;; Add copy of input FFT word at halfway - 3
	vaddsd	xmm9, xmm9, Q [rsi+r9-80]	;; Add copy of input FFT word at halfway - 2
	vaddsd	xmm10, xmm10, Q [rsi+r9-72]	;; Add copy of input FFT word at halfway - 1
	vaddsd	xmm11, xmm11, Q [rsi+r9-64]	;; Add copy of input FFT word at halfway + 0
	vaddsd	xmm12, xmm12, Q [rsi+r9-56]	;; Add copy of input FFT word at halfway + 1
	vaddsd	xmm13, xmm13, Q [rsi+r9-48]	;; Add copy of input FFT word at halfway + 2
	vaddsd	xmm14, xmm14, Q [rsi+r9-40]	;; Add copy of input FFT word at halfway + 3
	jmp	short orig

submul:	vsubsd	xmm8, xmm8, Q [rsi+r9-88]	;; Sub copy of input FFT word at halfway - 3
	vsubsd	xmm9, xmm9, Q [rsi+r9-80]	;; Sub copy of input FFT word at halfway - 2
	vsubsd	xmm10, xmm10, Q [rsi+r9-72]	;; Sub copy of input FFT word at halfway - 1
	vsubsd	xmm11, xmm11, Q [rsi+r9-64]	;; Sub copy of input FFT word at halfway + 0
	vsubsd	xmm12, xmm12, Q [rsi+r9-56]	;; Sub copy of input FFT word at halfway + 1
	vsubsd	xmm13, xmm13, Q [rsi+r9-48]	;; Sub copy of input FFT word at halfway + 2
	vsubsd	xmm14, xmm14, Q [rsi+r9-40]	;; Sub copy of input FFT word at halfway + 3

orig:	vmulsd	xmm0, xmm1, xmm14	;; Result0 = word-3 * word3
	zfmaddsd xmm0, xmm2, xmm13, xmm0 ;;	   + word-2 * word2
	zfmaddsd xmm0, xmm3, xmm12, xmm0 ;;	   + word-1 * word1
	zfmaddsd xmm0, xmm4, xmm11, xmm0 ;;	   + word0 * word0
	zfmaddsd xmm0, xmm5, xmm10, xmm0 ;;	   + word1 * word-1
	zfmaddsd xmm0, xmm6, xmm9, xmm0	;;	   + word2 * word-2
	zfmaddsd xmm0, xmm7, xmm8, xmm0	;;	   + word3 * word-3

	vmulsd	xmm1, xmm2, xmm14	;; Result1 = word-2 * word3
	zfmaddsd xmm1, xmm3, xmm13, xmm1 ;;	   + word-1 * word2
	zfmaddsd xmm1, xmm4, xmm12, xmm1 ;;	   + word0 * word1
	zfmaddsd xmm1, xmm5, xmm11, xmm1 ;;	   + word1 * word0
	zfmaddsd xmm1, xmm6, xmm10, xmm1 ;;	   + word2 * word-1
	zfmaddsd xmm1, xmm7, xmm9, xmm1	;;	   + word3 * word-2

	vmulsd	xmm2, xmm3, xmm14	;; Result2 = word-1 * word3
	zfmaddsd xmm2, xmm4, xmm13, xmm2 ;;	   + word0 * word2
	zfmaddsd xmm2, xmm5, xmm12, xmm2 ;;	   + word1 * word1
	zfmaddsd xmm2, xmm6, xmm11, xmm2 ;;	   + word2 * word0
	zfmaddsd xmm2, xmm7, xmm10, xmm2 ;;	   + word3 * word-1

	vmulsd	xmm3, xmm4, xmm14	;; Result3 = word0 * word3
	zfmaddsd xmm3, xmm5, xmm13, xmm3 ;;	   + word1 * word2
	zfmaddsd xmm3, xmm6, xmm12, xmm3 ;;	   + word2 * word1
	zfmaddsd xmm3, xmm7, xmm11, xmm3 ;;	   + word3 * word0

	vmulsd	xmm4, xmm5, xmm14	;; Result4 = word1 * word3
	zfmaddsd xmm4, xmm6, xmm13, xmm4 ;;	   + word2 * word2
	zfmaddsd xmm4, xmm7, xmm12, xmm4 ;;	   + word3 * word1

	vmulsd	xmm5, xmm6, xmm14	;; Result5 = word2 * word3
	zfmaddsd xmm5, xmm7, xmm13, xmm5 ;;	   + word3 * word2

	vmulsd	xmm6, xmm7, xmm14	;; Result6 = word3 * word3

	cmp	al, 3			;; Are we doing an muladd or mulsub
	jl	done			;; No, we're done

	mov	r10, SRC3ARG		;; Distance to FFT(1)
	cmp	r10, 1			;; Do we have an FFT(1)
	jne	short hard		;; Yes, do it the hard way

	vmovsd	xmm7, Q [rsi+r9-88]	;; Load addin0
	vmovsd	xmm8, Q [rsi+r9-80]	;; Load addin1
	vmovsd	xmm9, Q [rsi+r9-72]	;; Load addin2
	vmovsd	xmm10, Q [rsi+r9-64]	;; Load addin3
	vmovsd	xmm11, Q [rsi+r9-56]	;; Load addin4
	vmovsd	xmm12, Q [rsi+r9-48]	;; Load addin5
	vmovsd	xmm13, Q [rsi+r9-40]	;; Load addin6
	cmp	al, 3			;; Are we doing an muladd or mulsub
	jg	mulsub			;; 4 = mulsub
	je	muladd			;; 3 = muladd, fall through

hard:	vmovsd	xmm8, Q [rsi+r9-88]	;; Load copy of add-in FFT word at halfway - 3
	vmovsd	xmm9, Q [rsi+r9-80]	;; Load copy of add-in FFT word at halfway - 2
	vmovsd	xmm10, Q [rsi+r9-72]	;; Load copy of add-in FFT word at halfway - 1
	vmovsd	xmm11, Q [rsi+r9-64]	;; Load copy of add-in FFT word at halfway + 0
	vmovsd	xmm12, Q [rsi+r9-56]	;; Load copy of add-in FFT word at halfway + 1
	vmovsd	xmm13, Q [rsi+r9-48]	;; Load copy of add-in FFT word at halfway + 2
	vmovsd	xmm14, Q [rsi+r9-40]	;; Load copy of add-in FFT word at halfway + 3
	vmovsd	xmm15, Q [rsi+r10-88]	;; Load copy of FFT(1) word at halfway - 3
	vmovsd	xmm16, Q [rsi+r10-80]	;; Load copy of FFT(1) word at halfway - 2
	vmovsd	xmm17, Q [rsi+r10-72]	;; Load copy of FFT(1) word at halfway - 1
	vmovsd	xmm18, Q [rsi+r10-64]	;; Load copy of FFT(1) word at halfway + 0
	vmovsd	xmm19, Q [rsi+r10-56]	;; Load copy of FFT(1) word at halfway + 1
	vmovsd	xmm20, Q [rsi+r10-48]	;; Load copy of FFT(1) word at halfway + 2
	vmovsd	xmm21, Q [rsi+r10-40]	;; Load copy of FFT(1) word at halfway + 3

	vmulsd	xmm7, xmm8, xmm21		;; Addin0 = word-3 * word3
	zfmaddsd xmm7, xmm9, xmm20, xmm7	;;	  + word-2 * word2
	zfmaddsd xmm7, xmm10, xmm19, xmm7	;;	  + word-1 * word1
	zfmaddsd xmm7, xmm11, xmm18, xmm7	;;	  + word0 * word0
	zfmaddsd xmm7, xmm12, xmm17, xmm7	;;	  + word1 * word-1
	zfmaddsd xmm7, xmm13, xmm16, xmm7	;;	  + word2 * word-2
	zfmaddsd xmm7, xmm14, xmm15, xmm7	;;	  + word3 * word-3

	vmulsd	xmm8, xmm9, xmm21		;; Addin1 = word-2 * word3
	zfmaddsd xmm8, xmm10, xmm20, xmm8	;;	  + word-1 * word2
	zfmaddsd xmm8, xmm11, xmm19, xmm8	;;	  + word0 * word1
	zfmaddsd xmm8, xmm12, xmm18, xmm8	;;	  + word1 * word0
	zfmaddsd xmm8, xmm13, xmm17, xmm8	;;	  + word2 * word-1
	zfmaddsd xmm8, xmm14, xmm16, xmm8	;;	  + word3 * word-2

	vmulsd	xmm9, xmm10, xmm21		;; Addin2 = word-1 * word3
	zfmaddsd xmm9, xmm11, xmm20, xmm9	;;	  + word0 * word2
	zfmaddsd xmm9, xmm12, xmm19, xmm9	;;	  + word1 * word1
	zfmaddsd xmm9, xmm13, xmm18, xmm9	;;	  + word2 * word0
	zfmaddsd xmm9, xmm14, xmm17, xmm9	;;	  + word3 * word-1

	vmulsd	xmm10, xmm11, xmm21		;; Addin3 = word0 * word3
	zfmaddsd xmm10, xmm12, xmm20, xmm10	;;	  + word1 * word2
	zfmaddsd xmm10, xmm13, xmm19, xmm10	;;	  + word2 * word1
	zfmaddsd xmm10, xmm14, xmm18, xmm10	;;	  + word3 * word0

	vmulsd	xmm11, xmm12, xmm21		;; Addin4 = word1 * word3
	zfmaddsd xmm11, xmm13, xmm20, xmm11	;;	  + word2 * word2
	zfmaddsd xmm11, xmm14, xmm19, xmm11	;;	  + word3 * word1

	vmulsd	xmm12, xmm13, xmm21		;; Addin5 = word2 * word3
	zfmaddsd xmm12, xmm14, xmm20, xmm12	;;	  + word3 * word2

	vmulsd	xmm13, xmm14, xmm21		;; Addin6 = word3 * word3

	cmp	al, 3				;; Are we doing an muladd or mulsub
	jg	mulsub				;; 4 = mulsub
	;je	muladd				;; 3 = muladd, fall through

muladd:	vaddpd	xmm0, xmm0, xmm7		;; Add the addins
	vaddpd	xmm1, xmm1, xmm8
	vaddpd	xmm2, xmm2, xmm9
	vaddpd	xmm3, xmm3, xmm10
	vaddpd	xmm4, xmm4, xmm11
	vaddpd	xmm5, xmm5, xmm12
	vaddpd	xmm6, xmm6, xmm13
	jmp	done

mulsub:	vsubpd	xmm0, xmm0, xmm7		;; Subtract the addins
	vsubpd	xmm1, xmm1, xmm8
	vsubpd	xmm2, xmm2, xmm9
	vsubpd	xmm3, xmm3, xmm10
	vsubpd	xmm4, xmm4, xmm11
	vsubpd	xmm5, xmm5, xmm12
	vsubpd	xmm6, xmm6, xmm13

done:	vmovsd	ZPAD0, xmm0			;; Save the zero-pad adjustments
	vmovsd	ZPAD1, xmm1
	vmovsd	ZPAD2, xmm2
	vmovsd	ZPAD3, xmm3
	vmovsd	ZPAD4, xmm4
	vmovsd	ZPAD5, xmm5
	vmovsd	ZPAD6, xmm6

nozpad:
	ENDM
